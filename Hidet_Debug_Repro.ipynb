{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/AlphaAtlas/Diffusion-Compilaton-Testing/blob/main/Hidet_Debug_Repro.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMNKqS52Uy-V",
        "outputId": "21acaf75-39a5-47e9-cc67-4317360e09b9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Wed Jun  7 17:45:09 2023       \n",
            "+-----------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 525.85.12    Driver Version: 525.85.12    CUDA Version: 12.0     |\n",
            "|-------------------------------+----------------------+----------------------+\n",
            "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                               |                      |               MIG M. |\n",
            "|===============================+======================+======================|\n",
            "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   53C    P8    10W /  70W |      0MiB / 15360MiB |      0%      Default |\n",
            "|                               |                      |                  N/A |\n",
            "+-------------------------------+----------------------+----------------------+\n",
            "                                                                               \n",
            "+-----------------------------------------------------------------------------+\n",
            "| Processes:                                                                  |\n",
            "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
            "|        ID   ID                                                   Usage      |\n",
            "|=============================================================================|\n",
            "|  No running processes found                                                 |\n",
            "+-----------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "#Check GPU\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Upgrade numpy. This will require a runtime restart\n",
        "!pip install --upgrade numpy\n"
      ],
      "metadata": {
        "id": "EYi873wpAGYV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Install requirements, including the git version of diffusers with graph break fixes\n",
        "!pip install --pre transformers accelerate\n",
        "#!pip install torch torchvision pytorch-triton --extra-index-url https://download.pytorch.org/whl/nightly/cu121 --pre --upgrade  #Torch nightly is not needed to reproduce the bugs\n",
        "!pip install -v -U git+https://github.com/huggingface/diffusers.git\n",
        "!pip install --pre --extra-index-url https://download.hidet.org/whl hidet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PeYdMTPWSLeg",
        "outputId": "40aadfe4-62a1-4141-8bad-db5321a9176e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://download.hidet.org/whl\n",
            "Collecting hidet\n",
            "  Downloading https://download.hidet.org/whl/hidet/hidet-0.2.4.dev20230607-py3-none-any.whl (663 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m663.6/663.6 kB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy>=1.23 (from hidet)\n",
            "  Downloading numpy-1.25.0rc1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m65.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from hidet) (5.9.5)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from hidet) (4.65.0)\n",
            "Collecting nvtx (from hidet)\n",
            "  Downloading nvtx-0.2.5-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (428 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m428.4/428.4 kB\u001b[0m \u001b[31m44.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from hidet) (0.8.10)\n",
            "Requirement already satisfied: astunparse in /usr/local/lib/python3.10/dist-packages (from hidet) (1.6.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from hidet) (8.1.3)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hidet) (23.1)\n",
            "Collecting cuda-python>=11.6.1 (from hidet)\n",
            "  Downloading cuda_python-12.1.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.8/18.8 MB\u001b[0m \u001b[31m64.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: cython in /usr/local/lib/python3.10/dist-packages (from cuda-python>=11.6.1->hidet) (0.29.34)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse->hidet) (0.40.0)\n",
            "Requirement already satisfied: six<2.0,>=1.6.1 in /usr/local/lib/python3.10/dist-packages (from astunparse->hidet) (1.16.0)\n",
            "Installing collected packages: nvtx, numpy, cuda-python, hidet\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.22.4\n",
            "    Uninstalling numpy-1.22.4:\n",
            "      Successfully uninstalled numpy-1.22.4\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0rc1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0rc1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed cuda-python-12.1.0 hidet-0.2.4.dev20230607 numpy-1.25.0rc1 nvtx-0.2.5\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/huggingface/diffusers.git\n",
            "  Cloning https://github.com/huggingface/diffusers.git to /tmp/pip-req-build-cbo9gezp\n",
            "  Running command git version\n",
            "  git version 2.25.1\n",
            "  Running command git clone --filter=blob:none https://github.com/huggingface/diffusers.git /tmp/pip-req-build-cbo9gezp\n",
            "  Cloning into '/tmp/pip-req-build-cbo9gezp'...\n",
            "  Updating files:   0% (2/752)\n",
            "  Updating files:   1% (8/752)\n",
            "  Updating files:   2% (16/752)\n",
            "  Updating files:   3% (23/752)\n",
            "  Updating files:   4% (31/752)\n",
            "  Updating files:   5% (38/752)\n",
            "  Updating files:   6% (46/752)\n",
            "  Updating files:   7% (53/752)\n",
            "  Updating files:   8% (61/752)\n",
            "  Updating files:   9% (68/752)\n",
            "  Updating files:  10% (76/752)\n",
            "  Updating files:  11% (83/752)\n",
            "  Updating files:  12% (91/752)\n",
            "  Updating files:  13% (98/752)\n",
            "  Updating files:  14% (106/752)\n",
            "  Updating files:  15% (113/752)\n",
            "  Updating files:  16% (121/752)\n",
            "  Updating files:  17% (128/752)\n",
            "  Updating files:  18% (136/752)\n",
            "  Updating files:  19% (143/752)\n",
            "  Updating files:  20% (151/752)\n",
            "  Updating files:  21% (158/752)\n",
            "  Updating files:  22% (166/752)\n",
            "  Updating files:  23% (173/752)\n",
            "  Updating files:  24% (181/752)\n",
            "  Updating files:  25% (188/752)\n",
            "  Updating files:  26% (196/752)\n",
            "  Updating files:  27% (204/752)\n",
            "  Updating files:  28% (211/752)\n",
            "  Updating files:  29% (219/752)\n",
            "  Updating files:  30% (226/752)\n",
            "  Updating files:  31% (234/752)\n",
            "  Updating files:  32% (241/752)\n",
            "  Updating files:  33% (249/752)\n",
            "  Updating files:  34% (256/752)\n",
            "  Updating files:  35% (264/752)\n",
            "  Updating files:  36% (271/752)\n",
            "  Updating files:  37% (279/752)\n",
            "  Updating files:  38% (286/752)\n",
            "  Updating files:  39% (294/752)\n",
            "  Updating files:  40% (301/752)\n",
            "  Updating files:  41% (309/752)\n",
            "  Updating files:  42% (316/752)\n",
            "  Updating files:  43% (324/752)\n",
            "  Updating files:  44% (331/752)\n",
            "  Updating files:  45% (339/752)\n",
            "  Updating files:  46% (346/752)\n",
            "  Updating files:  47% (354/752)\n",
            "  Updating files:  48% (361/752)\n",
            "  Updating files:  49% (369/752)\n",
            "  Updating files:  50% (376/752)\n",
            "  Updating files:  51% (384/752)\n",
            "  Updating files:  52% (392/752)\n",
            "  Updating files:  53% (399/752)\n",
            "  Updating files:  54% (407/752)\n",
            "  Updating files:  55% (414/752)\n",
            "  Updating files:  56% (422/752)\n",
            "  Updating files:  57% (429/752)\n",
            "  Updating files:  58% (437/752)\n",
            "  Updating files:  59% (444/752)\n",
            "  Updating files:  60% (452/752)\n",
            "  Updating files:  61% (459/752)\n",
            "  Updating files:  62% (467/752)\n",
            "  Updating files:  63% (474/752)\n",
            "  Updating files:  64% (482/752)\n",
            "  Updating files:  65% (489/752)\n",
            "  Updating files:  66% (497/752)\n",
            "  Updating files:  67% (504/752)\n",
            "  Updating files:  68% (512/752)\n",
            "  Updating files:  69% (519/752)\n",
            "  Updating files:  70% (527/752)\n",
            "  Updating files:  71% (534/752)\n",
            "  Updating files:  72% (542/752)\n",
            "  Updating files:  73% (549/752)\n",
            "  Updating files:  74% (557/752)\n",
            "  Updating files:  75% (564/752)\n",
            "  Updating files:  76% (572/752)\n",
            "  Updating files:  77% (580/752)\n",
            "  Updating files:  78% (587/752)\n",
            "  Updating files:  79% (595/752)\n",
            "  Updating files:  80% (602/752)\n",
            "  Updating files:  81% (610/752)\n",
            "  Updating files:  82% (617/752)\n",
            "  Updating files:  83% (625/752)\n",
            "  Updating files:  84% (632/752)\n",
            "  Updating files:  85% (640/752)\n",
            "  Updating files:  86% (647/752)\n",
            "  Updating files:  87% (655/752)\n",
            "  Updating files:  88% (662/752)\n",
            "  Updating files:  89% (670/752)\n",
            "  Updating files:  90% (677/752)\n",
            "  Updating files:  91% (685/752)\n",
            "  Updating files:  92% (692/752)\n",
            "  Updating files:  93% (700/752)\n",
            "  Updating files:  94% (707/752)\n",
            "  Updating files:  95% (715/752)\n",
            "  Updating files:  96% (722/752)\n",
            "  Updating files:  97% (730/752)\n",
            "  Updating files:  98% (737/752)\n",
            "  Updating files:  99% (745/752)\n",
            "  Updating files: 100% (752/752)\n",
            "  Updating files: 100% (752/752), done.\n",
            "  Running command git rev-parse HEAD\n",
            "  500a3ff9ef53fafc52a01e94e1d88b1f7c502928\n",
            "  Resolved https://github.com/huggingface/diffusers.git to commit 500a3ff9ef53fafc52a01e94e1d88b1f7c502928\n",
            "  Running command git rev-parse HEAD\n",
            "  500a3ff9ef53fafc52a01e94e1d88b1f7c502928\n",
            "  Running command pip subprocess to install build dependencies\n",
            "  Using pip 23.1.2 from /usr/local/lib/python3.10/dist-packages/pip (python 3.10)\n",
            "  Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "  Collecting setuptools>=40.8.0\n",
            "    Downloading setuptools-67.8.0-py3-none-any.whl (1.1 MB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 1.1/1.1 MB 8.6 MB/s eta 0:00:00\n",
            "  Collecting wheel\n",
            "    Downloading wheel-0.40.0-py3-none-any.whl (64 kB)\n",
            "       ━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━ 64.5/64.5 kB 7.0 MB/s eta 0:00:00\n",
            "  Installing collected packages: wheel, setuptools\n",
            "    Creating /tmp/pip-build-env-bc0pwwiw/overlay/local/bin\n",
            "    changing mode of /tmp/pip-build-env-bc0pwwiw/overlay/local/bin/wheel to 755\n",
            "  ERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "  ipython 7.34.0 requires jedi>=0.16, which is not installed.\n",
            "  numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.0rc1 which is incompatible.\n",
            "  tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.0rc1 which is incompatible.\n",
            "  Successfully installed setuptools-67.8.0 wheel-0.40.0\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Getting requirements to build wheel\n",
            "  running egg_info\n",
            "  creating src/diffusers.egg-info\n",
            "  writing src/diffusers.egg-info/PKG-INFO\n",
            "  writing dependency_links to src/diffusers.egg-info/dependency_links.txt\n",
            "  writing entry points to src/diffusers.egg-info/entry_points.txt\n",
            "  writing requirements to src/diffusers.egg-info/requires.txt\n",
            "  writing top-level names to src/diffusers.egg-info/top_level.txt\n",
            "  writing manifest file 'src/diffusers.egg-info/SOURCES.txt'\n",
            "  reading manifest file 'src/diffusers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'src/diffusers.egg-info/SOURCES.txt'\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Running command Preparing metadata (pyproject.toml)\n",
            "  running dist_info\n",
            "  creating /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info\n",
            "  writing /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/PKG-INFO\n",
            "  writing dependency_links to /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/dependency_links.txt\n",
            "  writing entry points to /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/entry_points.txt\n",
            "  writing requirements to /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/requires.txt\n",
            "  writing top-level names to /tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/top_level.txt\n",
            "  writing manifest file '/tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/SOURCES.txt'\n",
            "  reading manifest file '/tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file '/tmp/pip-modern-metadata-y_xpihgo/diffusers.egg-info/SOURCES.txt'\n",
            "  creating '/tmp/pip-modern-metadata-y_xpihgo/diffusers-0.17.0.dev0.dist-info'\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting importlib-metadata (from diffusers==0.17.0.dev0)\n",
            "  Downloading importlib_metadata-6.6.0-py3-none-any.whl (22 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (3.12.0)\n",
            "Collecting huggingface-hub>=0.13.2 (from diffusers==0.17.0.dev0)\n",
            "  Downloading huggingface_hub-0.15.1-py3-none-any.whl (236 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m236.8/236.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (1.25.0rc1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (2.27.1)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from diffusers==0.17.0.dev0) (8.4.0)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (2023.4.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (4.65.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (4.5.0)\n",
            "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.2->diffusers==0.17.0.dev0) (23.1)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.10/dist-packages (from importlib-metadata->diffusers==0.17.0.dev0) (3.15.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->diffusers==0.17.0.dev0) (3.4)\n",
            "Building wheels for collected packages: diffusers\n",
            "  Running command Building wheel for diffusers (pyproject.toml)\n",
            "  running bdist_wheel\n",
            "  running build\n",
            "  running build_py\n",
            "  creating build\n",
            "  creating build/lib\n",
            "  creating build/lib/diffusers\n",
            "  copying src/diffusers/optimization.py -> build/lib/diffusers\n",
            "  copying src/diffusers/dependency_versions_table.py -> build/lib/diffusers\n",
            "  copying src/diffusers/training_utils.py -> build/lib/diffusers\n",
            "  copying src/diffusers/__init__.py -> build/lib/diffusers\n",
            "  copying src/diffusers/dependency_versions_check.py -> build/lib/diffusers\n",
            "  copying src/diffusers/image_processor.py -> build/lib/diffusers\n",
            "  copying src/diffusers/loaders.py -> build/lib/diffusers\n",
            "  copying src/diffusers/pipeline_utils.py -> build/lib/diffusers\n",
            "  copying src/diffusers/configuration_utils.py -> build/lib/diffusers\n",
            "  creating build/lib/diffusers/experimental\n",
            "  copying src/diffusers/experimental/__init__.py -> build/lib/diffusers/experimental\n",
            "  creating build/lib/diffusers/commands\n",
            "  copying src/diffusers/commands/env.py -> build/lib/diffusers/commands\n",
            "  copying src/diffusers/commands/__init__.py -> build/lib/diffusers/commands\n",
            "  copying src/diffusers/commands/diffusers_cli.py -> build/lib/diffusers/commands\n",
            "  creating build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_euler_ancestral_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_unclip.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ddpm.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_utils_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_euler_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_sde_ve_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_heun_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_dpmsolver_singlestep.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/__init__.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_sde_vp.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_pndm_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_repaint.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ddpm_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ddim_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ipndm.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ddim.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_k_dpm_2_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_dpmsolver_multistep.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_vq_diffusion.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_dpmsolver_sde.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_karras_ve.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_pndm.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_unipc_multistep.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_lms_discrete_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_utils.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_ddim_inverse.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_lms_discrete.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_deis_multistep.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_sde_ve.py -> build/lib/diffusers/schedulers\n",
            "  copying src/diffusers/schedulers/scheduling_karras_ve_flax.py -> build/lib/diffusers/schedulers\n",
            "  creating build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dynamic_modules_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_transformers_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/outputs.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/pil_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/__init__.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_note_seq_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_librosa_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/logging.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/doc_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_transformers_and_torch_and_note_seq_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_scipy_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_flax_and_transformers_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/deprecation_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/hub_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_onnx_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/testing_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/import_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/accelerate_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_flax_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/torch_utils.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_torch_and_torchsde_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/dummy_pt_objects.py -> build/lib/diffusers/utils\n",
            "  copying src/diffusers/utils/constants.py -> build/lib/diffusers/utils\n",
            "  creating build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_2d.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/controlnet.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/activations.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_2d_condition.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/attention.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_2d_blocks.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/resnet.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_2d_condition_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/attention_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/modeling_pytorch_flax_utils.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/__init__.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/vae.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/modeling_flax_utils.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/autoencoder_kl.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_1d.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_3d_condition.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/resnet_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/dual_transformer_2d.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/vae_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/controlnet_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/prior_transformer.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/attention_processor.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_1d_blocks.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_3d_blocks.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/vq_model.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/modeling_flax_pytorch_utils.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/modeling_utils.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/transformer_2d.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/cross_attention.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/t5_film_transformer.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/embeddings.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/embeddings_flax.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/transformer_temporal.py -> build/lib/diffusers/models\n",
            "  copying src/diffusers/models/unet_2d_blocks_flax.py -> build/lib/diffusers/models\n",
            "  creating build/lib/diffusers/pipelines\n",
            "  copying src/diffusers/pipelines/__init__.py -> build/lib/diffusers/pipelines\n",
            "  copying src/diffusers/pipelines/pipeline_flax_utils.py -> build/lib/diffusers/pipelines\n",
            "  copying src/diffusers/pipelines/onnx_utils.py -> build/lib/diffusers/pipelines\n",
            "  copying src/diffusers/pipelines/pipeline_utils.py -> build/lib/diffusers/pipelines\n",
            "  creating build/lib/diffusers/experimental/rl\n",
            "  copying src/diffusers/experimental/rl/__init__.py -> build/lib/diffusers/experimental/rl\n",
            "  copying src/diffusers/experimental/rl/value_guided_sampling.py -> build/lib/diffusers/experimental/rl\n",
            "  creating build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying src/diffusers/pipelines/spectrogram_diffusion/pipeline_spectrogram_diffusion.py -> build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying src/diffusers/pipelines/spectrogram_diffusion/__init__.py -> build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying src/diffusers/pipelines/spectrogram_diffusion/notes_encoder.py -> build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying src/diffusers/pipelines/spectrogram_diffusion/continous_encoder.py -> build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying src/diffusers/pipelines/spectrogram_diffusion/midi_utils.py -> build/lib/diffusers/pipelines/spectrogram_diffusion\n",
            "  creating build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_controlnet.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/safety_checker_flax.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/__init__.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_model_editing.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_diffedit.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/safety_checker.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  copying src/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py -> build/lib/diffusers/pipelines/stable_diffusion\n",
            "  creating build/lib/diffusers/pipelines/dance_diffusion\n",
            "  copying src/diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py -> build/lib/diffusers/pipelines/dance_diffusion\n",
            "  copying src/diffusers/pipelines/dance_diffusion/__init__.py -> build/lib/diffusers/pipelines/dance_diffusion\n",
            "  creating build/lib/diffusers/pipelines/dit\n",
            "  copying src/diffusers/pipelines/dit/__init__.py -> build/lib/diffusers/pipelines/dit\n",
            "  copying src/diffusers/pipelines/dit/pipeline_dit.py -> build/lib/diffusers/pipelines/dit\n",
            "  creating build/lib/diffusers/pipelines/ddim\n",
            "  copying src/diffusers/pipelines/ddim/__init__.py -> build/lib/diffusers/pipelines/ddim\n",
            "  copying src/diffusers/pipelines/ddim/pipeline_ddim.py -> build/lib/diffusers/pipelines/ddim\n",
            "  creating build/lib/diffusers/pipelines/unidiffuser\n",
            "  copying src/diffusers/pipelines/unidiffuser/__init__.py -> build/lib/diffusers/pipelines/unidiffuser\n",
            "  copying src/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py -> build/lib/diffusers/pipelines/unidiffuser\n",
            "  copying src/diffusers/pipelines/unidiffuser/modeling_text_decoder.py -> build/lib/diffusers/pipelines/unidiffuser\n",
            "  copying src/diffusers/pipelines/unidiffuser/modeling_uvit.py -> build/lib/diffusers/pipelines/unidiffuser\n",
            "  creating build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/__init__.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  copying src/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py -> build/lib/diffusers/pipelines/versatile_diffusion\n",
            "  creating build/lib/diffusers/pipelines/repaint\n",
            "  copying src/diffusers/pipelines/repaint/pipeline_repaint.py -> build/lib/diffusers/pipelines/repaint\n",
            "  copying src/diffusers/pipelines/repaint/__init__.py -> build/lib/diffusers/pipelines/repaint\n",
            "  creating build/lib/diffusers/pipelines/vq_diffusion\n",
            "  copying src/diffusers/pipelines/vq_diffusion/__init__.py -> build/lib/diffusers/pipelines/vq_diffusion\n",
            "  copying src/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py -> build/lib/diffusers/pipelines/vq_diffusion\n",
            "  creating build/lib/diffusers/pipelines/stochastic_karras_ve\n",
            "  copying src/diffusers/pipelines/stochastic_karras_ve/pipeline_stochastic_karras_ve.py -> build/lib/diffusers/pipelines/stochastic_karras_ve\n",
            "  copying src/diffusers/pipelines/stochastic_karras_ve/__init__.py -> build/lib/diffusers/pipelines/stochastic_karras_ve\n",
            "  creating build/lib/diffusers/pipelines/latent_diffusion\n",
            "  copying src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py -> build/lib/diffusers/pipelines/latent_diffusion\n",
            "  copying src/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py -> build/lib/diffusers/pipelines/latent_diffusion\n",
            "  copying src/diffusers/pipelines/latent_diffusion/__init__.py -> build/lib/diffusers/pipelines/latent_diffusion\n",
            "  creating build/lib/diffusers/pipelines/semantic_stable_diffusion\n",
            "  copying src/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py -> build/lib/diffusers/pipelines/semantic_stable_diffusion\n",
            "  copying src/diffusers/pipelines/semantic_stable_diffusion/__init__.py -> build/lib/diffusers/pipelines/semantic_stable_diffusion\n",
            "  creating build/lib/diffusers/pipelines/ddpm\n",
            "  copying src/diffusers/pipelines/ddpm/__init__.py -> build/lib/diffusers/pipelines/ddpm\n",
            "  copying src/diffusers/pipelines/ddpm/pipeline_ddpm.py -> build/lib/diffusers/pipelines/ddpm\n",
            "  creating build/lib/diffusers/pipelines/audio_diffusion\n",
            "  copying src/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py -> build/lib/diffusers/pipelines/audio_diffusion\n",
            "  copying src/diffusers/pipelines/audio_diffusion/__init__.py -> build/lib/diffusers/pipelines/audio_diffusion\n",
            "  copying src/diffusers/pipelines/audio_diffusion/mel.py -> build/lib/diffusers/pipelines/audio_diffusion\n",
            "  creating build/lib/diffusers/pipelines/latent_diffusion_uncond\n",
            "  copying src/diffusers/pipelines/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py -> build/lib/diffusers/pipelines/latent_diffusion_uncond\n",
            "  copying src/diffusers/pipelines/latent_diffusion_uncond/__init__.py -> build/lib/diffusers/pipelines/latent_diffusion_uncond\n",
            "  creating build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/watermark.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/__init__.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/timesteps.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/safety_checker.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  copying src/diffusers/pipelines/deepfloyd_if/pipeline_if.py -> build/lib/diffusers/pipelines/deepfloyd_if\n",
            "  creating build/lib/diffusers/pipelines/score_sde_ve\n",
            "  copying src/diffusers/pipelines/score_sde_ve/__init__.py -> build/lib/diffusers/pipelines/score_sde_ve\n",
            "  copying src/diffusers/pipelines/score_sde_ve/pipeline_score_sde_ve.py -> build/lib/diffusers/pipelines/score_sde_ve\n",
            "  creating build/lib/diffusers/pipelines/unclip\n",
            "  copying src/diffusers/pipelines/unclip/pipeline_unclip.py -> build/lib/diffusers/pipelines/unclip\n",
            "  copying src/diffusers/pipelines/unclip/__init__.py -> build/lib/diffusers/pipelines/unclip\n",
            "  copying src/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py -> build/lib/diffusers/pipelines/unclip\n",
            "  copying src/diffusers/pipelines/unclip/text_proj.py -> build/lib/diffusers/pipelines/unclip\n",
            "  creating build/lib/diffusers/pipelines/alt_diffusion\n",
            "  copying src/diffusers/pipelines/alt_diffusion/__init__.py -> build/lib/diffusers/pipelines/alt_diffusion\n",
            "  copying src/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py -> build/lib/diffusers/pipelines/alt_diffusion\n",
            "  copying src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py -> build/lib/diffusers/pipelines/alt_diffusion\n",
            "  copying src/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py -> build/lib/diffusers/pipelines/alt_diffusion\n",
            "  creating build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/pipeline_controlnet.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/multicontrolnet.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/__init__.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  copying src/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py -> build/lib/diffusers/pipelines/controlnet\n",
            "  creating build/lib/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying src/diffusers/pipelines/stable_diffusion_safe/__init__.py -> build/lib/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying src/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py -> build/lib/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying src/diffusers/pipelines/stable_diffusion_safe/safety_checker.py -> build/lib/diffusers/pipelines/stable_diffusion_safe\n",
            "  creating build/lib/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py -> build/lib/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying src/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py -> build/lib/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying src/diffusers/pipelines/text_to_video_synthesis/__init__.py -> build/lib/diffusers/pipelines/text_to_video_synthesis\n",
            "  creating build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/__init__.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/pipeline_kandinsky.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/text_encoder.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  copying src/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py -> build/lib/diffusers/pipelines/kandinsky\n",
            "  creating build/lib/diffusers/pipelines/pndm\n",
            "  copying src/diffusers/pipelines/pndm/__init__.py -> build/lib/diffusers/pipelines/pndm\n",
            "  copying src/diffusers/pipelines/pndm/pipeline_pndm.py -> build/lib/diffusers/pipelines/pndm\n",
            "  creating build/lib/diffusers/pipelines/audioldm\n",
            "  copying src/diffusers/pipelines/audioldm/__init__.py -> build/lib/diffusers/pipelines/audioldm\n",
            "  copying src/diffusers/pipelines/audioldm/pipeline_audioldm.py -> build/lib/diffusers/pipelines/audioldm\n",
            "  creating build/lib/diffusers/pipelines/paint_by_example\n",
            "  copying src/diffusers/pipelines/paint_by_example/__init__.py -> build/lib/diffusers/pipelines/paint_by_example\n",
            "  copying src/diffusers/pipelines/paint_by_example/image_encoder.py -> build/lib/diffusers/pipelines/paint_by_example\n",
            "  copying src/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py -> build/lib/diffusers/pipelines/paint_by_example\n",
            "  running egg_info\n",
            "  writing src/diffusers.egg-info/PKG-INFO\n",
            "  writing dependency_links to src/diffusers.egg-info/dependency_links.txt\n",
            "  writing entry points to src/diffusers.egg-info/entry_points.txt\n",
            "  writing requirements to src/diffusers.egg-info/requires.txt\n",
            "  writing top-level names to src/diffusers.egg-info/top_level.txt\n",
            "  reading manifest file 'src/diffusers.egg-info/SOURCES.txt'\n",
            "  reading manifest template 'MANIFEST.in'\n",
            "  adding license file 'LICENSE'\n",
            "  writing manifest file 'src/diffusers.egg-info/SOURCES.txt'\n",
            "  copying src/diffusers/utils/model_card_template.md -> build/lib/diffusers/utils\n",
            "  installing to build/bdist.linux-x86_64/wheel\n",
            "  running install\n",
            "  running install_lib\n",
            "  creating build/bdist.linux-x86_64\n",
            "  creating build/bdist.linux-x86_64/wheel\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/experimental\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/experimental/rl\n",
            "  copying build/lib/diffusers/experimental/rl/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/experimental/rl\n",
            "  copying build/lib/diffusers/experimental/rl/value_guided_sampling.py -> build/bdist.linux-x86_64/wheel/diffusers/experimental/rl\n",
            "  copying build/lib/diffusers/experimental/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/experimental\n",
            "  copying build/lib/diffusers/optimization.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  copying build/lib/diffusers/dependency_versions_table.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  copying build/lib/diffusers/training_utils.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  copying build/lib/diffusers/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/commands\n",
            "  copying build/lib/diffusers/commands/env.py -> build/bdist.linux-x86_64/wheel/diffusers/commands\n",
            "  copying build/lib/diffusers/commands/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/commands\n",
            "  copying build/lib/diffusers/commands/diffusers_cli.py -> build/bdist.linux-x86_64/wheel/diffusers/commands\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_euler_ancestral_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_unclip.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ddpm.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_utils_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_euler_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_sde_ve_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_heun_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_dpmsolver_singlestep.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_sde_vp.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_pndm_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_repaint.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ddpm_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ddim_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ipndm.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ddim.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_k_dpm_2_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_dpmsolver_multistep.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_vq_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_dpmsolver_sde.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_karras_ve.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_pndm.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_unipc_multistep.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_lms_discrete_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_ddim_inverse.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_lms_discrete.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_deis_multistep.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_sde_ve.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/schedulers/scheduling_karras_ve_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/schedulers\n",
            "  copying build/lib/diffusers/dependency_versions_check.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dynamic_modules_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_transformers_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/outputs.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/pil_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/model_card_template.md -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_note_seq_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_librosa_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/logging.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/doc_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_transformers_and_torch_and_note_seq_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_scipy_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_flax_and_transformers_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/deprecation_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/hub_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_onnx_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/testing_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/import_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/accelerate_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_flax_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/torch_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_torch_and_torchsde_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/dummy_pt_objects.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/utils/constants.py -> build/bdist.linux-x86_64/wheel/diffusers/utils\n",
            "  copying build/lib/diffusers/image_processor.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_2d.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/controlnet.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/activations.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_2d_condition.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/attention.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_2d_blocks.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/resnet.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_2d_condition_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/attention_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/modeling_pytorch_flax_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/vae.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/modeling_flax_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/autoencoder_kl.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_1d.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_3d_condition.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/resnet_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/dual_transformer_2d.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/vae_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/controlnet_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/prior_transformer.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/attention_processor.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_1d_blocks.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_3d_blocks.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/vq_model.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/modeling_flax_pytorch_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/modeling_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/transformer_2d.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/cross_attention.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/t5_film_transformer.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/embeddings.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/embeddings_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/transformer_temporal.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  copying build/lib/diffusers/models/unet_2d_blocks_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/models\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying build/lib/diffusers/pipelines/spectrogram_diffusion/pipeline_spectrogram_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying build/lib/diffusers/pipelines/spectrogram_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying build/lib/diffusers/pipelines/spectrogram_diffusion/notes_encoder.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying build/lib/diffusers/pipelines/spectrogram_diffusion/continous_encoder.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  copying build/lib/diffusers/pipelines/spectrogram_diffusion/midi_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/spectrogram_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_controlnet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/safety_checker_flax.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/convert_from_ckpt.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_model_editing.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_diffedit.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/safety_checker.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/dance_diffusion\n",
            "  copying build/lib/diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/dance_diffusion\n",
            "  copying build/lib/diffusers/pipelines/dance_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/dance_diffusion\n",
            "  copying build/lib/diffusers/pipelines/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/dit\n",
            "  copying build/lib/diffusers/pipelines/dit/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/dit\n",
            "  copying build/lib/diffusers/pipelines/dit/pipeline_dit.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/dit\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddim\n",
            "  copying build/lib/diffusers/pipelines/ddim/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddim\n",
            "  copying build/lib/diffusers/pipelines/ddim/pipeline_ddim.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddim\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/unidiffuser\n",
            "  copying build/lib/diffusers/pipelines/unidiffuser/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unidiffuser\n",
            "  copying build/lib/diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unidiffuser\n",
            "  copying build/lib/diffusers/pipelines/unidiffuser/modeling_text_decoder.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unidiffuser\n",
            "  copying build/lib/diffusers/pipelines/unidiffuser/modeling_uvit.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unidiffuser\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/modeling_text_unet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  copying build/lib/diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/versatile_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/repaint\n",
            "  copying build/lib/diffusers/pipelines/repaint/pipeline_repaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/repaint\n",
            "  copying build/lib/diffusers/pipelines/repaint/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/repaint\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/vq_diffusion\n",
            "  copying build/lib/diffusers/pipelines/vq_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/vq_diffusion\n",
            "  copying build/lib/diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/vq_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/stochastic_karras_ve\n",
            "  copying build/lib/diffusers/pipelines/stochastic_karras_ve/pipeline_stochastic_karras_ve.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stochastic_karras_ve\n",
            "  copying build/lib/diffusers/pipelines/stochastic_karras_ve/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stochastic_karras_ve\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion\n",
            "  copying build/lib/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion\n",
            "  copying build/lib/diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion\n",
            "  copying build/lib/diffusers/pipelines/latent_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/semantic_stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/semantic_stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/semantic_stable_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/semantic_stable_diffusion\n",
            "  copying build/lib/diffusers/pipelines/pipeline_flax_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines\n",
            "  copying build/lib/diffusers/pipelines/onnx_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddpm\n",
            "  copying build/lib/diffusers/pipelines/ddpm/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddpm\n",
            "  copying build/lib/diffusers/pipelines/ddpm/pipeline_ddpm.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/ddpm\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/audio_diffusion\n",
            "  copying build/lib/diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/audio_diffusion\n",
            "  copying build/lib/diffusers/pipelines/audio_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/audio_diffusion\n",
            "  copying build/lib/diffusers/pipelines/audio_diffusion/mel.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/audio_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion_uncond\n",
            "  copying build/lib/diffusers/pipelines/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion_uncond\n",
            "  copying build/lib/diffusers/pipelines/latent_diffusion_uncond/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/latent_diffusion_uncond\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/watermark.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/timesteps.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/safety_checker.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  copying build/lib/diffusers/pipelines/deepfloyd_if/pipeline_if.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/deepfloyd_if\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/score_sde_ve\n",
            "  copying build/lib/diffusers/pipelines/score_sde_ve/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/score_sde_ve\n",
            "  copying build/lib/diffusers/pipelines/score_sde_ve/pipeline_score_sde_ve.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/score_sde_ve\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/unclip\n",
            "  copying build/lib/diffusers/pipelines/unclip/pipeline_unclip.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unclip\n",
            "  copying build/lib/diffusers/pipelines/unclip/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unclip\n",
            "  copying build/lib/diffusers/pipelines/unclip/pipeline_unclip_image_variation.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unclip\n",
            "  copying build/lib/diffusers/pipelines/unclip/text_proj.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/unclip\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/alt_diffusion\n",
            "  copying build/lib/diffusers/pipelines/alt_diffusion/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/alt_diffusion\n",
            "  copying build/lib/diffusers/pipelines/alt_diffusion/modeling_roberta_series.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/alt_diffusion\n",
            "  copying build/lib/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/alt_diffusion\n",
            "  copying build/lib/diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/alt_diffusion\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/pipeline_controlnet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/multicontrolnet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/pipeline_flax_controlnet.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  copying build/lib/diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/controlnet\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion_safe/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying build/lib/diffusers/pipelines/stable_diffusion_safe/safety_checker.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/stable_diffusion_safe\n",
            "  copying build/lib/diffusers/pipelines/pipeline_utils.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying build/lib/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying build/lib/diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/text_to_video_synthesis\n",
            "  copying build/lib/diffusers/pipelines/text_to_video_synthesis/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/text_to_video_synthesis\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/pipeline_kandinsky.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/text_encoder.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  copying build/lib/diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/kandinsky\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/pndm\n",
            "  copying build/lib/diffusers/pipelines/pndm/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/pndm\n",
            "  copying build/lib/diffusers/pipelines/pndm/pipeline_pndm.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/pndm\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/audioldm\n",
            "  copying build/lib/diffusers/pipelines/audioldm/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/audioldm\n",
            "  copying build/lib/diffusers/pipelines/audioldm/pipeline_audioldm.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/audioldm\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers/pipelines/paint_by_example\n",
            "  copying build/lib/diffusers/pipelines/paint_by_example/__init__.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/paint_by_example\n",
            "  copying build/lib/diffusers/pipelines/paint_by_example/image_encoder.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/paint_by_example\n",
            "  copying build/lib/diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py -> build/bdist.linux-x86_64/wheel/diffusers/pipelines/paint_by_example\n",
            "  copying build/lib/diffusers/loaders.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  copying build/lib/diffusers/pipeline_utils.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  copying build/lib/diffusers/configuration_utils.py -> build/bdist.linux-x86_64/wheel/diffusers\n",
            "  running install_egg_info\n",
            "  Copying src/diffusers.egg-info to build/bdist.linux-x86_64/wheel/diffusers-0.17.0.dev0-py3.10.egg-info\n",
            "  running install_scripts\n",
            "  creating build/bdist.linux-x86_64/wheel/diffusers-0.17.0.dev0.dist-info/WHEEL\n",
            "  creating '/tmp/pip-wheel-fmiqb9pt/.tmp-01t581u2/diffusers-0.17.0.dev0-py3-none-any.whl' and adding 'build/bdist.linux-x86_64/wheel' to it\n",
            "  adding 'diffusers/__init__.py'\n",
            "  adding 'diffusers/configuration_utils.py'\n",
            "  adding 'diffusers/dependency_versions_check.py'\n",
            "  adding 'diffusers/dependency_versions_table.py'\n",
            "  adding 'diffusers/image_processor.py'\n",
            "  adding 'diffusers/loaders.py'\n",
            "  adding 'diffusers/optimization.py'\n",
            "  adding 'diffusers/pipeline_utils.py'\n",
            "  adding 'diffusers/training_utils.py'\n",
            "  adding 'diffusers/commands/__init__.py'\n",
            "  adding 'diffusers/commands/diffusers_cli.py'\n",
            "  adding 'diffusers/commands/env.py'\n",
            "  adding 'diffusers/experimental/__init__.py'\n",
            "  adding 'diffusers/experimental/rl/__init__.py'\n",
            "  adding 'diffusers/experimental/rl/value_guided_sampling.py'\n",
            "  adding 'diffusers/models/__init__.py'\n",
            "  adding 'diffusers/models/activations.py'\n",
            "  adding 'diffusers/models/attention.py'\n",
            "  adding 'diffusers/models/attention_flax.py'\n",
            "  adding 'diffusers/models/attention_processor.py'\n",
            "  adding 'diffusers/models/autoencoder_kl.py'\n",
            "  adding 'diffusers/models/controlnet.py'\n",
            "  adding 'diffusers/models/controlnet_flax.py'\n",
            "  adding 'diffusers/models/cross_attention.py'\n",
            "  adding 'diffusers/models/dual_transformer_2d.py'\n",
            "  adding 'diffusers/models/embeddings.py'\n",
            "  adding 'diffusers/models/embeddings_flax.py'\n",
            "  adding 'diffusers/models/modeling_flax_pytorch_utils.py'\n",
            "  adding 'diffusers/models/modeling_flax_utils.py'\n",
            "  adding 'diffusers/models/modeling_pytorch_flax_utils.py'\n",
            "  adding 'diffusers/models/modeling_utils.py'\n",
            "  adding 'diffusers/models/prior_transformer.py'\n",
            "  adding 'diffusers/models/resnet.py'\n",
            "  adding 'diffusers/models/resnet_flax.py'\n",
            "  adding 'diffusers/models/t5_film_transformer.py'\n",
            "  adding 'diffusers/models/transformer_2d.py'\n",
            "  adding 'diffusers/models/transformer_temporal.py'\n",
            "  adding 'diffusers/models/unet_1d.py'\n",
            "  adding 'diffusers/models/unet_1d_blocks.py'\n",
            "  adding 'diffusers/models/unet_2d.py'\n",
            "  adding 'diffusers/models/unet_2d_blocks.py'\n",
            "  adding 'diffusers/models/unet_2d_blocks_flax.py'\n",
            "  adding 'diffusers/models/unet_2d_condition.py'\n",
            "  adding 'diffusers/models/unet_2d_condition_flax.py'\n",
            "  adding 'diffusers/models/unet_3d_blocks.py'\n",
            "  adding 'diffusers/models/unet_3d_condition.py'\n",
            "  adding 'diffusers/models/vae.py'\n",
            "  adding 'diffusers/models/vae_flax.py'\n",
            "  adding 'diffusers/models/vq_model.py'\n",
            "  adding 'diffusers/pipelines/__init__.py'\n",
            "  adding 'diffusers/pipelines/onnx_utils.py'\n",
            "  adding 'diffusers/pipelines/pipeline_flax_utils.py'\n",
            "  adding 'diffusers/pipelines/pipeline_utils.py'\n",
            "  adding 'diffusers/pipelines/alt_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/alt_diffusion/modeling_roberta_series.py'\n",
            "  adding 'diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion.py'\n",
            "  adding 'diffusers/pipelines/alt_diffusion/pipeline_alt_diffusion_img2img.py'\n",
            "  adding 'diffusers/pipelines/audio_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/audio_diffusion/mel.py'\n",
            "  adding 'diffusers/pipelines/audio_diffusion/pipeline_audio_diffusion.py'\n",
            "  adding 'diffusers/pipelines/audioldm/__init__.py'\n",
            "  adding 'diffusers/pipelines/audioldm/pipeline_audioldm.py'\n",
            "  adding 'diffusers/pipelines/controlnet/__init__.py'\n",
            "  adding 'diffusers/pipelines/controlnet/multicontrolnet.py'\n",
            "  adding 'diffusers/pipelines/controlnet/pipeline_controlnet.py'\n",
            "  adding 'diffusers/pipelines/controlnet/pipeline_controlnet_img2img.py'\n",
            "  adding 'diffusers/pipelines/controlnet/pipeline_controlnet_inpaint.py'\n",
            "  adding 'diffusers/pipelines/controlnet/pipeline_flax_controlnet.py'\n",
            "  adding 'diffusers/pipelines/dance_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/dance_diffusion/pipeline_dance_diffusion.py'\n",
            "  adding 'diffusers/pipelines/ddim/__init__.py'\n",
            "  adding 'diffusers/pipelines/ddim/pipeline_ddim.py'\n",
            "  adding 'diffusers/pipelines/ddpm/__init__.py'\n",
            "  adding 'diffusers/pipelines/ddpm/pipeline_ddpm.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/__init__.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if_img2img.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if_img2img_superresolution.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if_inpainting_superresolution.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/pipeline_if_superresolution.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/safety_checker.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/timesteps.py'\n",
            "  adding 'diffusers/pipelines/deepfloyd_if/watermark.py'\n",
            "  adding 'diffusers/pipelines/dit/__init__.py'\n",
            "  adding 'diffusers/pipelines/dit/pipeline_dit.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/__init__.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/pipeline_kandinsky.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/pipeline_kandinsky_img2img.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/pipeline_kandinsky_inpaint.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/pipeline_kandinsky_prior.py'\n",
            "  adding 'diffusers/pipelines/kandinsky/text_encoder.py'\n",
            "  adding 'diffusers/pipelines/latent_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion.py'\n",
            "  adding 'diffusers/pipelines/latent_diffusion/pipeline_latent_diffusion_superresolution.py'\n",
            "  adding 'diffusers/pipelines/latent_diffusion_uncond/__init__.py'\n",
            "  adding 'diffusers/pipelines/latent_diffusion_uncond/pipeline_latent_diffusion_uncond.py'\n",
            "  adding 'diffusers/pipelines/paint_by_example/__init__.py'\n",
            "  adding 'diffusers/pipelines/paint_by_example/image_encoder.py'\n",
            "  adding 'diffusers/pipelines/paint_by_example/pipeline_paint_by_example.py'\n",
            "  adding 'diffusers/pipelines/pndm/__init__.py'\n",
            "  adding 'diffusers/pipelines/pndm/pipeline_pndm.py'\n",
            "  adding 'diffusers/pipelines/repaint/__init__.py'\n",
            "  adding 'diffusers/pipelines/repaint/pipeline_repaint.py'\n",
            "  adding 'diffusers/pipelines/score_sde_ve/__init__.py'\n",
            "  adding 'diffusers/pipelines/score_sde_ve/pipeline_score_sde_ve.py'\n",
            "  adding 'diffusers/pipelines/semantic_stable_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/semantic_stable_diffusion/pipeline_semantic_stable_diffusion.py'\n",
            "  adding 'diffusers/pipelines/spectrogram_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/spectrogram_diffusion/continous_encoder.py'\n",
            "  adding 'diffusers/pipelines/spectrogram_diffusion/midi_utils.py'\n",
            "  adding 'diffusers/pipelines/spectrogram_diffusion/notes_encoder.py'\n",
            "  adding 'diffusers/pipelines/spectrogram_diffusion/pipeline_spectrogram_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/convert_from_ckpt.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_cycle_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_controlnet.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_img2img.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_flax_stable_diffusion_inpaint.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_img2img.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_inpaint_legacy.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_onnx_stable_diffusion_upscale.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_attend_and_excite.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_controlnet.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_depth2img.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_diffedit.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_image_variation.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_img2img.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_inpaint_legacy.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_instruct_pix2pix.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_k_diffusion.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_latent_upscale.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_model_editing.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_panorama.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_pix2pix_zero.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_sag.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_diffusion_upscale.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_unclip.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/pipeline_stable_unclip_img2img.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/safety_checker.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/safety_checker_flax.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion/stable_unclip_image_normalizer.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion_safe/__init__.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion_safe/pipeline_stable_diffusion_safe.py'\n",
            "  adding 'diffusers/pipelines/stable_diffusion_safe/safety_checker.py'\n",
            "  adding 'diffusers/pipelines/stochastic_karras_ve/__init__.py'\n",
            "  adding 'diffusers/pipelines/stochastic_karras_ve/pipeline_stochastic_karras_ve.py'\n",
            "  adding 'diffusers/pipelines/text_to_video_synthesis/__init__.py'\n",
            "  adding 'diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_synth.py'\n",
            "  adding 'diffusers/pipelines/text_to_video_synthesis/pipeline_text_to_video_zero.py'\n",
            "  adding 'diffusers/pipelines/unclip/__init__.py'\n",
            "  adding 'diffusers/pipelines/unclip/pipeline_unclip.py'\n",
            "  adding 'diffusers/pipelines/unclip/pipeline_unclip_image_variation.py'\n",
            "  adding 'diffusers/pipelines/unclip/text_proj.py'\n",
            "  adding 'diffusers/pipelines/unidiffuser/__init__.py'\n",
            "  adding 'diffusers/pipelines/unidiffuser/modeling_text_decoder.py'\n",
            "  adding 'diffusers/pipelines/unidiffuser/modeling_uvit.py'\n",
            "  adding 'diffusers/pipelines/unidiffuser/pipeline_unidiffuser.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/modeling_text_unet.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_dual_guided.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_image_variation.py'\n",
            "  adding 'diffusers/pipelines/versatile_diffusion/pipeline_versatile_diffusion_text_to_image.py'\n",
            "  adding 'diffusers/pipelines/vq_diffusion/__init__.py'\n",
            "  adding 'diffusers/pipelines/vq_diffusion/pipeline_vq_diffusion.py'\n",
            "  adding 'diffusers/schedulers/__init__.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ddim.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ddim_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ddim_inverse.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ddpm.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ddpm_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_deis_multistep.py'\n",
            "  adding 'diffusers/schedulers/scheduling_dpmsolver_multistep.py'\n",
            "  adding 'diffusers/schedulers/scheduling_dpmsolver_multistep_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_dpmsolver_multistep_inverse.py'\n",
            "  adding 'diffusers/schedulers/scheduling_dpmsolver_sde.py'\n",
            "  adding 'diffusers/schedulers/scheduling_dpmsolver_singlestep.py'\n",
            "  adding 'diffusers/schedulers/scheduling_euler_ancestral_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_euler_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_heun_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_ipndm.py'\n",
            "  adding 'diffusers/schedulers/scheduling_k_dpm_2_ancestral_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_k_dpm_2_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_karras_ve.py'\n",
            "  adding 'diffusers/schedulers/scheduling_karras_ve_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_lms_discrete.py'\n",
            "  adding 'diffusers/schedulers/scheduling_lms_discrete_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_pndm.py'\n",
            "  adding 'diffusers/schedulers/scheduling_pndm_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_repaint.py'\n",
            "  adding 'diffusers/schedulers/scheduling_sde_ve.py'\n",
            "  adding 'diffusers/schedulers/scheduling_sde_ve_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_sde_vp.py'\n",
            "  adding 'diffusers/schedulers/scheduling_unclip.py'\n",
            "  adding 'diffusers/schedulers/scheduling_unipc_multistep.py'\n",
            "  adding 'diffusers/schedulers/scheduling_utils.py'\n",
            "  adding 'diffusers/schedulers/scheduling_utils_flax.py'\n",
            "  adding 'diffusers/schedulers/scheduling_vq_diffusion.py'\n",
            "  adding 'diffusers/utils/__init__.py'\n",
            "  adding 'diffusers/utils/accelerate_utils.py'\n",
            "  adding 'diffusers/utils/constants.py'\n",
            "  adding 'diffusers/utils/deprecation_utils.py'\n",
            "  adding 'diffusers/utils/doc_utils.py'\n",
            "  adding 'diffusers/utils/dummy_flax_and_transformers_objects.py'\n",
            "  adding 'diffusers/utils/dummy_flax_objects.py'\n",
            "  adding 'diffusers/utils/dummy_note_seq_objects.py'\n",
            "  adding 'diffusers/utils/dummy_onnx_objects.py'\n",
            "  adding 'diffusers/utils/dummy_pt_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_librosa_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_scipy_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_torchsde_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_transformers_and_k_diffusion_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_transformers_and_onnx_objects.py'\n",
            "  adding 'diffusers/utils/dummy_torch_and_transformers_objects.py'\n",
            "  adding 'diffusers/utils/dummy_transformers_and_torch_and_note_seq_objects.py'\n",
            "  adding 'diffusers/utils/dynamic_modules_utils.py'\n",
            "  adding 'diffusers/utils/hub_utils.py'\n",
            "  adding 'diffusers/utils/import_utils.py'\n",
            "  adding 'diffusers/utils/logging.py'\n",
            "  adding 'diffusers/utils/model_card_template.md'\n",
            "  adding 'diffusers/utils/outputs.py'\n",
            "  adding 'diffusers/utils/pil_utils.py'\n",
            "  adding 'diffusers/utils/testing_utils.py'\n",
            "  adding 'diffusers/utils/torch_utils.py'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/LICENSE'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/METADATA'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/WHEEL'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/entry_points.txt'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/top_level.txt'\n",
            "  adding 'diffusers-0.17.0.dev0.dist-info/RECORD'\n",
            "  removing build/bdist.linux-x86_64/wheel\n",
            "  Building wheel for diffusers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffusers: filename=diffusers-0.17.0.dev0-py3-none-any.whl size=1081842 sha256=83b88fe3b4cd596b31378deffcf265cdb9913ccc2e3d8e2ab98b19f617eaf915\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-7xsxcq7a/wheels/4d/b7/a8/6f9549ceec5daad78675b857ac57d697c387062506520a7b50\n",
            "Successfully built diffusers\n",
            "Installing collected packages: importlib-metadata, huggingface-hub, diffusers\n",
            "  changing mode of /usr/local/bin/huggingface-cli to 755\n",
            "  changing mode of /usr/local/bin/diffusers-cli to 755\n",
            "Successfully installed diffusers-0.17.0.dev0 huggingface-hub-0.15.1 importlib-metadata-6.6.0\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (1.25.0rc1)\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.29.2-py3-none-any.whl (7.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.1/7.1 MB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting accelerate\n",
            "  Downloading accelerate-0.20.0-py3-none-any.whl (227 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m227.4/227.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting xformers\n",
            "  Downloading xformers-0.0.21.dev544-cp310-cp310-manylinux2014_x86_64.whl (109.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m109.1/109.1 MB\u001b[0m \u001b[31m6.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.12.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.14.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.1)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (23.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2022.10.31)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.27.1)\n",
            "Collecting tokenizers!=0.11.3,<0.14,>=0.11.1 (from transformers)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.65.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: torch>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.0.1+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.6.0->accelerate) (2.0.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch>=1.6.0->accelerate) (16.0.5)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.14.1->transformers) (2023.4.0)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (1.26.15)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.12)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.6.0->accelerate) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.6.0->accelerate) (1.3.0)\n",
            "Installing collected packages: tokenizers, transformers, xformers, accelerate\n",
            "Successfully installed accelerate-0.20.0 tokenizers-0.13.3 transformers-4.29.2 xformers-0.0.21.dev544\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Check torch version\n",
        "import torch\n",
        "print(torch.__version__)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d4bbguBhPKJp",
        "outputId": "23de8637-ae72-417e-f87c-bc68cd488b4b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2.0.1+cu118\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import hidet\n",
        "import diffusers\n",
        "from diffusers import StableDiffusionPipeline\n",
        "import torch\n",
        "import torch._dynamo\n",
        "\n",
        "\n",
        "#Create pipeline\n",
        "pipe = StableDiffusionPipeline.from_pretrained(\"runwayml/stable-diffusion-v1-5\", torch_dtype=torch.float16)\n",
        "pipe = pipe.to(\"cuda\")\n",
        "#Make sure the unet is contiguous\n",
        "pipe.unet = pipe.unet.to(memory_format=torch.contiguous_format)\n",
        "\n",
        "\n",
        "#Some typical diffusers pipeline optimizations\n",
        "# pipe.unet.to(memory_format=torch.channels_last) #Unsupported by hidet, but does not seem to make a difference if disabled.\n",
        "#pipe.enable_vae_tiling()\n",
        "#pipe.enable_xformers_memory_efficient_attention()\n",
        "#\n",
        "#print(\"### Post optimization benchmark:\")\n",
        "image = pipe(\"a photo of an astronaut riding a horse on mars\").images[0]\n",
        "\n",
        "\n",
        "hidet.option.parallel_build(True)\n",
        "torch._dynamo.config.suppress_errors = True\n",
        "torch._dynamo.disallow_in_graph(diffusers.models.attention.BasicTransformerBlock)\n",
        "\n",
        "# more search\n",
        "hidet.torch.dynamo_config.search_space(0)\n",
        "#hidet.torch.dynamo_config.dump_graph_ir(\"./local_graph\")\n",
        "hidet.option.cache_dir(\"local_cache\")\n",
        "# automatically transform the model to use float16 data type\n",
        "hidet.torch.dynamo_config.use_fp16(True)\n",
        "# use float16 data type as the accumulate data type in operators with reduction\n",
        "hidet.torch.dynamo_config.use_fp16_reduction(True)\n",
        "# use tensorcore\n",
        "hidet.torch.dynamo_config.use_tensor_core()\n",
        "pipe.unet = torch.compile(pipe.unet, backend=\"hidet\")\n",
        "\n",
        "print(\"### torch.compile warmup:\")\n",
        "image = pipe(\"a photo of an astronaut riding a horse on mars\").images[0]\n",
        "\n",
        "print(\"torch.compile benchmark:\")\n",
        "image = pipe(\"a photo of an astronaut riding a horse on mars\").images[0]"
      ],
      "metadata": {
        "id": "xS4E0sdc8kxK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "cebb6ff7f9854be18ab45631532bb245",
            "f988a8b1c8214124aa1812aac60ffeab",
            "279fb451b8e84e619c17c1d807fac9c3",
            "39ed643430694988bc045e26ae2ca096",
            "e0016215a25e44bb9b341d7bf9a84471",
            "de458e35f6d44c969eb897be742301cd",
            "7b8fb10274e84fb2be7c106b53db209d",
            "f32a7d6e2b26441485b304379d17550c",
            "4e8c1d48c4644503a37282f9627956b4",
            "28d224a69cae483fa4fcd26fc1be3c8e",
            "f754a54ef2a44affa1cff5056b4b6e5c",
            "aaa807afeb7e43febaede286aaa18f70",
            "90c41a76bda545d1aaa91272e8fc3b86",
            "2da67c92167940018e6cf8028d17d9c0",
            "9c75c3abe6fb408db841a41eb4707807",
            "4bf245d5e776446bb9d2fc03f2d3dcea",
            "b6bbb54abe7548148a598413f27dba60",
            "98f94a81fb6840d992ee4e07a88c3b53",
            "34192a11df0843ef8d5ace02428743f5",
            "842ad49d22194a91804a6d29f5edb07f",
            "4e2a0565ea014e87b33ef007074db3cc",
            "bd6b95bb0c1e434382a22ab5283ba652",
            "700b4ddcf5db412bbed90f83a940cf01",
            "72213452182b423a9fa7c8f677d42e72",
            "d3e138eec60a4e5c924c4edbb6d67f50",
            "e33982fc0dd3447b9922f2b112648f3d",
            "0370e94056de4e46877609396fff5aee",
            "601af93f10d449a39621a2b2e1c1859f",
            "5ff25b47591a4d62a3c89fd8513271cd",
            "dff424c444be4f968fe6a472f7a93311",
            "354b8c92c8884af5b9620518a22629ab",
            "7719b8eff9bc4927abb537b7a3bcd813",
            "c385d79ccd6546b986504cb21df23340",
            "00a6d1275a0d487fa9b57e352a200ee1",
            "48dfd07c9c28490d851df24cb122211d",
            "1979d729156d458596d72bf98516d63d",
            "56ddfa4e878e47859af912dc387ac79e",
            "0332fa6047b541f1b7d80325c8e595b0",
            "66bbafa4722746b8a968b2d60e632bda",
            "0cbb4d831a3443f2b182a7bd530694d2",
            "8771e9dde25c4ab09dc7185260809121",
            "b487e1721fa0431a871e226dbbbcdf45",
            "518fe020b0f14e599113419f09da1db7",
            "b8b372f0ccf34db095a2f144fd6faa34",
            "1deab4a8ddc34286a8e574409cf95dde",
            "2d5c4771450442a881c4def31fbbdf0b",
            "97fcde821600472ea087146889787952",
            "7194a3c97ce048789a10be37f9e8eb22",
            "0ef6b91f0c6c4a938469f9a5ea1b39b6",
            "2b2960d912d447cf9a5f10fb4275693f",
            "f44eea2837d8450da09bda5eb0e043f5",
            "b57820b76c7d4623a1a41cd031fa6b65",
            "96bfeb7e0e8543959b2b02fb8f63798c",
            "b9bfa777aff047ccad6755cecf849a51",
            "2c1ae4474645473f9dcb48a6f895a6f0",
            "b2216dff695f4c92a876205e9197b122",
            "094f252598b94fbc88e1d89456fc3633",
            "4e97dc8beeed47fca16db734ae82e07b",
            "7a6122765eb6411f9161e22614f85747",
            "ebe1cb6dc3794ec3a1f578fc8662311a",
            "d050957797094724ab250b1fe6f8b76b",
            "2182a6fc824c4f5ebea2beb31084a124",
            "3eecf86e29474bb6b3d821e17ec4eaad",
            "6e84568e2458472cb1ad3681df5d0a16",
            "2aed0c110aef42f380eb64b8f8493338",
            "6a2fcb5e6f4a4e9db1c9978e57935810",
            "8e22ca70e8074c5f9d8977d6ea93f3f6",
            "fb189aa715bb47aa9ccd6a08a3b9d287",
            "09278e7da45f4e5e8752b38d8787bd73",
            "655dfc04df6840d8bd15b08450c08a8f",
            "0686e2ed209d4f3da85f1cd5f2fd370b",
            "8fc7fdb1c7e34de993670ed66adbbe75",
            "d284547b49644053aa813d5cfcf0c706",
            "362fe019d991460299d5cd29e14addc9",
            "192e152d25694245b39b137fd4ce5973",
            "c95e919a26534adfa7fe866934a82b89",
            "43e101b3e126461898da489463f457c4",
            "7c6f07e805ef4ce9a9d74bbb2b008cab",
            "1011765c22e9404fb61b5b1d2ecf6068",
            "f9203e5200e3474ea21e845a9db37299",
            "9392ed228dcc43669ef19c05013bc606",
            "145d440c3ab245f78f9cad216db0a21a",
            "6712acf9a30a480d9a71432e4b2bdcd6",
            "7ffa816771ed4b868f011e8b66019e85",
            "e26a77ffc3d84ab6ac24fffa104561ac",
            "20b9db4587424003a225855940d728c1",
            "af99804eea7544b9855fa0d88243c8a4",
            "c4ad3ddb7e424916abc0d4a1a44ca87d",
            "5d8a48a728bc4009bc8d29033324faf9",
            "59965090690747608bd2095c33c5f54f",
            "6de84956be8b460ea4547c6a08e69032",
            "dcb0f135674a4767981b0d51b915ed3c",
            "7474c6b13db742219f8b9fae3ca8e5e9",
            "6c8198731bcf441ab40ba8676e38e5fb",
            "3a237f4c05284d8b8a48155c2d0cc7df",
            "48a82e73db1e40d48d133010e65bea45",
            "a040e05765244590836507066b86cde5",
            "8c409d6229734866b04dd85865313f55",
            "d247f50f18db4cb5a084b9dbbaf4ea60",
            "9d89531727054bf595f29bf96574593e",
            "d8f516ce4e4245d8962f204c764d5d4e",
            "3a5950e92d2146628c1572ccbeef3645",
            "25c1fa3179b74a4593398755040fc4d4",
            "7a0bc9700e3f48df91f4d3bfcef04ef3",
            "619eb5eadd784bfe9aa9b894b650b454",
            "8a25221d527e4892a747a86a20669eed",
            "589c4d5195c7488eb1c051164ac24039",
            "8c9b917e9ae74cf69e729dcb584c5610",
            "715bd7bd8fd040db87f50e6003d6ef5c",
            "0d53a6118de74e7faa360bb63f3ae7ab",
            "32d546a8801843fd930b265ce53fe7da",
            "633626951197482bb60b9db58cc1821c",
            "87c8fc867cff4091a2e8a1a667962d2f",
            "752ed006d29742819ca0540301688cc1",
            "94d1164cb9db4b7c91441388e810e102",
            "4b1c9ab48fa84f708f6cec2d806275f1",
            "648d5eb18d254b7685a4789fc20201b0",
            "ecb4ca8b11ea435f96b66401681b91a8",
            "44a2178eee0f4a74959658da86682462",
            "68da365b26864d368c526faedeb394c0",
            "5f0fa895708e4793b56458d3958b8995",
            "19712264493744e69242b80aba9350c1",
            "3f2bfec010f546579f1dd112482ef47e",
            "efbeeacabbc34349a03c3e65bb2a707f",
            "8b7971b09e504e1f92e3ee8b3c0b3101",
            "4cf4f4ca07e549e480e196293e83723d",
            "367bbe72b45a45dc947bce8a74354e87",
            "4696c98ca7db46ac8e38a0f672bb4186",
            "6cfdb583e3224618bdb7c38f79d38338",
            "8dee36c8906f4dd89a39d49092eeb139",
            "1e29e1c32833496b818e04903a439738",
            "afa953817100431e95b43023683e706e",
            "c7e6459b81434c908d86194eef083f30",
            "2fb2a053a7fe4b489a6672a2ec90c0c8",
            "5ecf4b2bddbf4e9e91d26c3da33ac314",
            "f704b8ca34a04b95b53e17aa586789ff",
            "b90c9b68748f441da87d042a4b2db808",
            "e515314b2528479c9ee84d66507760b7",
            "f49af0de7764491bbf061520ca46a2c3",
            "cc29bdf1568f43d4bce808cacaab4cbc",
            "b5d8a4cc9a0f4ed4853b3f4c6436ae3b",
            "7b139b5f9c6e4d258ede4bee6a6f5d10",
            "867442b9ec974644aa0176ffb244cd05",
            "28116b65548446808cfd1f247d5f3310",
            "470713a6ea3643f5baf537722eadd740",
            "149f24fa6c84463b9210078e1006366a",
            "ce2e836ceea644bf92ef78baf5494a94",
            "cd6f1d9541594bfeacb7e32e4e9de7b1",
            "094ad3c0b3444107b642d0c065734bbd",
            "2042c95befc742c79f8065aad1e77265",
            "13639ede86f94c15b336504a30e29a9c",
            "ac42116dc32e4fba80a8b7b361518573",
            "2b3e710398db47ba9eef708e68cc65a0",
            "7bf68d30f36c4674a548b62b63e5fefe",
            "31e11e7961a8476c8f01aa34f3efb850",
            "bfaa3abca1234dd29fd9fdf5bd9d9d48",
            "d153917a48984991b4f60a23aa4e6eaa",
            "5f4bba9d674244ad94c41c20f7a1c8b0",
            "cc01e46517e646f3a0d3ce1ae8510d04",
            "9728e1bd20464055ac10c7a54571fa18",
            "e307ff835ba34787854697af186380bb",
            "002ac38a6a6d4f888608989c0b973ec5",
            "f77b94d8b704463d97389e42746cd6dd",
            "6e4f93c0eaa44198adbdd8a3cecaf7df",
            "731e1d3785904700888a170bf771d70f",
            "97ecc262cfe2405f8526beadc394c9f1",
            "8f5a0ddfdd9841f6abaa380a738c2e37",
            "d055cf30f6f34962a3b867af89efdc4b",
            "f612b25683654ed596af74c4febf5060",
            "d54791ba84d14f8fb5cc154f1bb6297d",
            "66a998ff2d0548cf97c9e046e4a1fde9",
            "6ab1392947934c78b1482629c298fa1b",
            "1360611c64144103b4034ad769785792",
            "80d9c0c036bb4d659a47b176c1c44eb0",
            "e930c17235c94733971f5a7e55808c37",
            "5aa636cdf24c491cb86a62f8b41ad3b7",
            "5d912348ae64466a9dac3bc6f9b202d6",
            "8fb385cd1a804275a05e113ec9d38080",
            "9999e0c4944a453b869117117abdeab2",
            "214a407cec084678b1e1d2fd55150639",
            "210f8e2eafea4de8bbf79149cdb10982",
            "fb323df4e2b9413aab02464a3b91667a",
            "4938eaa2e6f641f39941ceb22d2e842a",
            "be3b8ab1ccd941dc9fe8713b77c1d1a8",
            "b4fd476bb1754223b48d339a053713d5",
            "a60d4681a9e24819974d45982ff02b87",
            "6906523f79db47d3a8a90bbbff594127",
            "40d102c2aeb64df7b7f9f4f500dc9c80",
            "0bf1f963ddd3466291e90a12cdd0740f",
            "9ef38652377246eb97ef986902b9d260",
            "18429a36b02d4fcbb3dba6dbc71b1bd7",
            "73ad556ec14d4685a17945d1545beaee",
            "6b2345295aff4d0f86ef5967590135ac",
            "b6f7a3b23e1b4b3d838c23d2ea0c4733",
            "9cc7d6a18c444318bec77016ba49ba75",
            "200c75379ba04656866437a5790bcdd4",
            "8728c3632de046fb9d3a6cd7fa13ad14",
            "34e52035789c4b568f42d680cf4cad2e"
          ]
        },
        "outputId": "1e4001b8-b373-4b4e-8dfb-09312e7ad3c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)ain/model_index.json:   0%|          | 0.00/541 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "cebb6ff7f9854be18ab45631532bb245"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "aaa807afeb7e43febaede286aaa18f70"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)rocessor_config.json:   0%|          | 0.00/342 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "700b4ddcf5db412bbed90f83a940cf01"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cheduler_config.json:   0%|          | 0.00/308 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "00a6d1275a0d487fa9b57e352a200ee1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)cial_tokens_map.json:   0%|          | 0.00/472 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "1deab4a8ddc34286a8e574409cf95dde"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_checker/config.json:   0%|          | 0.00/4.72k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "b2216dff695f4c92a876205e9197b122"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)_encoder/config.json:   0%|          | 0.00/617 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8e22ca70e8074c5f9d8977d6ea93f3f6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/1.22G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "7c6f07e805ef4ce9a9d74bbb2b008cab"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d8a48a728bc4009bc8d29033324faf9"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading pytorch_model.bin:   0%|          | 0.00/492M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9d89531727054bf595f29bf96574593e"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)okenizer_config.json:   0%|          | 0.00/806 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "32d546a8801843fd930b265ce53fe7da"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)e6a/unet/config.json:   0%|          | 0.00/743 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "19712264493744e69242b80aba9350c1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)8e6a/vae/config.json:   0%|          | 0.00/547 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "c7e6459b81434c908d86194eef083f30"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "28116b65548446808cfd1f247d5f3310"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/3.44G [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "31e11e7961a8476c8f01aa34f3efb850"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading (…)on_pytorch_model.bin:   0%|          | 0.00/335M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "97ecc262cfe2405f8526beadc394c9f1"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "`text_config_dict` is provided which will be used to initialize `CLIPTextConfig`. The value `text_config[\"id2label\"]` will be overriden.\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5d912348ae64466a9dac3bc6f9b202d6"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "### torch.compile warmup:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "  0%|          | 0/50 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "40d102c2aeb64df7b7f9f4f500dc9c80"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 751 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %sample : torch.Tensor [#users=1] = placeholder[target=sample]\n",
            "    %timestep : torch.Tensor [#users=1] = placeholder[target=timestep]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=16] = placeholder[target=encoder_hidden_states]\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%timestep, None), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%getitem, cuda:0), kwargs = {})\n",
            "    %expand : [#users=1] = call_method[target=expand](args = (%to, 2), kwargs = {})\n",
            "    %arange : [#users=1] = call_function[target=torch.arange](args = (), kwargs = {start: 0, end: 160, dtype: torch.float32, device: cuda:0})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (-9.210340371976184, %arange), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%mul, 160), kwargs = {})\n",
            "    %exp : [#users=1] = call_function[target=torch.exp](args = (%truediv,), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%expand, (slice(None, None, None), None)), kwargs = {})\n",
            "    %float_1 : [#users=1] = call_method[target=float](args = (%getitem_1,), kwargs = {})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%exp, (None, slice(None, None, None))), kwargs = {})\n",
            "    %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%float_1, %getitem_2), kwargs = {})\n",
            "    %mul_2 : [#users=2] = call_function[target=operator.mul](args = (1, %mul_1), kwargs = {})\n",
            "    %sin : [#users=1] = call_function[target=torch.sin](args = (%mul_2,), kwargs = {})\n",
            "    %cos : [#users=1] = call_function[target=torch.cos](args = (%mul_2,), kwargs = {})\n",
            "    %cat : [#users=2] = call_function[target=torch.cat](args = ([%sin, %cos],), kwargs = {dim: -1})\n",
            "    %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(160, None, None))), kwargs = {})\n",
            "    %getitem_4 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(None, 160, None))), kwargs = {})\n",
            "    %cat_1 : [#users=1] = call_function[target=torch.cat](args = ([%getitem_3, %getitem_4],), kwargs = {dim: -1})\n",
            "    %to_1 : [#users=1] = call_method[target=to](args = (%cat_1,), kwargs = {dtype: torch.float16})\n",
            "    %self_time_embedding_linear_1 : [#users=1] = call_module[target=self_time_embedding_linear_1](args = (%to_1,), kwargs = {})\n",
            "    %self_time_embedding_act : [#users=1] = call_module[target=self_time_embedding_act](args = (%self_time_embedding_linear_1,), kwargs = {})\n",
            "    %self_time_embedding_linear_2 : [#users=22] = call_module[target=self_time_embedding_linear_2](args = (%self_time_embedding_act,), kwargs = {})\n",
            "    %self_conv_in : [#users=3] = call_module[target=self_conv_in](args = (%sample,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_norm1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_norm1](args = (%self_conv_in,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_nonlinearity : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_nonlinearity](args = (%self_down_blocks_0_resnets_0_norm1,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_conv1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_conv1](args = (%self_down_blocks_0_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_time_emb_proj](args = (%self_down_blocks_0_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_5 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_0_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_0_resnets_0_conv1, %getitem_5), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_norm2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_nonlinearity](args = (%self_down_blocks_0_resnets_0_norm2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_dropout : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_dropout](args = (%self_down_blocks_0_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_0_conv2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_0_conv2](args = (%self_down_blocks_0_resnets_0_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_in, %self_down_blocks_0_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=2] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_norm : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_norm](args = (%truediv_1,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_proj_in : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_proj_in](args = (%self_down_blocks_0_attentions_0_norm,), kwargs = {})\n",
            "    %permute : [#users=1] = call_method[target=permute](args = (%self_down_blocks_0_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape : [#users=2] = call_method[target=reshape](args = (%permute, 2, 4096, 320), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_norm1](args = (%reshape,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_attn1](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_2 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_attn1, %reshape), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_attn2](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_3 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_attn2, %add_2), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_norm3](args = (%add_3,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_6 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_7 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_7,), kwargs = {})\n",
            "    %mul_3 : [#users=1] = call_function[target=operator.mul](args = (%getitem_6, %gelu), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_3,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_0_transformer_blocks_0_ff_net_2, %add_3), kwargs = {})\n",
            "    %reshape_1 : [#users=1] = call_method[target=reshape](args = (%add_4, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_1 : [#users=1] = call_method[target=permute](args = (%reshape_1, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous : [#users=1] = call_method[target=contiguous](args = (%permute_1,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_0_proj_out : [#users=1] = call_module[target=self_down_blocks_0_attentions_0_proj_out](args = (%contiguous,), kwargs = {})\n",
            "    %add_5 : [#users=3] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_0_proj_out, %truediv_1), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_norm1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_norm1](args = (%add_5,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_nonlinearity : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_nonlinearity](args = (%self_down_blocks_0_resnets_1_norm1,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_conv1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_conv1](args = (%self_down_blocks_0_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_time_emb_proj](args = (%self_down_blocks_0_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_8 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_0_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_6 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_0_resnets_1_conv1, %getitem_8), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_norm2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_norm2](args = (%add_6,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_nonlinearity](args = (%self_down_blocks_0_resnets_1_norm2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_dropout : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_dropout](args = (%self_down_blocks_0_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_0_resnets_1_conv2 : [#users=1] = call_module[target=self_down_blocks_0_resnets_1_conv2](args = (%self_down_blocks_0_resnets_1_dropout,), kwargs = {})\n",
            "    %add_7 : [#users=1] = call_function[target=operator.add](args = (%add_5, %self_down_blocks_0_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_2 : [#users=2] = call_function[target=operator.truediv](args = (%add_7, 1.0), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_norm : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_norm](args = (%truediv_2,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_proj_in : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_proj_in](args = (%self_down_blocks_0_attentions_1_norm,), kwargs = {})\n",
            "    %permute_2 : [#users=1] = call_method[target=permute](args = (%self_down_blocks_0_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_2 : [#users=2] = call_method[target=reshape](args = (%permute_2, 2, 4096, 320), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_norm1](args = (%reshape_2,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_attn1](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_8 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_attn1, %reshape_2), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_norm2](args = (%add_8,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_attn2](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_9 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_attn2, %add_8), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_norm3](args = (%add_9,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_1 : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_9 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 0), kwargs = {})\n",
            "    %getitem_10 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 1), kwargs = {})\n",
            "    %gelu_1 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_10,), kwargs = {})\n",
            "    %mul_4 : [#users=1] = call_function[target=operator.mul](args = (%getitem_9, %gelu_1), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_4,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_10 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_1_transformer_blocks_0_ff_net_2, %add_9), kwargs = {})\n",
            "    %reshape_3 : [#users=1] = call_method[target=reshape](args = (%add_10, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_3 : [#users=1] = call_method[target=permute](args = (%reshape_3, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_1 : [#users=1] = call_method[target=contiguous](args = (%permute_3,), kwargs = {})\n",
            "    %self_down_blocks_0_attentions_1_proj_out : [#users=1] = call_module[target=self_down_blocks_0_attentions_1_proj_out](args = (%contiguous_1,), kwargs = {})\n",
            "    %add_11 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_0_attentions_1_proj_out, %truediv_2), kwargs = {})\n",
            "    %self_down_blocks_0_downsamplers_0_conv : [#users=3] = call_module[target=self_down_blocks_0_downsamplers_0_conv](args = (%add_11,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_norm1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_norm1](args = (%self_down_blocks_0_downsamplers_0_conv,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_nonlinearity : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_nonlinearity](args = (%self_down_blocks_1_resnets_0_norm1,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_conv1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_conv1](args = (%self_down_blocks_1_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_time_emb_proj](args = (%self_down_blocks_1_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_11 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_1_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_12 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_1_resnets_0_conv1, %getitem_11), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_norm2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_norm2](args = (%add_12,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_nonlinearity](args = (%self_down_blocks_1_resnets_0_norm2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_dropout : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_dropout](args = (%self_down_blocks_1_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_conv2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_conv2](args = (%self_down_blocks_1_resnets_0_dropout,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_down_blocks_1_resnets_0_conv_shortcut](args = (%self_down_blocks_0_downsamplers_0_conv,), kwargs = {})\n",
            "    %add_13 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_1_resnets_0_conv_shortcut, %self_down_blocks_1_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_3 : [#users=2] = call_function[target=operator.truediv](args = (%add_13, 1.0), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_norm : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_norm](args = (%truediv_3,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_proj_in : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_proj_in](args = (%self_down_blocks_1_attentions_0_norm,), kwargs = {})\n",
            "    %permute_4 : [#users=1] = call_method[target=permute](args = (%self_down_blocks_1_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_4 : [#users=2] = call_method[target=reshape](args = (%permute_4, 2, 1024, 640), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_norm1](args = (%reshape_4,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_attn1](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_14 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_attn1, %reshape_4), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_norm2](args = (%add_14,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_attn2](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_15 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_attn2, %add_14), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_norm3](args = (%add_15,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_2 : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_12 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_2, 0), kwargs = {})\n",
            "    %getitem_13 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_2, 1), kwargs = {})\n",
            "    %gelu_2 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_13,), kwargs = {})\n",
            "    %mul_5 : [#users=1] = call_function[target=operator.mul](args = (%getitem_12, %gelu_2), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_5,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_16 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_0_transformer_blocks_0_ff_net_2, %add_15), kwargs = {})\n",
            "    %reshape_5 : [#users=1] = call_method[target=reshape](args = (%add_16, 2, 32, 32, 640), kwargs = {})\n",
            "    %permute_5 : [#users=1] = call_method[target=permute](args = (%reshape_5, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_2 : [#users=1] = call_method[target=contiguous](args = (%permute_5,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_0_proj_out : [#users=1] = call_module[target=self_down_blocks_1_attentions_0_proj_out](args = (%contiguous_2,), kwargs = {})\n",
            "    %add_17 : [#users=3] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_0_proj_out, %truediv_3), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_norm1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_norm1](args = (%add_17,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_nonlinearity : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_nonlinearity](args = (%self_down_blocks_1_resnets_1_norm1,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_conv1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_conv1](args = (%self_down_blocks_1_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_time_emb_proj](args = (%self_down_blocks_1_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_14 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_1_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_18 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_1_resnets_1_conv1, %getitem_14), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_norm2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_norm2](args = (%add_18,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_nonlinearity](args = (%self_down_blocks_1_resnets_1_norm2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_dropout : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_dropout](args = (%self_down_blocks_1_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_1_resnets_1_conv2 : [#users=1] = call_module[target=self_down_blocks_1_resnets_1_conv2](args = (%self_down_blocks_1_resnets_1_dropout,), kwargs = {})\n",
            "    %add_19 : [#users=1] = call_function[target=operator.add](args = (%add_17, %self_down_blocks_1_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_4 : [#users=2] = call_function[target=operator.truediv](args = (%add_19, 1.0), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_norm : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_norm](args = (%truediv_4,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_proj_in : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_proj_in](args = (%self_down_blocks_1_attentions_1_norm,), kwargs = {})\n",
            "    %permute_6 : [#users=1] = call_method[target=permute](args = (%self_down_blocks_1_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_6 : [#users=2] = call_method[target=reshape](args = (%permute_6, 2, 1024, 640), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_norm1](args = (%reshape_6,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_attn1](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_20 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_attn1, %reshape_6), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_norm2](args = (%add_20,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_attn2](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_21 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_attn2, %add_20), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_norm3](args = (%add_21,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_3 : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_15 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_3, 0), kwargs = {})\n",
            "    %getitem_16 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_3, 1), kwargs = {})\n",
            "    %gelu_3 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_16,), kwargs = {})\n",
            "    %mul_6 : [#users=1] = call_function[target=operator.mul](args = (%getitem_15, %gelu_3), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_6,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_22 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_1_transformer_blocks_0_ff_net_2, %add_21), kwargs = {})\n",
            "    %reshape_7 : [#users=1] = call_method[target=reshape](args = (%add_22, 2, 32, 32, 640), kwargs = {})\n",
            "    %permute_7 : [#users=1] = call_method[target=permute](args = (%reshape_7, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_3 : [#users=1] = call_method[target=contiguous](args = (%permute_7,), kwargs = {})\n",
            "    %self_down_blocks_1_attentions_1_proj_out : [#users=1] = call_module[target=self_down_blocks_1_attentions_1_proj_out](args = (%contiguous_3,), kwargs = {})\n",
            "    %add_23 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_1_attentions_1_proj_out, %truediv_4), kwargs = {})\n",
            "    %self_down_blocks_1_downsamplers_0_conv : [#users=3] = call_module[target=self_down_blocks_1_downsamplers_0_conv](args = (%add_23,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_norm1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_norm1](args = (%self_down_blocks_1_downsamplers_0_conv,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_nonlinearity : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_nonlinearity](args = (%self_down_blocks_2_resnets_0_norm1,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_conv1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_conv1](args = (%self_down_blocks_2_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_time_emb_proj](args = (%self_down_blocks_2_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_17 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_2_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_24 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_resnets_0_conv1, %getitem_17), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_norm2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_norm2](args = (%add_24,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_nonlinearity](args = (%self_down_blocks_2_resnets_0_norm2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_dropout : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_dropout](args = (%self_down_blocks_2_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_conv2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_conv2](args = (%self_down_blocks_2_resnets_0_dropout,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_down_blocks_2_resnets_0_conv_shortcut](args = (%self_down_blocks_1_downsamplers_0_conv,), kwargs = {})\n",
            "    %add_25 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_resnets_0_conv_shortcut, %self_down_blocks_2_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_5 : [#users=2] = call_function[target=operator.truediv](args = (%add_25, 1.0), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_norm : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_norm](args = (%truediv_5,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_proj_in : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_proj_in](args = (%self_down_blocks_2_attentions_0_norm,), kwargs = {})\n",
            "    %permute_8 : [#users=1] = call_method[target=permute](args = (%self_down_blocks_2_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_8 : [#users=2] = call_method[target=reshape](args = (%permute_8, 2, 256, 1280), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_norm1](args = (%reshape_8,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_attn1](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_26 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_attn1, %reshape_8), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_norm2](args = (%add_26,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_attn2](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_27 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_attn2, %add_26), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_norm3](args = (%add_27,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_4 : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_18 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_4, 0), kwargs = {})\n",
            "    %getitem_19 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_4, 1), kwargs = {})\n",
            "    %gelu_4 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_19,), kwargs = {})\n",
            "    %mul_7 : [#users=1] = call_function[target=operator.mul](args = (%getitem_18, %gelu_4), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_7,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_28 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_0_transformer_blocks_0_ff_net_2, %add_27), kwargs = {})\n",
            "    %reshape_9 : [#users=1] = call_method[target=reshape](args = (%add_28, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_9 : [#users=1] = call_method[target=permute](args = (%reshape_9, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_4 : [#users=1] = call_method[target=contiguous](args = (%permute_9,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_0_proj_out : [#users=1] = call_module[target=self_down_blocks_2_attentions_0_proj_out](args = (%contiguous_4,), kwargs = {})\n",
            "    %add_29 : [#users=3] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_0_proj_out, %truediv_5), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_norm1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_norm1](args = (%add_29,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_nonlinearity : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_nonlinearity](args = (%self_down_blocks_2_resnets_1_norm1,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_conv1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_conv1](args = (%self_down_blocks_2_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_time_emb_proj](args = (%self_down_blocks_2_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_20 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_2_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_30 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_resnets_1_conv1, %getitem_20), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_norm2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_norm2](args = (%add_30,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_nonlinearity](args = (%self_down_blocks_2_resnets_1_norm2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_dropout : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_dropout](args = (%self_down_blocks_2_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_2_resnets_1_conv2 : [#users=1] = call_module[target=self_down_blocks_2_resnets_1_conv2](args = (%self_down_blocks_2_resnets_1_dropout,), kwargs = {})\n",
            "    %add_31 : [#users=1] = call_function[target=operator.add](args = (%add_29, %self_down_blocks_2_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_6 : [#users=2] = call_function[target=operator.truediv](args = (%add_31, 1.0), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_norm : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_norm](args = (%truediv_6,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_proj_in : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_proj_in](args = (%self_down_blocks_2_attentions_1_norm,), kwargs = {})\n",
            "    %permute_10 : [#users=1] = call_method[target=permute](args = (%self_down_blocks_2_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_10 : [#users=2] = call_method[target=reshape](args = (%permute_10, 2, 256, 1280), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_norm1](args = (%reshape_10,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_attn1](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_32 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_attn1, %reshape_10), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_norm2](args = (%add_32,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_attn2](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_33 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_attn2, %add_32), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_norm3](args = (%add_33,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_5 : [#users=2] = call_method[target=chunk](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_21 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_5, 0), kwargs = {})\n",
            "    %getitem_22 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_5, 1), kwargs = {})\n",
            "    %gelu_5 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_22,), kwargs = {})\n",
            "    %mul_8 : [#users=1] = call_function[target=operator.mul](args = (%getitem_21, %gelu_5), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_8,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_34 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_1_transformer_blocks_0_ff_net_2, %add_33), kwargs = {})\n",
            "    %reshape_11 : [#users=1] = call_method[target=reshape](args = (%add_34, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_11 : [#users=1] = call_method[target=permute](args = (%reshape_11, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_5 : [#users=1] = call_method[target=contiguous](args = (%permute_11,), kwargs = {})\n",
            "    %self_down_blocks_2_attentions_1_proj_out : [#users=1] = call_module[target=self_down_blocks_2_attentions_1_proj_out](args = (%contiguous_5,), kwargs = {})\n",
            "    %add_35 : [#users=2] = call_function[target=operator.add](args = (%self_down_blocks_2_attentions_1_proj_out, %truediv_6), kwargs = {})\n",
            "    %self_down_blocks_2_downsamplers_0_conv : [#users=3] = call_module[target=self_down_blocks_2_downsamplers_0_conv](args = (%add_35,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_norm1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_norm1](args = (%self_down_blocks_2_downsamplers_0_conv,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_nonlinearity : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_nonlinearity](args = (%self_down_blocks_3_resnets_0_norm1,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_conv1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_conv1](args = (%self_down_blocks_3_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_time_emb_proj](args = (%self_down_blocks_3_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_23 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_3_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_36 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_3_resnets_0_conv1, %getitem_23), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_norm2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_norm2](args = (%add_36,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_nonlinearity](args = (%self_down_blocks_3_resnets_0_norm2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_dropout : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_dropout](args = (%self_down_blocks_3_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_0_conv2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_0_conv2](args = (%self_down_blocks_3_resnets_0_dropout,), kwargs = {})\n",
            "    %add_37 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_2_downsamplers_0_conv, %self_down_blocks_3_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_7 : [#users=3] = call_function[target=operator.truediv](args = (%add_37, 1.0), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_norm1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_norm1](args = (%truediv_7,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_nonlinearity : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_nonlinearity](args = (%self_down_blocks_3_resnets_1_norm1,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_conv1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_conv1](args = (%self_down_blocks_3_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_time_emb_proj](args = (%self_down_blocks_3_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_24 : [#users=1] = call_function[target=operator.getitem](args = (%self_down_blocks_3_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_38 : [#users=1] = call_function[target=operator.add](args = (%self_down_blocks_3_resnets_1_conv1, %getitem_24), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_norm2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_norm2](args = (%add_38,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_nonlinearity](args = (%self_down_blocks_3_resnets_1_norm2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_dropout : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_dropout](args = (%self_down_blocks_3_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_down_blocks_3_resnets_1_conv2 : [#users=1] = call_module[target=self_down_blocks_3_resnets_1_conv2](args = (%self_down_blocks_3_resnets_1_dropout,), kwargs = {})\n",
            "    %add_39 : [#users=1] = call_function[target=operator.add](args = (%truediv_7, %self_down_blocks_3_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_8 : [#users=3] = call_function[target=operator.truediv](args = (%add_39, 1.0), kwargs = {})\n",
            "    %self_mid_block_resnets_0_norm1 : [#users=1] = call_module[target=self_mid_block_resnets_0_norm1](args = (%truediv_8,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_nonlinearity : [#users=1] = call_module[target=self_mid_block_resnets_0_nonlinearity](args = (%self_mid_block_resnets_0_norm1,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_conv1 : [#users=1] = call_module[target=self_mid_block_resnets_0_conv1](args = (%self_mid_block_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_mid_block_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_mid_block_resnets_0_time_emb_proj](args = (%self_mid_block_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_25 : [#users=1] = call_function[target=operator.getitem](args = (%self_mid_block_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_40 : [#users=1] = call_function[target=operator.add](args = (%self_mid_block_resnets_0_conv1, %getitem_25), kwargs = {})\n",
            "    %self_mid_block_resnets_0_norm2 : [#users=1] = call_module[target=self_mid_block_resnets_0_norm2](args = (%add_40,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_mid_block_resnets_0_nonlinearity](args = (%self_mid_block_resnets_0_norm2,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_dropout : [#users=1] = call_module[target=self_mid_block_resnets_0_dropout](args = (%self_mid_block_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_mid_block_resnets_0_conv2 : [#users=1] = call_module[target=self_mid_block_resnets_0_conv2](args = (%self_mid_block_resnets_0_dropout,), kwargs = {})\n",
            "    %add_41 : [#users=1] = call_function[target=operator.add](args = (%truediv_8, %self_mid_block_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_9 : [#users=2] = call_function[target=operator.truediv](args = (%add_41, 1), kwargs = {})\n",
            "    %self_mid_block_attentions_0_norm : [#users=1] = call_module[target=self_mid_block_attentions_0_norm](args = (%truediv_9,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_proj_in : [#users=1] = call_module[target=self_mid_block_attentions_0_proj_in](args = (%self_mid_block_attentions_0_norm,), kwargs = {})\n",
            "    %permute_12 : [#users=1] = call_method[target=permute](args = (%self_mid_block_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_12 : [#users=2] = call_method[target=reshape](args = (%permute_12, 2, 64, 1280), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_norm1](args = (%reshape_12,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_attn1](args = (%self_mid_block_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_42 : [#users=2] = call_function[target=operator.add](args = (%self_mid_block_attentions_0_transformer_blocks_0_attn1, %reshape_12), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_norm2](args = (%add_42,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_attn2](args = (%self_mid_block_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_43 : [#users=2] = call_function[target=operator.add](args = (%self_mid_block_attentions_0_transformer_blocks_0_attn2, %add_42), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_norm3](args = (%add_43,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_mid_block_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_6 : [#users=2] = call_method[target=chunk](args = (%self_mid_block_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_26 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_6, 0), kwargs = {})\n",
            "    %getitem_27 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_6, 1), kwargs = {})\n",
            "    %gelu_6 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_27,), kwargs = {})\n",
            "    %mul_9 : [#users=1] = call_function[target=operator.mul](args = (%getitem_26, %gelu_6), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_9,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_mid_block_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_mid_block_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_44 : [#users=1] = call_function[target=operator.add](args = (%self_mid_block_attentions_0_transformer_blocks_0_ff_net_2, %add_43), kwargs = {})\n",
            "    %reshape_13 : [#users=1] = call_method[target=reshape](args = (%add_44, 2, 8, 8, 1280), kwargs = {})\n",
            "    %permute_13 : [#users=1] = call_method[target=permute](args = (%reshape_13, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_6 : [#users=1] = call_method[target=contiguous](args = (%permute_13,), kwargs = {})\n",
            "    %self_mid_block_attentions_0_proj_out : [#users=1] = call_module[target=self_mid_block_attentions_0_proj_out](args = (%contiguous_6,), kwargs = {})\n",
            "    %add_45 : [#users=2] = call_function[target=operator.add](args = (%self_mid_block_attentions_0_proj_out, %truediv_9), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_norm1 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_norm1](args = (%add_45,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_nonlinearity : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_nonlinearity](args = (%self_mid_block_resnets_slice_1__none__none___0_norm1,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_conv1 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_conv1](args = (%self_mid_block_resnets_slice_1__none__none___0_nonlinearity,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_nonlinearity_1 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_time_emb_proj : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_time_emb_proj](args = (%self_mid_block_resnets_slice_1__none__none___0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_28 : [#users=1] = call_function[target=operator.getitem](args = (%self_mid_block_resnets_slice_1__none__none___0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_46 : [#users=1] = call_function[target=operator.add](args = (%self_mid_block_resnets_slice_1__none__none___0_conv1, %getitem_28), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_norm2 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_norm2](args = (%add_46,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_nonlinearity_2 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_nonlinearity](args = (%self_mid_block_resnets_slice_1__none__none___0_norm2,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_dropout : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_dropout](args = (%self_mid_block_resnets_slice_1__none__none___0_nonlinearity_2,), kwargs = {})\n",
            "    %self_mid_block_resnets_slice_1__none__none___0_conv2 : [#users=1] = call_module[target=self_mid_block_resnets_slice_1__None__None___0_conv2](args = (%self_mid_block_resnets_slice_1__none__none___0_dropout,), kwargs = {})\n",
            "    %add_47 : [#users=1] = call_function[target=operator.add](args = (%add_45, %self_mid_block_resnets_slice_1__none__none___0_conv2), kwargs = {})\n",
            "    %truediv_10 : [#users=1] = call_function[target=operator.truediv](args = (%add_47, 1), kwargs = {})\n",
            "    %cat_2 : [#users=2] = call_function[target=torch.cat](args = ([%truediv_10, %truediv_8],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_0_resnets_0_norm1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_norm1](args = (%cat_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_nonlinearity : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_nonlinearity](args = (%self_up_blocks_0_resnets_0_norm1,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_conv1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_conv1](args = (%self_up_blocks_0_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_time_emb_proj](args = (%self_up_blocks_0_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_29 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_0_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_48 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_0_conv1, %getitem_29), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_norm2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_norm2](args = (%add_48,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_nonlinearity](args = (%self_up_blocks_0_resnets_0_norm2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_dropout : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_dropout](args = (%self_up_blocks_0_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_conv2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_conv2](args = (%self_up_blocks_0_resnets_0_dropout,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_0_resnets_0_conv_shortcut](args = (%cat_2,), kwargs = {})\n",
            "    %add_49 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_0_conv_shortcut, %self_up_blocks_0_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_11 : [#users=1] = call_function[target=operator.truediv](args = (%add_49, 1.0), kwargs = {})\n",
            "    %cat_3 : [#users=2] = call_function[target=torch.cat](args = ([%truediv_11, %truediv_7],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_0_resnets_1_norm1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_norm1](args = (%cat_3,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_nonlinearity : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_nonlinearity](args = (%self_up_blocks_0_resnets_1_norm1,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_conv1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_conv1](args = (%self_up_blocks_0_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_time_emb_proj](args = (%self_up_blocks_0_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_30 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_0_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_50 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_1_conv1, %getitem_30), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_norm2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_norm2](args = (%add_50,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_nonlinearity](args = (%self_up_blocks_0_resnets_1_norm2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_dropout : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_dropout](args = (%self_up_blocks_0_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_conv2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_conv2](args = (%self_up_blocks_0_resnets_1_dropout,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_0_resnets_1_conv_shortcut](args = (%cat_3,), kwargs = {})\n",
            "    %add_51 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_1_conv_shortcut, %self_up_blocks_0_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_12 : [#users=1] = call_function[target=operator.truediv](args = (%add_51, 1.0), kwargs = {})\n",
            "    %cat_4 : [#users=2] = call_function[target=torch.cat](args = ([%truediv_12, %self_down_blocks_2_downsamplers_0_conv],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_0_resnets_2_norm1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_norm1](args = (%cat_4,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_nonlinearity : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_nonlinearity](args = (%self_up_blocks_0_resnets_2_norm1,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_conv1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_conv1](args = (%self_up_blocks_0_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_time_emb_proj](args = (%self_up_blocks_0_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_31 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_0_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_52 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_2_conv1, %getitem_31), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_norm2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_norm2](args = (%add_52,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_nonlinearity](args = (%self_up_blocks_0_resnets_2_norm2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_dropout : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_dropout](args = (%self_up_blocks_0_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_conv2 : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_conv2](args = (%self_up_blocks_0_resnets_2_dropout,), kwargs = {})\n",
            "    %self_up_blocks_0_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_0_resnets_2_conv_shortcut](args = (%cat_4,), kwargs = {})\n",
            "    %add_53 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_0_resnets_2_conv_shortcut, %self_up_blocks_0_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_13 : [#users=1] = call_function[target=operator.truediv](args = (%add_53, 1.0), kwargs = {})\n",
            "    %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%truediv_13,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_up_blocks_0_upsamplers_0_conv : [#users=1] = call_module[target=self_up_blocks_0_upsamplers_0_conv](args = (%interpolate,), kwargs = {})\n",
            "    %cat_5 : [#users=2] = call_function[target=torch.cat](args = ([%self_up_blocks_0_upsamplers_0_conv, %add_35],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_1_resnets_0_norm1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_norm1](args = (%cat_5,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_nonlinearity : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_nonlinearity](args = (%self_up_blocks_1_resnets_0_norm1,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_conv1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_conv1](args = (%self_up_blocks_1_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_time_emb_proj](args = (%self_up_blocks_1_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_32 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_1_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_54 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_0_conv1, %getitem_32), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_norm2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_norm2](args = (%add_54,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_nonlinearity](args = (%self_up_blocks_1_resnets_0_norm2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_dropout : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_dropout](args = (%self_up_blocks_1_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_conv2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_conv2](args = (%self_up_blocks_1_resnets_0_dropout,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_1_resnets_0_conv_shortcut](args = (%cat_5,), kwargs = {})\n",
            "    %add_55 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_0_conv_shortcut, %self_up_blocks_1_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_14 : [#users=2] = call_function[target=operator.truediv](args = (%add_55, 1.0), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_norm : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_norm](args = (%truediv_14,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_proj_in : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_proj_in](args = (%self_up_blocks_1_attentions_0_norm,), kwargs = {})\n",
            "    %permute_14 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_1_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_14 : [#users=2] = call_method[target=reshape](args = (%permute_14, 2, 256, 1280), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_norm1](args = (%reshape_14,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_attn1](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_56 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_attn1, %reshape_14), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_norm2](args = (%add_56,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_attn2](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_57 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_attn2, %add_56), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_norm3](args = (%add_57,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_7 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_33 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_7, 0), kwargs = {})\n",
            "    %getitem_34 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_7, 1), kwargs = {})\n",
            "    %gelu_7 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_34,), kwargs = {})\n",
            "    %mul_10 : [#users=1] = call_function[target=operator.mul](args = (%getitem_33, %gelu_7), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_10,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_58 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_0_transformer_blocks_0_ff_net_2, %add_57), kwargs = {})\n",
            "    %reshape_15 : [#users=1] = call_method[target=reshape](args = (%add_58, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_15 : [#users=1] = call_method[target=permute](args = (%reshape_15, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_7 : [#users=1] = call_method[target=contiguous](args = (%permute_15,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_0_proj_out : [#users=1] = call_module[target=self_up_blocks_1_attentions_0_proj_out](args = (%contiguous_7,), kwargs = {})\n",
            "    %add_59 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_0_proj_out, %truediv_14), kwargs = {})\n",
            "    %cat_6 : [#users=2] = call_function[target=torch.cat](args = ([%add_59, %add_29],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_1_resnets_1_norm1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_norm1](args = (%cat_6,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_nonlinearity : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_nonlinearity](args = (%self_up_blocks_1_resnets_1_norm1,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_conv1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_conv1](args = (%self_up_blocks_1_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_time_emb_proj](args = (%self_up_blocks_1_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_35 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_1_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_60 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_1_conv1, %getitem_35), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_norm2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_norm2](args = (%add_60,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_nonlinearity](args = (%self_up_blocks_1_resnets_1_norm2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_dropout : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_dropout](args = (%self_up_blocks_1_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_conv2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_conv2](args = (%self_up_blocks_1_resnets_1_dropout,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_1_resnets_1_conv_shortcut](args = (%cat_6,), kwargs = {})\n",
            "    %add_61 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_1_conv_shortcut, %self_up_blocks_1_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_15 : [#users=2] = call_function[target=operator.truediv](args = (%add_61, 1.0), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_norm : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_norm](args = (%truediv_15,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_proj_in : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_proj_in](args = (%self_up_blocks_1_attentions_1_norm,), kwargs = {})\n",
            "    %permute_16 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_1_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_16 : [#users=2] = call_method[target=reshape](args = (%permute_16, 2, 256, 1280), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_norm1](args = (%reshape_16,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_attn1](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_62 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_attn1, %reshape_16), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_norm2](args = (%add_62,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_attn2](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_63 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_attn2, %add_62), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_norm3](args = (%add_63,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_8 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_36 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_8, 0), kwargs = {})\n",
            "    %getitem_37 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_8, 1), kwargs = {})\n",
            "    %gelu_8 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_37,), kwargs = {})\n",
            "    %mul_11 : [#users=1] = call_function[target=operator.mul](args = (%getitem_36, %gelu_8), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_11,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_64 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_1_transformer_blocks_0_ff_net_2, %add_63), kwargs = {})\n",
            "    %reshape_17 : [#users=1] = call_method[target=reshape](args = (%add_64, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_17 : [#users=1] = call_method[target=permute](args = (%reshape_17, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_8 : [#users=1] = call_method[target=contiguous](args = (%permute_17,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_1_proj_out : [#users=1] = call_module[target=self_up_blocks_1_attentions_1_proj_out](args = (%contiguous_8,), kwargs = {})\n",
            "    %add_65 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_1_proj_out, %truediv_15), kwargs = {})\n",
            "    %cat_7 : [#users=2] = call_function[target=torch.cat](args = ([%add_65, %self_down_blocks_1_downsamplers_0_conv],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_1_resnets_2_norm1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_norm1](args = (%cat_7,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_nonlinearity : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_nonlinearity](args = (%self_up_blocks_1_resnets_2_norm1,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_conv1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_conv1](args = (%self_up_blocks_1_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_time_emb_proj](args = (%self_up_blocks_1_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_38 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_1_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_66 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_2_conv1, %getitem_38), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_norm2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_norm2](args = (%add_66,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_nonlinearity](args = (%self_up_blocks_1_resnets_2_norm2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_dropout : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_dropout](args = (%self_up_blocks_1_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_conv2 : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_conv2](args = (%self_up_blocks_1_resnets_2_dropout,), kwargs = {})\n",
            "    %self_up_blocks_1_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_1_resnets_2_conv_shortcut](args = (%cat_7,), kwargs = {})\n",
            "    %add_67 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_resnets_2_conv_shortcut, %self_up_blocks_1_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_16 : [#users=2] = call_function[target=operator.truediv](args = (%add_67, 1.0), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_norm : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_norm](args = (%truediv_16,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_proj_in : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_proj_in](args = (%self_up_blocks_1_attentions_2_norm,), kwargs = {})\n",
            "    %permute_18 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_1_attentions_2_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_18 : [#users=2] = call_method[target=reshape](args = (%permute_18, 2, 256, 1280), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_norm1](args = (%reshape_18,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_attn1](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_68 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_attn1, %reshape_18), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_norm2](args = (%add_68,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_attn2](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_69 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_attn2, %add_68), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_norm3](args = (%add_69,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_9 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_39 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_9, 0), kwargs = {})\n",
            "    %getitem_40 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_9, 1), kwargs = {})\n",
            "    %gelu_9 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_40,), kwargs = {})\n",
            "    %mul_12 : [#users=1] = call_function[target=operator.mul](args = (%getitem_39, %gelu_9), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1](args = (%mul_12,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_70 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_2_transformer_blocks_0_ff_net_2, %add_69), kwargs = {})\n",
            "    %reshape_19 : [#users=1] = call_method[target=reshape](args = (%add_70, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_19 : [#users=1] = call_method[target=permute](args = (%reshape_19, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_9 : [#users=1] = call_method[target=contiguous](args = (%permute_19,), kwargs = {})\n",
            "    %self_up_blocks_1_attentions_2_proj_out : [#users=1] = call_module[target=self_up_blocks_1_attentions_2_proj_out](args = (%contiguous_9,), kwargs = {})\n",
            "    %add_71 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_1_attentions_2_proj_out, %truediv_16), kwargs = {})\n",
            "    %interpolate_1 : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%add_71,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_up_blocks_1_upsamplers_0_conv : [#users=1] = call_module[target=self_up_blocks_1_upsamplers_0_conv](args = (%interpolate_1,), kwargs = {})\n",
            "    %cat_8 : [#users=2] = call_function[target=torch.cat](args = ([%self_up_blocks_1_upsamplers_0_conv, %add_23],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_2_resnets_0_norm1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_norm1](args = (%cat_8,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_nonlinearity : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_nonlinearity](args = (%self_up_blocks_2_resnets_0_norm1,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_conv1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_conv1](args = (%self_up_blocks_2_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_time_emb_proj](args = (%self_up_blocks_2_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_41 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_2_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_72 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_0_conv1, %getitem_41), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_norm2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_norm2](args = (%add_72,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_nonlinearity](args = (%self_up_blocks_2_resnets_0_norm2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_dropout : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_dropout](args = (%self_up_blocks_2_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_conv2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_conv2](args = (%self_up_blocks_2_resnets_0_dropout,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_2_resnets_0_conv_shortcut](args = (%cat_8,), kwargs = {})\n",
            "    %add_73 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_0_conv_shortcut, %self_up_blocks_2_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_17 : [#users=2] = call_function[target=operator.truediv](args = (%add_73, 1.0), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_norm : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_norm](args = (%truediv_17,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_proj_in : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_proj_in](args = (%self_up_blocks_2_attentions_0_norm,), kwargs = {})\n",
            "    %permute_20 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_2_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_20 : [#users=2] = call_method[target=reshape](args = (%permute_20, 2, 1024, 640), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_norm1](args = (%reshape_20,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_attn1](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_74 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_attn1, %reshape_20), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_norm2](args = (%add_74,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_attn2](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_75 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_attn2, %add_74), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_norm3](args = (%add_75,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_10 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_42 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_10, 0), kwargs = {})\n",
            "    %getitem_43 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_10, 1), kwargs = {})\n",
            "    %gelu_10 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_43,), kwargs = {})\n",
            "    %mul_13 : [#users=1] = call_function[target=operator.mul](args = (%getitem_42, %gelu_10), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_13,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_76 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_0_transformer_blocks_0_ff_net_2, %add_75), kwargs = {})\n",
            "    %reshape_21 : [#users=1] = call_method[target=reshape](args = (%add_76, 2, 32, 32, 640), kwargs = {})\n",
            "    %permute_21 : [#users=1] = call_method[target=permute](args = (%reshape_21, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_10 : [#users=1] = call_method[target=contiguous](args = (%permute_21,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_0_proj_out : [#users=1] = call_module[target=self_up_blocks_2_attentions_0_proj_out](args = (%contiguous_10,), kwargs = {})\n",
            "    %add_77 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_0_proj_out, %truediv_17), kwargs = {})\n",
            "    %cat_9 : [#users=2] = call_function[target=torch.cat](args = ([%add_77, %add_17],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_2_resnets_1_norm1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_norm1](args = (%cat_9,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_nonlinearity : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_nonlinearity](args = (%self_up_blocks_2_resnets_1_norm1,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_conv1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_conv1](args = (%self_up_blocks_2_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_time_emb_proj](args = (%self_up_blocks_2_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_44 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_2_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_78 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_1_conv1, %getitem_44), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_norm2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_norm2](args = (%add_78,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_nonlinearity](args = (%self_up_blocks_2_resnets_1_norm2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_dropout : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_dropout](args = (%self_up_blocks_2_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_conv2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_conv2](args = (%self_up_blocks_2_resnets_1_dropout,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_2_resnets_1_conv_shortcut](args = (%cat_9,), kwargs = {})\n",
            "    %add_79 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_1_conv_shortcut, %self_up_blocks_2_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_18 : [#users=2] = call_function[target=operator.truediv](args = (%add_79, 1.0), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_norm : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_norm](args = (%truediv_18,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_proj_in : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_proj_in](args = (%self_up_blocks_2_attentions_1_norm,), kwargs = {})\n",
            "    %permute_22 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_2_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_22 : [#users=2] = call_method[target=reshape](args = (%permute_22, 2, 1024, 640), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_norm1](args = (%reshape_22,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_attn1](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_80 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_attn1, %reshape_22), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_norm2](args = (%add_80,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_attn2](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_81 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_attn2, %add_80), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_norm3](args = (%add_81,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_11 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_45 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_11, 0), kwargs = {})\n",
            "    %getitem_46 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_11, 1), kwargs = {})\n",
            "    %gelu_11 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_46,), kwargs = {})\n",
            "    %mul_14 : [#users=1] = call_function[target=operator.mul](args = (%getitem_45, %gelu_11), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_14,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_82 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_1_transformer_blocks_0_ff_net_2, %add_81), kwargs = {})\n",
            "    %reshape_23 : [#users=1] = call_method[target=reshape](args = (%add_82, 2, 32, 32, 640), kwargs = {})\n",
            "    %permute_23 : [#users=1] = call_method[target=permute](args = (%reshape_23, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_11 : [#users=1] = call_method[target=contiguous](args = (%permute_23,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_1_proj_out : [#users=1] = call_module[target=self_up_blocks_2_attentions_1_proj_out](args = (%contiguous_11,), kwargs = {})\n",
            "    %add_83 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_1_proj_out, %truediv_18), kwargs = {})\n",
            "    %cat_10 : [#users=2] = call_function[target=torch.cat](args = ([%add_83, %self_down_blocks_0_downsamplers_0_conv],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_2_resnets_2_norm1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_norm1](args = (%cat_10,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_nonlinearity : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_nonlinearity](args = (%self_up_blocks_2_resnets_2_norm1,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_conv1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_conv1](args = (%self_up_blocks_2_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_time_emb_proj](args = (%self_up_blocks_2_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_47 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_2_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_84 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_2_conv1, %getitem_47), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_norm2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_norm2](args = (%add_84,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_nonlinearity](args = (%self_up_blocks_2_resnets_2_norm2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_dropout : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_dropout](args = (%self_up_blocks_2_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_conv2 : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_conv2](args = (%self_up_blocks_2_resnets_2_dropout,), kwargs = {})\n",
            "    %self_up_blocks_2_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_2_resnets_2_conv_shortcut](args = (%cat_10,), kwargs = {})\n",
            "    %add_85 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_resnets_2_conv_shortcut, %self_up_blocks_2_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_19 : [#users=2] = call_function[target=operator.truediv](args = (%add_85, 1.0), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_norm : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_norm](args = (%truediv_19,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_proj_in : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_proj_in](args = (%self_up_blocks_2_attentions_2_norm,), kwargs = {})\n",
            "    %permute_24 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_2_attentions_2_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_24 : [#users=2] = call_method[target=reshape](args = (%permute_24, 2, 1024, 640), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_norm1](args = (%reshape_24,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_attn1](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_86 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_attn1, %reshape_24), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_norm2](args = (%add_86,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_attn2](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_87 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_attn2, %add_86), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_norm3](args = (%add_87,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_12 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_48 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_12, 0), kwargs = {})\n",
            "    %getitem_49 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_12, 1), kwargs = {})\n",
            "    %gelu_12 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_49,), kwargs = {})\n",
            "    %mul_15 : [#users=1] = call_function[target=operator.mul](args = (%getitem_48, %gelu_12), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_1](args = (%mul_15,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_88 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_2_transformer_blocks_0_ff_net_2, %add_87), kwargs = {})\n",
            "    %reshape_25 : [#users=1] = call_method[target=reshape](args = (%add_88, 2, 32, 32, 640), kwargs = {})\n",
            "    %permute_25 : [#users=1] = call_method[target=permute](args = (%reshape_25, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_12 : [#users=1] = call_method[target=contiguous](args = (%permute_25,), kwargs = {})\n",
            "    %self_up_blocks_2_attentions_2_proj_out : [#users=1] = call_module[target=self_up_blocks_2_attentions_2_proj_out](args = (%contiguous_12,), kwargs = {})\n",
            "    %add_89 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_2_attentions_2_proj_out, %truediv_19), kwargs = {})\n",
            "    %interpolate_2 : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%add_89,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_up_blocks_2_upsamplers_0_conv : [#users=1] = call_module[target=self_up_blocks_2_upsamplers_0_conv](args = (%interpolate_2,), kwargs = {})\n",
            "    %cat_11 : [#users=2] = call_function[target=torch.cat](args = ([%self_up_blocks_2_upsamplers_0_conv, %add_11],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_3_resnets_0_norm1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_norm1](args = (%cat_11,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_nonlinearity : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_nonlinearity](args = (%self_up_blocks_3_resnets_0_norm1,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_conv1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_conv1](args = (%self_up_blocks_3_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_time_emb_proj](args = (%self_up_blocks_3_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_50 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_3_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_90 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_0_conv1, %getitem_50), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_norm2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_norm2](args = (%add_90,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_nonlinearity](args = (%self_up_blocks_3_resnets_0_norm2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_dropout : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_dropout](args = (%self_up_blocks_3_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_conv2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_conv2](args = (%self_up_blocks_3_resnets_0_dropout,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_3_resnets_0_conv_shortcut](args = (%cat_11,), kwargs = {})\n",
            "    %add_91 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_0_conv_shortcut, %self_up_blocks_3_resnets_0_conv2), kwargs = {})\n",
            "    %truediv_20 : [#users=2] = call_function[target=operator.truediv](args = (%add_91, 1.0), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_norm : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_norm](args = (%truediv_20,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_proj_in : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_proj_in](args = (%self_up_blocks_3_attentions_0_norm,), kwargs = {})\n",
            "    %permute_26 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_3_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_26 : [#users=2] = call_method[target=reshape](args = (%permute_26, 2, 4096, 320), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_norm1](args = (%reshape_26,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_attn1](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_92 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_attn1, %reshape_26), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_norm2](args = (%add_92,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_attn2](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_93 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_attn2, %add_92), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_norm3](args = (%add_93,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_13 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_51 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_13, 0), kwargs = {})\n",
            "    %getitem_52 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_13, 1), kwargs = {})\n",
            "    %gelu_13 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_52,), kwargs = {})\n",
            "    %mul_16 : [#users=1] = call_function[target=operator.mul](args = (%getitem_51, %gelu_13), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul_16,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_94 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_0_transformer_blocks_0_ff_net_2, %add_93), kwargs = {})\n",
            "    %reshape_27 : [#users=1] = call_method[target=reshape](args = (%add_94, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_27 : [#users=1] = call_method[target=permute](args = (%reshape_27, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_13 : [#users=1] = call_method[target=contiguous](args = (%permute_27,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_0_proj_out : [#users=1] = call_module[target=self_up_blocks_3_attentions_0_proj_out](args = (%contiguous_13,), kwargs = {})\n",
            "    %add_95 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_0_proj_out, %truediv_20), kwargs = {})\n",
            "    %cat_12 : [#users=2] = call_function[target=torch.cat](args = ([%add_95, %add_5],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_3_resnets_1_norm1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_norm1](args = (%cat_12,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_nonlinearity : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_nonlinearity](args = (%self_up_blocks_3_resnets_1_norm1,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_conv1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_conv1](args = (%self_up_blocks_3_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_time_emb_proj](args = (%self_up_blocks_3_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_53 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_3_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_96 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_1_conv1, %getitem_53), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_norm2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_norm2](args = (%add_96,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_nonlinearity](args = (%self_up_blocks_3_resnets_1_norm2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_dropout : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_dropout](args = (%self_up_blocks_3_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_conv2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_conv2](args = (%self_up_blocks_3_resnets_1_dropout,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_3_resnets_1_conv_shortcut](args = (%cat_12,), kwargs = {})\n",
            "    %add_97 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_1_conv_shortcut, %self_up_blocks_3_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_21 : [#users=2] = call_function[target=operator.truediv](args = (%add_97, 1.0), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_norm : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_norm](args = (%truediv_21,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_proj_in : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_proj_in](args = (%self_up_blocks_3_attentions_1_norm,), kwargs = {})\n",
            "    %permute_28 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_3_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_28 : [#users=2] = call_method[target=reshape](args = (%permute_28, 2, 4096, 320), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_norm1](args = (%reshape_28,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_attn1](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_98 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_attn1, %reshape_28), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_norm2](args = (%add_98,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_attn2](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_99 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_attn2, %add_98), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_norm3](args = (%add_99,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_14 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_54 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_14, 0), kwargs = {})\n",
            "    %getitem_55 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_14, 1), kwargs = {})\n",
            "    %gelu_14 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_55,), kwargs = {})\n",
            "    %mul_17 : [#users=1] = call_function[target=operator.mul](args = (%getitem_54, %gelu_14), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_17,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_100 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_1_transformer_blocks_0_ff_net_2, %add_99), kwargs = {})\n",
            "    %reshape_29 : [#users=1] = call_method[target=reshape](args = (%add_100, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_29 : [#users=1] = call_method[target=permute](args = (%reshape_29, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_14 : [#users=1] = call_method[target=contiguous](args = (%permute_29,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_1_proj_out : [#users=1] = call_module[target=self_up_blocks_3_attentions_1_proj_out](args = (%contiguous_14,), kwargs = {})\n",
            "    %add_101 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_1_proj_out, %truediv_21), kwargs = {})\n",
            "    %cat_13 : [#users=2] = call_function[target=torch.cat](args = ([%add_101, %self_conv_in],), kwargs = {dim: 1})\n",
            "    %self_up_blocks_3_resnets_2_norm1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_norm1](args = (%cat_13,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_nonlinearity : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_nonlinearity](args = (%self_up_blocks_3_resnets_2_norm1,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_conv1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_conv1](args = (%self_up_blocks_3_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_nonlinearity](args = (%self_time_embedding_linear_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_time_emb_proj](args = (%self_up_blocks_3_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_56 : [#users=1] = call_function[target=operator.getitem](args = (%self_up_blocks_3_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_102 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_2_conv1, %getitem_56), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_norm2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_norm2](args = (%add_102,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_nonlinearity](args = (%self_up_blocks_3_resnets_2_norm2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_dropout : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_dropout](args = (%self_up_blocks_3_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_conv2 : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_conv2](args = (%self_up_blocks_3_resnets_2_dropout,), kwargs = {})\n",
            "    %self_up_blocks_3_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_up_blocks_3_resnets_2_conv_shortcut](args = (%cat_13,), kwargs = {})\n",
            "    %add_103 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_resnets_2_conv_shortcut, %self_up_blocks_3_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_22 : [#users=2] = call_function[target=operator.truediv](args = (%add_103, 1.0), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_norm : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_norm](args = (%truediv_22,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_proj_in : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_proj_in](args = (%self_up_blocks_3_attentions_2_norm,), kwargs = {})\n",
            "    %permute_30 : [#users=1] = call_method[target=permute](args = (%self_up_blocks_3_attentions_2_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_30 : [#users=2] = call_method[target=reshape](args = (%permute_30, 2, 4096, 320), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_norm1](args = (%reshape_30,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_attn1](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_104 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_attn1, %reshape_30), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_norm2](args = (%add_104,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_attn2](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_105 : [#users=2] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_attn2, %add_104), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_norm3](args = (%add_105,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_15 : [#users=2] = call_method[target=chunk](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_57 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_15, 0), kwargs = {})\n",
            "    %getitem_58 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_15, 1), kwargs = {})\n",
            "    %gelu_15 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_58,), kwargs = {})\n",
            "    %mul_18 : [#users=1] = call_function[target=operator.mul](args = (%getitem_57, %gelu_15), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_1](args = (%mul_18,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_106 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_2_transformer_blocks_0_ff_net_2, %add_105), kwargs = {})\n",
            "    %reshape_31 : [#users=1] = call_method[target=reshape](args = (%add_106, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_31 : [#users=1] = call_method[target=permute](args = (%reshape_31, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_15 : [#users=1] = call_method[target=contiguous](args = (%permute_31,), kwargs = {})\n",
            "    %self_up_blocks_3_attentions_2_proj_out : [#users=1] = call_module[target=self_up_blocks_3_attentions_2_proj_out](args = (%contiguous_15,), kwargs = {})\n",
            "    %add_107 : [#users=1] = call_function[target=operator.add](args = (%self_up_blocks_3_attentions_2_proj_out, %truediv_22), kwargs = {})\n",
            "    %self_conv_norm_out : [#users=1] = call_module[target=self_conv_norm_out](args = (%add_107,), kwargs = {})\n",
            "    %self_conv_act : [#users=1] = call_module[target=self_conv_act](args = (%self_conv_norm_out,), kwargs = {})\n",
            "    %self_conv_out : [#users=1] = call_module[target=self_conv_out](args = (%self_conv_act,), kwargs = {})\n",
            "    return (self_conv_out,)\n",
            "[2023-06-07 17:50:19,608] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_condition.py line 636 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %timesteps : torch.Tensor [#users=1] = placeholder[target=timesteps]\n",
            "    %arange : [#users=1] = call_function[target=torch.arange](args = (), kwargs = {start: 0, end: 160, dtype: torch.float32, device: cuda:0})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (-9.210340371976184, %arange), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%mul, 160), kwargs = {})\n",
            "    %exp : [#users=1] = call_function[target=torch.exp](args = (%truediv,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%timesteps, (slice(None, None, None), None)), kwargs = {})\n",
            "    %float_1 : [#users=1] = call_method[target=float](args = (%getitem,), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%exp, (None, slice(None, None, None))), kwargs = {})\n",
            "    %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%float_1, %getitem_1), kwargs = {})\n",
            "    %mul_2 : [#users=2] = call_function[target=operator.mul](args = (1, %mul_1), kwargs = {})\n",
            "    %sin : [#users=1] = call_function[target=torch.sin](args = (%mul_2,), kwargs = {})\n",
            "    %cos : [#users=1] = call_function[target=torch.cos](args = (%mul_2,), kwargs = {})\n",
            "    %cat : [#users=2] = call_function[target=torch.cat](args = ([%sin, %cos],), kwargs = {dim: -1})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(160, None, None))), kwargs = {})\n",
            "    %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(None, 160, None))), kwargs = {})\n",
            "    %cat_1 : [#users=1] = call_function[target=torch.cat](args = ([%getitem_2, %getitem_3],), kwargs = {dim: -1})\n",
            "    return (cat_1,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2,), dtype='int64', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %timesteps : torch.Tensor [#users=1] = placeholder[target=timesteps]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %arange : [#users=1] = call_function[target=torch.arange](args = (), kwargs = {start: 0, end: 160, dtype: torch.float32, device: cuda:0})\n",
            "Compiling cuda task \u001b[92marange(c=float32(160,), start=0, stop=160, step=1, dtype='float32')\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %mul : [#users=1] = call_function[target=operator.mul](args = (-9.210340371976184, %arange), kwargs = {})\n",
            "Compiling cuda task \u001b[92mmuls(x=float32(160,), y=float32(160,))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%mul, 160), kwargs = {})\n",
            "Compiling cuda task \u001b[92mfull(c=float32(), shape=[], value=160.0f, dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mdivs(x=float32(160,), y=float32(160,))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %exp : [#users=1] = call_function[target=torch.exp](args = (%truediv,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mexp(x=float32(160,), y=float32(160,))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%timesteps, (slice(None, None, None), None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %float_1 : [#users=1] = call_method[target=float](args = (%getitem,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%exp, (None, slice(None, None, None))), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float32(160,), y=float32(1, 160))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mslice(data=float32(1, 160), out=float32(1, 160))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%float_1, %getitem_1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %mul_2 : [#users=2] = call_function[target=operator.mul](args = (1, %mul_1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %sin : [#users=1] = call_function[target=torch.sin](args = (%mul_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %cos : [#users=1] = call_function[target=torch.cos](args = (%mul_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %cat : [#users=2] = call_function[target=torch.cat](args = ([%sin, %cos],), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(160, None, None))), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%cat, (slice(None, None, None), slice(None, 160, None))), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %cat_1 : [#users=1] = call_function[target=torch.cat](args = ([%getitem_2, %getitem_3],), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (cat_1,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mcast(x=float32(1, 160), y=float16(1, 160))\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(y=float16(1, 160), x=int64(2,), y=float32(2, 320), fused_ops='rearrange slice cast cast mul muls sin cos concat slice slice concat cast', anchor='cast')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/dynamo_backends.py:62: UserWarning: Hidet received a non-contiguous torch input tensor, converting it to contiguous\n",
            "  warnings.warn_once('Hidet received a non-contiguous torch input tensor, converting it to contiguous')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 5 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %sample : torch.Tensor [#users=1] = placeholder[target=sample]\n",
            "    %self_linear_1 : [#users=1] = call_module[target=self_linear_1](args = (%sample,), kwargs = {})\n",
            "    %self_act : [#users=1] = call_module[target=self_act](args = (%self_linear_1,), kwargs = {})\n",
            "    %self_linear_2 : [#users=1] = call_module[target=self_linear_2](args = (%self_act,), kwargs = {})\n",
            "    return (self_linear_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %sample : torch.Tensor [#users=1] = placeholder[target=sample]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_linear_1 : [#users=1] = call_module[target=self_linear_1](args = (%sample,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1280, 320), y=float16(320, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_act : [#users=1] = call_module[target=self_act](args = (%self_linear_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_linear_2 : [#users=1] = call_module[target=self_linear_2](args = (%self_act,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1280, 1280), y=float16(1280, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: return (self_linear_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(320, 1280), out=float16(1, 320, 1280))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 320, 1280), y=float16(1, 8, 40, 1280), shape=[1, 8, 40, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 8, 40, 1280), y=float16(8, 40, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(1280, 1280), out=float16(1, 1280, 1280))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 1280, 1280), y=float16(1, 8, 160, 1280), shape=[1, 8, 160, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 8, 160, 1280), y=float16(8, 160, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(1, 8, 2, 1280), y=float16(1, 2, 1280), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 40, 1280), data=float16(2, 320), y=float16(1, 8, 2, 1280), fused_ops='broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 160, 1280), y=float16(1280,), x=float16(1, 2, 1280), y=float16(1, 8, 2, 1280), fused_ops='reshape add silu broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(y=float16(1280,), x=float16(1, 2, 1280), z=float16(2, 1280), fused_ops='reshape add', anchor='add')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 81 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "    %temb : torch.Tensor [#users=2] = placeholder[target=temb]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=2] = placeholder[target=encoder_hidden_states]\n",
            "    %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%hidden_states,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "    %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "    %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "    %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%hidden_states, %self_resnets_0_conv2), kwargs = {})\n",
            "    %truediv : [#users=2] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    %self_attentions_0_norm : [#users=1] = call_module[target=self_attentions_0_norm](args = (%truediv,), kwargs = {})\n",
            "    %self_attentions_0_proj_in : [#users=1] = call_module[target=self_attentions_0_proj_in](args = (%self_attentions_0_norm,), kwargs = {})\n",
            "    %permute : [#users=1] = call_method[target=permute](args = (%self_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape : [#users=2] = call_method[target=reshape](args = (%permute, 2, 4096, 320), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm1](args = (%reshape,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn1](args = (%self_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_2 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn1, %reshape), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn2](args = (%self_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_3 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn2, %add_2), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm3](args = (%add_3,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_2,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem_1, %gelu), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_ff_net_2, %add_3), kwargs = {})\n",
            "    %reshape_1 : [#users=1] = call_method[target=reshape](args = (%add_4, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_1 : [#users=1] = call_method[target=permute](args = (%reshape_1, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous : [#users=1] = call_method[target=contiguous](args = (%permute_1,), kwargs = {})\n",
            "    %self_attentions_0_proj_out : [#users=1] = call_module[target=self_attentions_0_proj_out](args = (%contiguous,), kwargs = {})\n",
            "    %add_5 : [#users=3] = call_function[target=operator.add](args = (%self_attentions_0_proj_out, %truediv), kwargs = {})\n",
            "    %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%add_5,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "    %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_6 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_3), kwargs = {})\n",
            "    %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_6,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "    %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "    %add_7 : [#users=1] = call_function[target=operator.add](args = (%add_5, %self_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=2] = call_function[target=operator.truediv](args = (%add_7, 1.0), kwargs = {})\n",
            "    %self_attentions_1_norm : [#users=1] = call_module[target=self_attentions_1_norm](args = (%truediv_1,), kwargs = {})\n",
            "    %self_attentions_1_proj_in : [#users=1] = call_module[target=self_attentions_1_proj_in](args = (%self_attentions_1_norm,), kwargs = {})\n",
            "    %permute_2 : [#users=1] = call_method[target=permute](args = (%self_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_2 : [#users=2] = call_method[target=reshape](args = (%permute_2, 2, 4096, 320), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm1](args = (%reshape_2,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_attn1](args = (%self_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_8 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_attn1, %reshape_2), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm2](args = (%add_8,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_attn2](args = (%self_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_9 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_attn2, %add_8), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm3](args = (%add_9,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_1 : [#users=2] = call_method[target=chunk](args = (%self_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_4 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 0), kwargs = {})\n",
            "    %getitem_5 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 1), kwargs = {})\n",
            "    %gelu_1 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_5,), kwargs = {})\n",
            "    %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%getitem_4, %gelu_1), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_1,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_10 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_ff_net_2, %add_9), kwargs = {})\n",
            "    %reshape_3 : [#users=1] = call_method[target=reshape](args = (%add_10, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_3 : [#users=1] = call_method[target=permute](args = (%reshape_3, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_1 : [#users=1] = call_method[target=contiguous](args = (%permute_3,), kwargs = {})\n",
            "    %self_attentions_1_proj_out : [#users=1] = call_module[target=self_attentions_1_proj_out](args = (%contiguous_1,), kwargs = {})\n",
            "    %add_11 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_1_proj_out, %truediv_1), kwargs = {})\n",
            "    %self_downsamplers_0_conv : [#users=1] = call_module[target=self_downsamplers_0_conv](args = (%add_11,), kwargs = {})\n",
            "    return (self_downsamplers_0_conv, add_5, add_11)\n",
            "[2023-06-07 17:51:28,652] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_blocks.py line 881 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 320, 64, 64), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(320,), y=float16(320, 1, 1), shape=[320, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(320,), y=float16(1, 320, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(320, 1280), y=float16(1280, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "Compiling cuda task \u001b[92mfull(c=float16(), shape=[], value=half(1.0), dtype='float16')\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(320, 1, 1), y=float16(1, 320, 1, 1))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(1280, 320), out=float16(1, 1280, 320))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 1280, 320), y=float16(1, 8, 160, 320), shape=[1, 8, 160, 320])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 8, 160, 320), y=float16(8, 160, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(320, 320, 3, 3), y=float16(1, 320, 320, 3, 3), shape=[1, 320, 320, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 320, 320, 3, 3), y=float16(1, 2880, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(1, 8, 2, 320), y=float16(1, 2, 320), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 160, 320), x=float16(2, 1280), y=float16(1, 8, 2, 320), fused_ops='silu broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 320, 64, 64), y=float16(2, 32, 10, 64, 64), shape=[2, 32, 10, 64, 64])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 10, 64, 64), y=float16(2, 32, 10, 64, 64), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2880, 320), y=float16(1, 320, 1, 1), y=float16(320,), y=float16(320, 1, 1), x=float16(1, 2, 320), y=float16(320, 1, 1), x=float16(2, 32, 10, 64, 64), y=float16(2, 32, 10, 64, 64), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2880, 320), y=float16(1, 320, 1, 1), x=float16(2, 320, 64, 64), y=float16(320, 1, 1), y=float16(320, 1, 1), x=float16(2, 32, 10, 64, 64), y=float16(2, 320, 64, 64), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 28 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=1] = placeholder[target=encoder_hidden_states]\n",
            "    %self_norm : [#users=1] = call_module[target=self_norm](args = (%hidden_states,), kwargs = {})\n",
            "    %self_proj_in : [#users=1] = call_module[target=self_proj_in](args = (%self_norm,), kwargs = {})\n",
            "    %permute : [#users=1] = call_method[target=permute](args = (%self_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape : [#users=2] = call_method[target=reshape](args = (%permute, 2, 4096, 320), kwargs = {})\n",
            "    %self_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_transformer_blocks_0_norm1](args = (%reshape,), kwargs = {})\n",
            "    %self_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_transformer_blocks_0_attn1](args = (%self_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add : [#users=2] = call_function[target=operator.add](args = (%self_transformer_blocks_0_attn1, %reshape), kwargs = {})\n",
            "    %self_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_transformer_blocks_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_transformer_blocks_0_attn2](args = (%self_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_1 : [#users=2] = call_function[target=operator.add](args = (%self_transformer_blocks_0_attn2, %add), kwargs = {})\n",
            "    %self_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_transformer_blocks_0_norm3](args = (%add_1,), kwargs = {})\n",
            "    %self_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_transformer_blocks_0_ff_net_0_proj](args = (%self_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_transformer_blocks_0_ff_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_transformer_blocks_0_ff_net_2](args = (%self_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_transformer_blocks_0_ff_net_2, %add_1), kwargs = {})\n",
            "    %reshape_1 : [#users=1] = call_method[target=reshape](args = (%add_2, 2, 64, 64, 320), kwargs = {})\n",
            "    %permute_1 : [#users=1] = call_method[target=permute](args = (%reshape_1, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous : [#users=1] = call_method[target=contiguous](args = (%permute_1,), kwargs = {})\n",
            "    %self_proj_out : [#users=1] = call_module[target=self_proj_out](args = (%contiguous,), kwargs = {})\n",
            "    %add_3 : [#users=1] = call_function[target=operator.add](args = (%self_proj_out, %hidden_states), kwargs = {})\n",
            "    return (add_3,)\n",
            "[2023-06-07 17:52:51,199] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/transformer_2d.py line 214 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 19 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=1] = placeholder[target=encoder_hidden_states]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%hidden_states,), kwargs = {})\n",
            "    %self_attn1 : [#users=1] = call_module[target=self_attn1](args = (%self_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add : [#users=2] = call_function[target=operator.add](args = (%self_attn1, %hidden_states), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_attn2 : [#users=1] = call_module[target=self_attn2](args = (%self_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_1 : [#users=2] = call_function[target=operator.add](args = (%self_attn2, %add), kwargs = {})\n",
            "    %self_norm3 : [#users=1] = call_module[target=self_norm3](args = (%add_1,), kwargs = {})\n",
            "    %self_ff_net_0_proj : [#users=1] = call_module[target=self_ff_net_0_proj](args = (%self_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_ff_net_1 : [#users=1] = call_module[target=self_ff_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_ff_net_2 : [#users=1] = call_module[target=self_ff_net_2](args = (%self_ff_net_1,), kwargs = {})\n",
            "    %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_ff_net_2, %add_1), kwargs = {})\n",
            "    return (add_2,)\n",
            "[2023-06-07 17:52:51,339] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/attention.py line 122 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 18 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "    %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "    %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "    %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "    %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "    %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "    %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "    %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "    %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "    %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(320, 320), y=float16(320, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "Compiling cuda task \u001b[92mfull(c=float16(), shape=[], value=half(6.324555320336759), dtype='float16')\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mconcat(x0=float16(320, 320), x1=float16(320, 320), x2=float16(320, 320), out=float16(320, 960))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: matmul(x, c1)|matmul(x, c2)|matmul(x, c3) => matmul(x, concat(c1, c2, c3)) followed by split\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(320, 960), out=float16(2, 320, 960))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(320, 320), out=float16(2, 320, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mbatch_matmul(a=float16(2, 4096, 320), b=float16(2, 320, 960), c=float16(2, 4096, 960), batch_size=2, m_size=4096, n_size=960, k_size=320, mma='mma')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 4096, 960), y=float16(2, 8, 4096, 40), fused_ops='slice reshape rearrange', anchor='rearrange')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 4096, 960), y=float16(2, 8, 40, 4096), fused_ops='slice reshape rearrange rearrange divs', anchor='divs')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 4096, 960), y=float16(2, 8, 4096, 40), fused_ops='slice reshape rearrange', anchor='rearrange')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mattn(q=float16(2, 8, 4096, 40), k=float16(2, 8, 40, 4096), v=float16(2, 8, 4096, 40), o=float16(2, 8, 4096, 40), is_causal=False)\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 320, 320), y=float16(320,), x=float16(2, 8, 4096, 40), y=float16(2, 4096, 320), fused_ops='rearrange reshape batch_matmul add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 19 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=2] = placeholder[target=encoder_hidden_states]\n",
            "    %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%encoder_hidden_states,), kwargs = {})\n",
            "    %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%encoder_hidden_states,), kwargs = {})\n",
            "    %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "    %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "    %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "    %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "    %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "    %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "    %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "    %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 77, 768), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %encoder_hidden_states : torch.Tensor [#users=2] = placeholder[target=encoder_hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%encoder_hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(320, 768), y=float16(768, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%encoder_hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 18: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mconcat(x0=float16(768, 320), x1=float16(768, 320), out=float16(768, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: matmul(x, c1)|matmul(x, c2) ==> matmul(x, concat(c1, c2)) followed by split\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(768, 640), out=float16(2, 768, 640))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 768, 640), y=float16(2, 4, 192, 640), shape=[2, 4, 192, 640])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(2, 4, 192, 640), y=float16(8, 192, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 192, 640), x=float16(2, 77, 768), y=float16(2, 4, 77, 640), fused_ops='reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 4096, 320), b=float16(2, 320, 320), y=float16(16, 4096, 40), fused_ops='batch_matmul reshape rearrange rearrange', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(2, 4, 77, 640), y=float16(2, 77, 640), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(a=float16(16, 4096, 40), data=float16(2, 77, 640), y=float16(2, 8, 4096, 77), fused_ops='slice reshape rearrange rearrange divs rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(x=float16(2, 8, 4096, 77), y=float16(16, 4096, 77), fused_ops='softmax rearrange', anchor='softmax')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(a=float16(16, 4096, 77), data=float16(2, 77, 640), y=float16(2, 4096, 320), fused_ops='slice reshape rearrange rearrange batch_matmul reshape rearrange reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 4096, 320), b=float16(2, 320, 320), y=float16(320,), y=float16(2, 4096, 320), fused_ops='batch_matmul add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(2560, 320), y=float16(320, 2560))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(320, 2560), out=float16(2, 320, 2560))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(1280, 320), out=float16(2, 1280, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 4096, 320), b=float16(2, 320, 2560), y=float16(2560,), z=float16(2, 4096, 2560), fused_ops='batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 1280, 320), y=float16(320,), data=float16(2, 4096, 2560), z=float16(2, 4096, 320), fused_ops='slice slice gelu mul batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 320, 64, 64), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 18 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "    %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "    %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "    %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "    %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "    %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "    %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "    %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "    %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "    %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: matmul(x, c1)|matmul(x, c2)|matmul(x, c3) => matmul(x, concat(c1, c2, c3)) followed by split\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 19 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=2] = placeholder[target=encoder_hidden_states]\n",
            "    %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%encoder_hidden_states,), kwargs = {})\n",
            "    %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%encoder_hidden_states,), kwargs = {})\n",
            "    %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "    %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "    %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "    %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "    %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "    %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "    %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "    %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "    %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 77, 768), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %encoder_hidden_states : torch.Tensor [#users=2] = placeholder[target=encoder_hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%encoder_hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%encoder_hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 40), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 320), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 18: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: matmul(x, c1)|matmul(x, c2) ==> matmul(x, concat(c1, c2)) followed by split\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 4096, 320), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 3 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "    return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 320, 64, 64), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2880, 320), y=float16(1, 320, 1, 1), data=float16(2, 320, 64, 64), z=float16(2, 320, 32, 32), fused_ops='pad conv2d_gemm_image_transform batch_matmul reshape rearrange add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 320, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(640,), y=float16(1, 640, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(640, 1280), y=float16(1280, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640,), y=float16(640, 1, 1), shape=[640, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92madd(x=float16(1, 640, 1, 1), y=float16(1, 640, 1, 1), z=float16(1, 640, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(640, 1, 1), y=float16(1, 640, 1, 1))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(1280, 640), out=float16(1, 1280, 640))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 1280, 640), y=float16(1, 8, 160, 640), shape=[1, 8, 160, 640])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 8, 160, 640), y=float16(8, 160, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 320, 1, 1), y=float16(1, 640, 320, 1, 1), shape=[1, 640, 320, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 320, 1, 1), y=float16(1, 320, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 320, 3, 3), y=float16(1, 640, 320, 3, 3), shape=[1, 640, 320, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 320, 3, 3), y=float16(1, 2880, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 640, 3, 3), y=float16(1, 640, 640, 3, 3), shape=[1, 640, 640, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 640, 3, 3), y=float16(1, 5760, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(1, 8, 2, 640), y=float16(1, 2, 640), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 160, 640), x=float16(2, 1280), y=float16(1, 8, 2, 640), fused_ops='silu broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 320, 32, 32), y=float16(2, 32, 10, 32, 32), shape=[2, 32, 10, 32, 32])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 10, 32, 32), y=float16(2, 32, 10, 32, 32), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2880, 640), y=float16(1, 640, 1, 1), y=float16(640,), y=float16(320, 1, 1), x=float16(1, 2, 640), y=float16(320, 1, 1), x=float16(2, 32, 10, 32, 32), y=float16(2, 32, 20, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 20, 32, 32), y=float16(2, 32, 20, 32, 32), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 5760, 640), y=float16(640, 1, 1), y=float16(640, 1, 1), x=float16(2, 32, 20, 32, 32), y=float16(2, 640, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 320, 640), y=float16(2, 640, 32, 32), y=float16(1, 640, 1, 1), x=float16(2, 320, 32, 32), y=float16(2, 640, 32, 32), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 18 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "    %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "    %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "    %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 80), kwargs = {})\n",
            "    %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "    %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 80), kwargs = {})\n",
            "    %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "    %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 80), kwargs = {})\n",
            "    %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "    %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "    %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "    %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 640), kwargs = {})\n",
            "    %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "    %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "    %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=3] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %attn_to_q : [#users=1] = call_module[target=attn_to_q](args = (%hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(640, 640), y=float16(640, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %attn_to_k : [#users=1] = call_module[target=attn_to_k](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %attn_to_v : [#users=1] = call_module[target=attn_to_v](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %view : [#users=1] = call_method[target=view](args = (%attn_to_q, 2, -1, 8, 80), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %transpose : [#users=1] = call_method[target=transpose](args = (%view, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %view_1 : [#users=1] = call_method[target=view](args = (%attn_to_k, 2, -1, 8, 80), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %transpose_1 : [#users=1] = call_method[target=transpose](args = (%view_1, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %view_2 : [#users=1] = call_method[target=view](args = (%attn_to_v, 2, -1, 8, 80), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %transpose_2 : [#users=1] = call_method[target=transpose](args = (%view_2, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %scaled_dot_product_attention : [#users=1] = call_function[target=torch._C._nn.scaled_dot_product_attention](args = (%transpose, %transpose_1, %transpose_2), kwargs = {attn_mask: None, dropout_p: 0.0, is_causal: False})\n",
            "Compiling cuda task \u001b[92mfull(c=float16(), shape=[], value=half(8.94427190999916), dtype='float16')\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %transpose_3 : [#users=1] = call_method[target=transpose](args = (%scaled_dot_product_attention, 1, 2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %reshape : [#users=1] = call_method[target=reshape](args = (%transpose_3, 2, -1, 640), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %to : [#users=1] = call_method[target=to](args = (%reshape, torch.float16), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %sub0_0 : [#users=1] = call_module[target=sub0_0](args = (%to,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %sub1_1 : [#users=1] = call_module[target=sub1_1](args = (%sub0_0,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%sub1_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mconcat(x0=float16(640, 640), x1=float16(640, 640), x2=float16(640, 640), out=float16(640, 1920))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: matmul(x, c1)|matmul(x, c2)|matmul(x, c3) => matmul(x, concat(c1, c2, c3)) followed by split\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(640, 1920), out=float16(2, 640, 1920))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(640, 640), out=float16(2, 640, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mbatch_matmul(a=float16(2, 1024, 640), b=float16(2, 640, 1920), c=float16(2, 1024, 1920), batch_size=2, m_size=1024, n_size=1920, k_size=640, mma='mma')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 1024, 1920), y=float16(2, 8, 1024, 80), fused_ops='slice reshape rearrange', anchor='rearrange')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 1024, 1920), y=float16(2, 8, 80, 1024), fused_ops='slice reshape rearrange rearrange divs', anchor='divs')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(data=float16(2, 1024, 1920), y=float16(2, 8, 1024, 80), fused_ops='slice reshape rearrange', anchor='rearrange')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mattn(q=float16(2, 8, 1024, 80), k=float16(2, 8, 80, 1024), v=float16(2, 8, 1024, 80), o=float16(2, 8, 1024, 80), is_causal=False)\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 640, 640), y=float16(640,), x=float16(2, 8, 1024, 80), y=float16(2, 1024, 640), fused_ops='rearrange reshape batch_matmul add divs', anchor='batch_matmul')\u001b[0m...\n",
            "[2023-06-07 17:58:24,069] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT __call__ /usr/local/lib/python3.10/dist-packages/diffusers/models/attention_processor.py line 1076 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 272, in build_task_batch\n",
            "    raise RuntimeError('\\n'.join(msg))\n",
            "RuntimeError: Failed to build 1 tasks:\n",
            "  [cuda] attn(q=float16(2, 8, 1024, 80), k=float16(2, 8, 80, 1024), v=float16(2, 8, 1024, 80), o=float16(2, 8, 1024, 80), is_causal=False)\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 248, in build_job\n",
            "        build_task(task, target, load=False)\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 233, in build_task\n",
            "        build_task_module(task, candidates, task_dir, target)\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 61, in build_task_module\n",
            "        raise ValueError('No candidate found.')\n",
            "    ValueError: No candidate found.\n",
            "    \n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised RuntimeError: Failed to build 1 tasks:\n",
            "  [cuda] attn(q=float16(2, 8, 1024, 80), k=float16(2, 8, 80, 1024), v=float16(2, 8, 1024, 80), o=float16(2, 8, 1024, 80), is_causal=False)\n",
            "\n",
            "    Traceback (most recent call last):\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 248, in build_job\n",
            "        build_task(task, target, load=False)\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 233, in build_task\n",
            "        build_task_module(task, candidates, task_dir, target)\n",
            "      File \"/usr/local/lib/python3.10/dist-packages/hidet/drivers/build_task.py\", line 61, in build_task_module\n",
            "        raise ValueError('No candidate found.')\n",
            "    ValueError: No candidate found.\n",
            "    \n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(5120, 640), y=float16(640, 5120))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(640, 2560), y=float16(2560, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(640, 5120), out=float16(2, 640, 5120))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(2560, 640), out=float16(2, 2560, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 1024, 640), b=float16(2, 640, 5120), y=float16(5120,), z=float16(2, 1024, 5120), fused_ops='batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 2560, 640), y=float16(640,), data=float16(2, 1024, 5120), z=float16(2, 1024, 640), fused_ops='slice slice gelu mul batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 640, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 640, 32, 32), y=float16(2, 32, 20, 32, 32), shape=[2, 32, 20, 32, 32])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 5760, 640), y=float16(1, 640, 1, 1), y=float16(640,), y=float16(640, 1, 1), x=float16(1, 2, 640), y=float16(640, 1, 1), x=float16(2, 32, 20, 32, 32), y=float16(2, 32, 20, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 5760, 640), y=float16(1, 640, 1, 1), x=float16(2, 640, 32, 32), y=float16(640, 1, 1), y=float16(640, 1, 1), x=float16(2, 32, 20, 32, 32), y=float16(2, 640, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 3 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "    return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 640, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 5760, 640), y=float16(1, 2, 2880, 640), shape=[1, 2, 2880, 640])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 2, 2880, 640), y=float16(2, 2880, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 2880, 640), data=float16(2, 640, 32, 32), y=float16(1, 2, 512, 640), fused_ops='pad conv2d_gemm_image_transform reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(1, 2, 512, 640), y=float16(1, 512, 640), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(y=float16(1, 640, 1, 1), x=float16(1, 512, 640), z=float16(2, 640, 16, 16), fused_ops='reshape rearrange add', anchor='add')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 640, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1280,), y=float16(1, 1280, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280,), y=float16(1280, 1, 1), shape=[1280, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92madd(x=float16(1, 1280, 1, 1), y=float16(1, 1280, 1, 1), z=float16(1, 1280, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1280, 1, 1), y=float16(1, 1280, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 640, 1, 1), y=float16(1, 1280, 640, 1, 1), shape=[1, 1280, 640, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 640, 1, 1), y=float16(1, 640, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 640, 3, 3), y=float16(1, 1280, 640, 3, 3), shape=[1, 1280, 640, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 640, 3, 3), y=float16(1, 5760, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 1280, 3, 3), y=float16(1, 1280, 1280, 3, 3), shape=[1, 1280, 1280, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 1280, 3, 3), y=float16(1, 11520, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 160, 1280), x=float16(2, 1280), y=float16(1, 8, 2, 1280), fused_ops='silu broadcast reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 640, 16, 16), y=float16(2, 32, 20, 16, 16), shape=[2, 32, 20, 16, 16])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 20, 16, 16), y=float16(2, 32, 20, 16, 16), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 5760, 1280), y=float16(1, 1280, 1, 1), y=float16(1280,), y=float16(640, 1, 1), x=float16(1, 2, 1280), y=float16(640, 1, 1), x=float16(2, 32, 20, 16, 16), y=float16(2, 32, 40, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 40, 16, 16), y=float16(2, 32, 40, 16, 16), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 1280), y=float16(1280, 1, 1), y=float16(1280, 1, 1), x=float16(2, 32, 40, 16, 16), y=float16(2, 1280, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 640, 1280), y=float16(2, 1280, 16, 16), y=float16(1, 1280, 1, 1), x=float16(2, 640, 16, 16), y=float16(2, 1280, 16, 16), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 256, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(10240, 1280), y=float16(1280, 10240))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1280, 5120), y=float16(5120, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(1280, 10240), out=float16(2, 1280, 10240))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mbroadcast(data=float16(5120, 1280), out=float16(2, 5120, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 256, 1280), b=float16(2, 1280, 10240), y=float16(10240,), z=float16(2, 256, 10240), fused_ops='batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(2, 5120, 1280), y=float16(1280,), data=float16(2, 256, 10240), z=float16(2, 256, 1280), fused_ops='slice slice gelu mul batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 1280, 16, 16), y=float16(2, 32, 40, 16, 16), shape=[2, 32, 40, 16, 16])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 1280), y=float16(1, 1280, 1, 1), y=float16(1280,), y=float16(1280, 1, 1), x=float16(1, 2, 1280), y=float16(1280, 1, 1), x=float16(2, 32, 40, 16, 16), y=float16(2, 32, 40, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 1280), y=float16(1, 1280, 1, 1), x=float16(2, 1280, 16, 16), y=float16(1280, 1, 1), y=float16(1280, 1, 1), x=float16(2, 32, 40, 16, 16), y=float16(2, 1280, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 256, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 3 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "    return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_conv : [#users=1] = call_module[target=self_conv](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 11520, 1280), y=float16(1, 4, 2880, 1280), shape=[1, 4, 2880, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 4, 2880, 1280), y=float16(4, 2880, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(1, 4, 128, 1280), y=float16(1, 128, 1280), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(4, 2880, 1280), data=float16(2, 1280, 16, 16), y=float16(1, 4, 128, 1280), fused_ops='pad conv2d_gemm_image_transform reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(y=float16(1, 1280, 1, 1), x=float16(1, 128, 1280), z=float16(2, 1280, 8, 8), fused_ops='reshape rearrange add', anchor='add')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 29 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "    %temb : torch.Tensor [#users=2] = placeholder[target=temb]\n",
            "    %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%hidden_states,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "    %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "    %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "    %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%hidden_states, %self_resnets_0_conv2), kwargs = {})\n",
            "    %truediv : [#users=3] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%truediv,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "    %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_1), kwargs = {})\n",
            "    %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "    %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "    %add_3 : [#users=1] = call_function[target=operator.add](args = (%truediv, %self_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=1] = call_function[target=operator.truediv](args = (%add_3, 1.0), kwargs = {})\n",
            "    return (truediv_1, truediv)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=2] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%hidden_states, %self_resnets_0_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=3] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%truediv,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 18: %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 19: %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 20: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 21: %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 22: %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 23: %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 24: %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 25: %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 26: %add_3 : [#users=1] = call_function[target=operator.add](args = (%truediv, %self_resnets_1_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 27: %truediv_1 : [#users=1] = call_function[target=operator.truediv](args = (%add_3, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 28: return (truediv_1, truediv)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 1280, 8, 8), y=float16(2, 32, 40, 8, 8), shape=[2, 32, 40, 8, 8])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 40, 8, 8), y=float16(2, 32, 40, 8, 8), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(4, 2880, 1280), y=float16(1280, 1, 1), y=float16(1280, 1, 1), x=float16(2, 32, 40, 8, 8), y=float16(1, 4, 128, 1280), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(y=float16(1, 1280, 1, 1), x=float16(1, 128, 1280), y=float16(1280,), x=float16(1, 2, 1280), y=float16(2, 32, 40, 8, 8), fused_ops='reshape rearrange add reshape add rearrange slice add reshape', anchor='reshape')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(x=float16(2, 1280, 8, 8), y=float16(1, 1280, 1, 1), x=float16(1, 128, 1280), y=float16(2, 1280, 8, 8), fused_ops='reshape rearrange add add divs', anchor='divs')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 55 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=2] = placeholder[target=hidden_states]\n",
            "    %temb : torch.Tensor [#users=2] = placeholder[target=temb]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=1] = placeholder[target=encoder_hidden_states]\n",
            "    %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%hidden_states,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "    %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "    %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "    %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%hidden_states, %self_resnets_0_conv2), kwargs = {})\n",
            "    %truediv : [#users=2] = call_function[target=operator.truediv](args = (%add_1, 1), kwargs = {})\n",
            "    %self_attentions_0_norm : [#users=1] = call_module[target=self_attentions_0_norm](args = (%truediv,), kwargs = {})\n",
            "    %self_attentions_0_proj_in : [#users=1] = call_module[target=self_attentions_0_proj_in](args = (%self_attentions_0_norm,), kwargs = {})\n",
            "    %permute : [#users=1] = call_method[target=permute](args = (%self_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape : [#users=2] = call_method[target=reshape](args = (%permute, 2, 64, 1280), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm1](args = (%reshape,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn1](args = (%self_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_2 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn1, %reshape), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn2](args = (%self_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_3 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn2, %add_2), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm3](args = (%add_3,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_2,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem_1, %gelu), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_ff_net_2, %add_3), kwargs = {})\n",
            "    %reshape_1 : [#users=1] = call_method[target=reshape](args = (%add_4, 2, 8, 8, 1280), kwargs = {})\n",
            "    %permute_1 : [#users=1] = call_method[target=permute](args = (%reshape_1, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous : [#users=1] = call_method[target=contiguous](args = (%permute_1,), kwargs = {})\n",
            "    %self_attentions_0_proj_out : [#users=1] = call_module[target=self_attentions_0_proj_out](args = (%contiguous,), kwargs = {})\n",
            "    %add_5 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_proj_out, %truediv), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_norm1 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_norm1](args = (%add_5,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_nonlinearity : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_nonlinearity](args = (%self_resnets_slice_1__none__none___0_norm1,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_conv1 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_conv1](args = (%self_resnets_slice_1__none__none___0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_time_emb_proj : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_time_emb_proj](args = (%self_resnets_slice_1__none__none___0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_slice_1__none__none___0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_6 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_slice_1__none__none___0_conv1, %getitem_3), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_norm2 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_norm2](args = (%add_6,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_nonlinearity](args = (%self_resnets_slice_1__none__none___0_norm2,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_dropout : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_dropout](args = (%self_resnets_slice_1__none__none___0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_slice_1__none__none___0_conv2 : [#users=1] = call_module[target=self_resnets_slice_1__None__None___0_conv2](args = (%self_resnets_slice_1__none__none___0_dropout,), kwargs = {})\n",
            "    %add_7 : [#users=1] = call_function[target=operator.add](args = (%add_5, %self_resnets_slice_1__none__none___0_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=1] = call_function[target=operator.truediv](args = (%add_7, 1), kwargs = {})\n",
            "    return (truediv_1,)\n",
            "[2023-06-07 18:04:02,271] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_blocks.py line 563 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "[2023-06-07 18:04:04,489] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT __init__ /usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py line 276 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/exc.py\", line 71, in unimplemented\n",
            "    raise Unsupported(msg)\n",
            "torch._dynamo.exc.Unsupported: Guard setup for uninitialized class <class 'torch.nn.modules.container.ModuleList'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 64, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 5120, 1280), y=float16(2, 4, 1280, 1280), shape=[2, 4, 1280, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(2, 4, 1280, 1280), y=float16(8, 1280, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(a=float16(2, 64, 1280), b=float16(2, 1280, 10240), y=float16(10240,), z=float16(2, 64, 10240), fused_ops='batch_matmul add', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(8, 1280, 1280), data=float16(2, 64, 10240), y=float16(2, 4, 64, 1280), fused_ops='slice slice gelu mul reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreduce_sum_f16(x=float16(2, 4, 64, 1280), y=float16(2, 64, 1280), dims=[1], keep_dim=False, reduce_type=sum, accumulate_dtype='float32')\u001b[0m...\n",
            "Compiling cuda task \u001b[92madd(x=float16(2, 64, 1280), y=float16(1280,), z=float16(2, 64, 1280))\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 16 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %add_1 : [#users=1] = call_function[target=operator.add](args = (%input_tensor, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 53 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %res_hidden_states_tuple_0_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_0_]\n",
            "    %res_hidden_states_tuple_1_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_1_]\n",
            "    %res_hidden_states_tuple_2_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_2_]\n",
            "    %temb : torch.Tensor [#users=3] = placeholder[target=temb]\n",
            "    %cat : [#users=2] = call_function[target=torch.cat](args = ([%hidden_states, %res_hidden_states_tuple_2_],), kwargs = {dim: 1})\n",
            "    %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%cat,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "    %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "    %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "    %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "    %self_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_resnets_0_conv_shortcut](args = (%cat,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv_shortcut, %self_resnets_0_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    %cat_1 : [#users=2] = call_function[target=torch.cat](args = ([%truediv, %res_hidden_states_tuple_1_],), kwargs = {dim: 1})\n",
            "    %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%cat_1,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "    %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_1), kwargs = {})\n",
            "    %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "    %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "    %self_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_resnets_1_conv_shortcut](args = (%cat_1,), kwargs = {})\n",
            "    %add_3 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv_shortcut, %self_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=1] = call_function[target=operator.truediv](args = (%add_3, 1.0), kwargs = {})\n",
            "    %cat_2 : [#users=2] = call_function[target=torch.cat](args = ([%truediv_1, %res_hidden_states_tuple_0_],), kwargs = {dim: 1})\n",
            "    %self_resnets_2_norm1 : [#users=1] = call_module[target=self_resnets_2_norm1](args = (%cat_2,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm1,), kwargs = {})\n",
            "    %self_resnets_2_conv1 : [#users=1] = call_module[target=self_resnets_2_conv1](args = (%self_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_resnets_2_time_emb_proj](args = (%self_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv1, %getitem_2), kwargs = {})\n",
            "    %self_resnets_2_norm2 : [#users=1] = call_module[target=self_resnets_2_norm2](args = (%add_4,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm2,), kwargs = {})\n",
            "    %self_resnets_2_dropout : [#users=1] = call_module[target=self_resnets_2_dropout](args = (%self_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_2_conv2 : [#users=1] = call_module[target=self_resnets_2_conv2](args = (%self_resnets_2_dropout,), kwargs = {})\n",
            "    %self_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_resnets_2_conv_shortcut](args = (%cat_2,), kwargs = {})\n",
            "    %add_5 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv_shortcut, %self_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_2 : [#users=1] = call_function[target=operator.truediv](args = (%add_5, 1.0), kwargs = {})\n",
            "    %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%truediv_2,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_upsamplers_0_conv : [#users=1] = call_module[target=self_upsamplers_0_conv](args = (%interpolate,), kwargs = {})\n",
            "    return (self_upsamplers_0_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 8, 8), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %res_hidden_states_tuple_0_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_0_]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %res_hidden_states_tuple_1_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_1_]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %res_hidden_states_tuple_2_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_2_]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %temb : torch.Tensor [#users=3] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %cat : [#users=2] = call_function[target=torch.cat](args = ([%hidden_states, %res_hidden_states_tuple_2_],), kwargs = {dim: 1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%cat,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2560,), y=float16(2560, 1, 1), shape=[2560, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 17: %self_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_resnets_0_conv_shortcut](args = (%cat,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 18: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv_shortcut, %self_resnets_0_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 19: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 20: %cat_1 : [#users=2] = call_function[target=torch.cat](args = ([%truediv, %res_hidden_states_tuple_1_],), kwargs = {dim: 1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 21: %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%cat_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 22: %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 23: %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 24: %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 25: %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 26: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 27: %add_2 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 28: %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 29: %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 30: %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 31: %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 32: %self_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_resnets_1_conv_shortcut](args = (%cat_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 33: %add_3 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv_shortcut, %self_resnets_1_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 34: %truediv_1 : [#users=1] = call_function[target=operator.truediv](args = (%add_3, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 35: %cat_2 : [#users=2] = call_function[target=torch.cat](args = ([%truediv_1, %res_hidden_states_tuple_0_],), kwargs = {dim: 1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 36: %self_resnets_2_norm1 : [#users=1] = call_module[target=self_resnets_2_norm1](args = (%cat_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 37: %self_resnets_2_nonlinearity : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 38: %self_resnets_2_conv1 : [#users=1] = call_module[target=self_resnets_2_conv1](args = (%self_resnets_2_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 39: %self_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 40: %self_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_resnets_2_time_emb_proj](args = (%self_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 41: %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 42: %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv1, %getitem_2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 43: %self_resnets_2_norm2 : [#users=1] = call_module[target=self_resnets_2_norm2](args = (%add_4,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 44: %self_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 45: %self_resnets_2_dropout : [#users=1] = call_module[target=self_resnets_2_dropout](args = (%self_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 46: %self_resnets_2_conv2 : [#users=1] = call_module[target=self_resnets_2_conv2](args = (%self_resnets_2_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 47: %self_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_resnets_2_conv_shortcut](args = (%cat_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 48: %add_5 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv_shortcut, %self_resnets_2_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 49: %truediv_2 : [#users=1] = call_function[target=operator.truediv](args = (%add_5, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 50: %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%truediv_2,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 51: %self_upsamplers_0_conv : [#users=1] = call_module[target=self_upsamplers_0_conv](args = (%interpolate,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 52: return (self_upsamplers_0_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(2560, 1, 1), y=float16(1, 2560, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 2560, 1, 1), y=float16(1, 1280, 2560, 1, 1), shape=[1, 1280, 2560, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 2560, 1, 1), y=float16(1, 2560, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 2560, 3, 3), y=float16(1, 1280, 2560, 3, 3), shape=[1, 1280, 2560, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 2560, 3, 3), y=float16(1, 23040, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 2560, 1280), y=float16(1, 4, 640, 1280), shape=[1, 4, 640, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 4, 640, 1280), y=float16(4, 640, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1, 23040, 1280), y=float16(1, 4, 5760, 1280), shape=[1, 4, 5760, 1280])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 4, 5760, 1280), y=float16(4, 5760, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mconcat(x0=float16(2, 1280, 8, 8), x1=float16(2, 1280, 8, 8), out=float16(2, 2560, 8, 8))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(4, 640, 1280), x=float16(2, 2560, 8, 8), y=float16(1, 4, 128, 1280), fused_ops='conv2d_gemm_image_transform reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 2560, 8, 8), y=float16(2, 32, 80, 8, 8), shape=[2, 32, 80, 8, 8])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 80, 8, 8), y=float16(2, 32, 80, 8, 8), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(4, 5760, 1280), y=float16(2560, 1, 1), y=float16(2560, 1, 1), x=float16(2, 32, 80, 8, 8), y=float16(1, 4, 128, 1280), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform reshape rearrange batch_matmul reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(x1=float16(2, 1280, 8, 8), y=float16(1, 1280, 1, 1), x=float16(1, 128, 1280), x=float16(1, 128, 1280), out=float16(2, 2560, 8, 8), fused_ops='reshape rearrange reshape rearrange add add divs concat', anchor='concat')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 1280), y=float16(1, 1280, 1, 1), y=float16(1, 1280, 1, 1), x=float16(1, 128, 1280), x=float16(1, 128, 1280), z=float16(2, 1280, 16, 16), fused_ops='reshape rearrange reshape rearrange add add divs resize2d pad conv2d_gemm_image_transform batch_matmul reshape rearrange add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 129 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %res_hidden_states_tuple_0_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_0_]\n",
            "    %res_hidden_states_tuple_1_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_1_]\n",
            "    %res_hidden_states_tuple_2_ : torch.Tensor [#users=1] = placeholder[target=res_hidden_states_tuple_2_]\n",
            "    %temb : torch.Tensor [#users=3] = placeholder[target=temb]\n",
            "    %encoder_hidden_states : torch.Tensor [#users=3] = placeholder[target=encoder_hidden_states]\n",
            "    %cat : [#users=2] = call_function[target=torch.cat](args = ([%hidden_states, %res_hidden_states_tuple_2_],), kwargs = {dim: 1})\n",
            "    %self_resnets_0_norm1 : [#users=1] = call_module[target=self_resnets_0_norm1](args = (%cat,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm1,), kwargs = {})\n",
            "    %self_resnets_0_conv1 : [#users=1] = call_module[target=self_resnets_0_conv1](args = (%self_resnets_0_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_0_time_emb_proj : [#users=1] = call_module[target=self_resnets_0_time_emb_proj](args = (%self_resnets_0_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_0_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv1, %getitem), kwargs = {})\n",
            "    %self_resnets_0_norm2 : [#users=1] = call_module[target=self_resnets_0_norm2](args = (%add,), kwargs = {})\n",
            "    %self_resnets_0_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_0_nonlinearity](args = (%self_resnets_0_norm2,), kwargs = {})\n",
            "    %self_resnets_0_dropout : [#users=1] = call_module[target=self_resnets_0_dropout](args = (%self_resnets_0_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_0_conv2 : [#users=1] = call_module[target=self_resnets_0_conv2](args = (%self_resnets_0_dropout,), kwargs = {})\n",
            "    %self_resnets_0_conv_shortcut : [#users=1] = call_module[target=self_resnets_0_conv_shortcut](args = (%cat,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_0_conv_shortcut, %self_resnets_0_conv2), kwargs = {})\n",
            "    %truediv : [#users=2] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    %self_attentions_0_norm : [#users=1] = call_module[target=self_attentions_0_norm](args = (%truediv,), kwargs = {})\n",
            "    %self_attentions_0_proj_in : [#users=1] = call_module[target=self_attentions_0_proj_in](args = (%self_attentions_0_norm,), kwargs = {})\n",
            "    %permute : [#users=1] = call_method[target=permute](args = (%self_attentions_0_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape : [#users=2] = call_method[target=reshape](args = (%permute, 2, 256, 1280), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm1](args = (%reshape,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn1](args = (%self_attentions_0_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_2 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn1, %reshape), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm2](args = (%add_2,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_attn2](args = (%self_attentions_0_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_3 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_attn2, %add_2), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_norm3](args = (%add_3,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_0_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_attentions_0_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_2 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_2,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem_1, %gelu), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_attentions_0_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_0_transformer_blocks_0_ff_net_2](args = (%self_attentions_0_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_4 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_0_transformer_blocks_0_ff_net_2, %add_3), kwargs = {})\n",
            "    %reshape_1 : [#users=1] = call_method[target=reshape](args = (%add_4, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_1 : [#users=1] = call_method[target=permute](args = (%reshape_1, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous : [#users=1] = call_method[target=contiguous](args = (%permute_1,), kwargs = {})\n",
            "    %self_attentions_0_proj_out : [#users=1] = call_module[target=self_attentions_0_proj_out](args = (%contiguous,), kwargs = {})\n",
            "    %add_5 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_0_proj_out, %truediv), kwargs = {})\n",
            "    %cat_1 : [#users=2] = call_function[target=torch.cat](args = ([%add_5, %res_hidden_states_tuple_1_],), kwargs = {dim: 1})\n",
            "    %self_resnets_1_norm1 : [#users=1] = call_module[target=self_resnets_1_norm1](args = (%cat_1,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm1,), kwargs = {})\n",
            "    %self_resnets_1_conv1 : [#users=1] = call_module[target=self_resnets_1_conv1](args = (%self_resnets_1_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_1_time_emb_proj : [#users=1] = call_module[target=self_resnets_1_time_emb_proj](args = (%self_resnets_1_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_3 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_1_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_6 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv1, %getitem_3), kwargs = {})\n",
            "    %self_resnets_1_norm2 : [#users=1] = call_module[target=self_resnets_1_norm2](args = (%add_6,), kwargs = {})\n",
            "    %self_resnets_1_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_1_nonlinearity](args = (%self_resnets_1_norm2,), kwargs = {})\n",
            "    %self_resnets_1_dropout : [#users=1] = call_module[target=self_resnets_1_dropout](args = (%self_resnets_1_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_1_conv2 : [#users=1] = call_module[target=self_resnets_1_conv2](args = (%self_resnets_1_dropout,), kwargs = {})\n",
            "    %self_resnets_1_conv_shortcut : [#users=1] = call_module[target=self_resnets_1_conv_shortcut](args = (%cat_1,), kwargs = {})\n",
            "    %add_7 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_1_conv_shortcut, %self_resnets_1_conv2), kwargs = {})\n",
            "    %truediv_1 : [#users=2] = call_function[target=operator.truediv](args = (%add_7, 1.0), kwargs = {})\n",
            "    %self_attentions_1_norm : [#users=1] = call_module[target=self_attentions_1_norm](args = (%truediv_1,), kwargs = {})\n",
            "    %self_attentions_1_proj_in : [#users=1] = call_module[target=self_attentions_1_proj_in](args = (%self_attentions_1_norm,), kwargs = {})\n",
            "    %permute_2 : [#users=1] = call_method[target=permute](args = (%self_attentions_1_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_2 : [#users=2] = call_method[target=reshape](args = (%permute_2, 2, 256, 1280), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm1](args = (%reshape_2,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_attn1](args = (%self_attentions_1_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_8 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_attn1, %reshape_2), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm2](args = (%add_8,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_attn2](args = (%self_attentions_1_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_9 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_attn2, %add_8), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_norm3](args = (%add_9,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_1_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_1 : [#users=2] = call_method[target=chunk](args = (%self_attentions_1_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_4 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 0), kwargs = {})\n",
            "    %getitem_5 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_1, 1), kwargs = {})\n",
            "    %gelu_1 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_5,), kwargs = {})\n",
            "    %mul_1 : [#users=1] = call_function[target=operator.mul](args = (%getitem_4, %gelu_1), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_1](args = (%mul_1,), kwargs = {})\n",
            "    %self_attentions_1_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_1_transformer_blocks_0_ff_net_2](args = (%self_attentions_1_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_10 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_1_transformer_blocks_0_ff_net_2, %add_9), kwargs = {})\n",
            "    %reshape_3 : [#users=1] = call_method[target=reshape](args = (%add_10, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_3 : [#users=1] = call_method[target=permute](args = (%reshape_3, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_1 : [#users=1] = call_method[target=contiguous](args = (%permute_3,), kwargs = {})\n",
            "    %self_attentions_1_proj_out : [#users=1] = call_module[target=self_attentions_1_proj_out](args = (%contiguous_1,), kwargs = {})\n",
            "    %add_11 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_1_proj_out, %truediv_1), kwargs = {})\n",
            "    %cat_2 : [#users=2] = call_function[target=torch.cat](args = ([%add_11, %res_hidden_states_tuple_0_],), kwargs = {dim: 1})\n",
            "    %self_resnets_2_norm1 : [#users=1] = call_module[target=self_resnets_2_norm1](args = (%cat_2,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm1,), kwargs = {})\n",
            "    %self_resnets_2_conv1 : [#users=1] = call_module[target=self_resnets_2_conv1](args = (%self_resnets_2_nonlinearity,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity_1 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_resnets_2_time_emb_proj : [#users=1] = call_module[target=self_resnets_2_time_emb_proj](args = (%self_resnets_2_nonlinearity_1,), kwargs = {})\n",
            "    %getitem_6 : [#users=1] = call_function[target=operator.getitem](args = (%self_resnets_2_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add_12 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv1, %getitem_6), kwargs = {})\n",
            "    %self_resnets_2_norm2 : [#users=1] = call_module[target=self_resnets_2_norm2](args = (%add_12,), kwargs = {})\n",
            "    %self_resnets_2_nonlinearity_2 : [#users=1] = call_module[target=self_resnets_2_nonlinearity](args = (%self_resnets_2_norm2,), kwargs = {})\n",
            "    %self_resnets_2_dropout : [#users=1] = call_module[target=self_resnets_2_dropout](args = (%self_resnets_2_nonlinearity_2,), kwargs = {})\n",
            "    %self_resnets_2_conv2 : [#users=1] = call_module[target=self_resnets_2_conv2](args = (%self_resnets_2_dropout,), kwargs = {})\n",
            "    %self_resnets_2_conv_shortcut : [#users=1] = call_module[target=self_resnets_2_conv_shortcut](args = (%cat_2,), kwargs = {})\n",
            "    %add_13 : [#users=1] = call_function[target=operator.add](args = (%self_resnets_2_conv_shortcut, %self_resnets_2_conv2), kwargs = {})\n",
            "    %truediv_2 : [#users=2] = call_function[target=operator.truediv](args = (%add_13, 1.0), kwargs = {})\n",
            "    %self_attentions_2_norm : [#users=1] = call_module[target=self_attentions_2_norm](args = (%truediv_2,), kwargs = {})\n",
            "    %self_attentions_2_proj_in : [#users=1] = call_module[target=self_attentions_2_proj_in](args = (%self_attentions_2_norm,), kwargs = {})\n",
            "    %permute_4 : [#users=1] = call_method[target=permute](args = (%self_attentions_2_proj_in, 0, 2, 3, 1), kwargs = {})\n",
            "    %reshape_4 : [#users=2] = call_method[target=reshape](args = (%permute_4, 2, 256, 1280), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_norm1 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_norm1](args = (%reshape_4,), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_attn1 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_attn1](args = (%self_attentions_2_transformer_blocks_0_norm1,), kwargs = {encoder_hidden_states: None, attention_mask: None})\n",
            "    %add_14 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_2_transformer_blocks_0_attn1, %reshape_4), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_norm2 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_norm2](args = (%add_14,), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_attn2 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_attn2](args = (%self_attentions_2_transformer_blocks_0_norm2,), kwargs = {encoder_hidden_states: %encoder_hidden_states, attention_mask: None})\n",
            "    %add_15 : [#users=2] = call_function[target=operator.add](args = (%self_attentions_2_transformer_blocks_0_attn2, %add_14), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_norm3 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_norm3](args = (%add_15,), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_ff_net_0_proj : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_ff_net_0_proj](args = (%self_attentions_2_transformer_blocks_0_norm3,), kwargs = {})\n",
            "    %chunk_2 : [#users=2] = call_method[target=chunk](args = (%self_attentions_2_transformer_blocks_0_ff_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem_7 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_2, 0), kwargs = {})\n",
            "    %getitem_8 : [#users=1] = call_function[target=operator.getitem](args = (%chunk_2, 1), kwargs = {})\n",
            "    %gelu_2 : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_8,), kwargs = {})\n",
            "    %mul_2 : [#users=1] = call_function[target=operator.mul](args = (%getitem_7, %gelu_2), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_ff_net_1 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_ff_net_1](args = (%mul_2,), kwargs = {})\n",
            "    %self_attentions_2_transformer_blocks_0_ff_net_2 : [#users=1] = call_module[target=self_attentions_2_transformer_blocks_0_ff_net_2](args = (%self_attentions_2_transformer_blocks_0_ff_net_1,), kwargs = {})\n",
            "    %add_16 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_2_transformer_blocks_0_ff_net_2, %add_15), kwargs = {})\n",
            "    %reshape_5 : [#users=1] = call_method[target=reshape](args = (%add_16, 2, 16, 16, 1280), kwargs = {})\n",
            "    %permute_5 : [#users=1] = call_method[target=permute](args = (%reshape_5, 0, 3, 1, 2), kwargs = {})\n",
            "    %contiguous_2 : [#users=1] = call_method[target=contiguous](args = (%permute_5,), kwargs = {})\n",
            "    %self_attentions_2_proj_out : [#users=1] = call_module[target=self_attentions_2_proj_out](args = (%contiguous_2,), kwargs = {})\n",
            "    %add_17 : [#users=1] = call_function[target=operator.add](args = (%self_attentions_2_proj_out, %truediv_2), kwargs = {})\n",
            "    %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%add_17,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_upsamplers_0_conv : [#users=1] = call_module[target=self_upsamplers_0_conv](args = (%interpolate,), kwargs = {})\n",
            "    return (self_upsamplers_0_conv,)\n",
            "[2023-06-07 18:05:58,931] torch._dynamo.convert_frame: [ERROR] WON'T CONVERT forward /usr/local/lib/python3.10/dist-packages/diffusers/models/unet_2d_blocks.py line 1969 \n",
            "due to: \n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/hidet/graph/frontend/torch/interpreter.py\", line 220, in _check_support\n",
            "    raise NotImplementedError(\"\\n\".join(lines))\n",
            "NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "The above exception was the direct cause of the following exception:\n",
            "\n",
            "Traceback (most recent call last):\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_dynamo/output_graph.py\", line 675, in call_user_compiler\n",
            "    raise BackendCompilerFailed(self.compiler_fn, e) from e\n",
            "torch._dynamo.exc.BackendCompilerFailed: hidet_backend raised NotImplementedError: The following modules/functions are not supported by hidet yet:\n",
            "  <class 'diffusers.models.attention_processor.Attention'>\n",
            "\n",
            "Set torch._dynamo.config.verbose=True for more information\n",
            "\n",
            "\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 2560, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 2560, 16, 16), y=float16(2, 32, 80, 16, 16), shape=[2, 32, 80, 16, 16])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 80, 16, 16), y=float16(2, 32, 80, 16, 16), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 23040, 1280), y=float16(1, 1280, 1, 1), y=float16(1280,), y=float16(2560, 1, 1), x=float16(1, 2, 1280), y=float16(2560, 1, 1), x=float16(2, 32, 80, 16, 16), y=float16(2, 32, 40, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2560, 1280), y=float16(2, 1280, 16, 16), y=float16(1, 1280, 1, 1), x=float16(2, 2560, 16, 16), y=float16(2, 1280, 16, 16), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 256, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 2560, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 256, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1920, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1920,), y=float16(1920, 1, 1), shape=[1920, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1920, 1, 1), y=float16(1, 1920, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 1920, 1, 1), y=float16(1, 1280, 1920, 1, 1), shape=[1, 1280, 1920, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 1920, 1, 1), y=float16(1, 1920, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(1280, 1920, 3, 3), y=float16(1, 1280, 1920, 3, 3), shape=[1, 1280, 1920, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 1280, 1920, 3, 3), y=float16(1, 17280, 1280))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 60, 16, 16), y=float16(2, 32, 60, 16, 16), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 1920, 16, 16), y=float16(2, 32, 60, 16, 16), shape=[2, 32, 60, 16, 16])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 17280, 1280), y=float16(1, 1280, 1, 1), y=float16(1280,), y=float16(1920, 1, 1), x=float16(1, 2, 1280), y=float16(1920, 1, 1), x=float16(2, 32, 60, 16, 16), y=float16(2, 32, 40, 16, 16), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 1920, 1280), y=float16(2, 1280, 16, 16), y=float16(1, 1280, 1, 1), x=float16(2, 1920, 16, 16), y=float16(2, 1280, 16, 16), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 256, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 4 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%hidden_states,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_conv : [#users=1] = call_module[target=self_conv](args = (%interpolate,), kwargs = {})\n",
            "    return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 16, 16), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%hidden_states,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_conv : [#users=1] = call_module[target=self_conv](args = (%interpolate,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 1280), y=float16(1, 1280, 1, 1), data=float16(2, 1280, 16, 16), z=float16(2, 1280, 32, 32), fused_ops='resize2d pad conv2d_gemm_image_transform batch_matmul reshape rearrange add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1920, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 1920, 1, 1), y=float16(1, 640, 1920, 1, 1), shape=[1, 640, 1920, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 1920, 1, 1), y=float16(1, 1920, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 1920, 3, 3), y=float16(1, 640, 1920, 3, 3), shape=[1, 640, 1920, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 1920, 3, 3), y=float16(1, 17280, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 1920, 32, 32), y=float16(2, 32, 60, 32, 32), shape=[2, 32, 60, 32, 32])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 60, 32, 32), y=float16(2, 32, 60, 32, 32), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 17280, 640), y=float16(1, 640, 1, 1), y=float16(640,), y=float16(1920, 1, 1), x=float16(1, 2, 640), y=float16(1920, 1, 1), x=float16(2, 32, 60, 32, 32), y=float16(2, 32, 20, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 1920, 640), y=float16(2, 640, 32, 32), y=float16(1, 640, 1, 1), x=float16(2, 1920, 32, 32), y=float16(2, 640, 32, 32), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 1280, 1, 1), y=float16(1, 640, 1280, 1, 1), shape=[1, 640, 1280, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 1280, 1, 1), y=float16(1, 1280, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 1280, 3, 3), y=float16(1, 640, 1280, 3, 3), shape=[1, 640, 1280, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 1280, 3, 3), y=float16(1, 11520, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 40, 32, 32), y=float16(2, 32, 40, 32, 32), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 1280, 32, 32), y=float16(2, 32, 40, 32, 32), shape=[2, 32, 40, 32, 32])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 11520, 640), y=float16(1, 640, 1, 1), y=float16(640,), y=float16(1280, 1, 1), x=float16(1, 2, 640), y=float16(1280, 1, 1), x=float16(2, 32, 40, 32, 32), y=float16(2, 32, 20, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 1280, 640), y=float16(2, 640, 32, 32), y=float16(1, 640, 1, 1), x=float16(2, 1280, 32, 32), y=float16(2, 640, 32, 32), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 960, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(960,), y=float16(960, 1, 1), shape=[960, 1, 1])\u001b[0m...\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(960, 1, 1), y=float16(1, 960, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 960, 1, 1), y=float16(1, 640, 960, 1, 1), shape=[1, 640, 960, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 960, 1, 1), y=float16(1, 960, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(640, 960, 3, 3), y=float16(1, 640, 960, 3, 3), shape=[1, 640, 960, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 640, 960, 3, 3), y=float16(1, 8640, 640))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 960, 32, 32), y=float16(2, 32, 30, 32, 32), shape=[2, 32, 30, 32, 32])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 30, 32, 32), y=float16(2, 32, 30, 32, 32), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 8640, 640), y=float16(1, 640, 1, 1), y=float16(640,), y=float16(960, 1, 1), x=float16(1, 2, 640), y=float16(960, 1, 1), x=float16(2, 32, 30, 32, 32), y=float16(2, 32, 20, 32, 32), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 960, 640), y=float16(2, 640, 32, 32), y=float16(1, 640, 1, 1), x=float16(2, 960, 32, 32), y=float16(2, 640, 32, 32), fused_ops='conv2d_gemm_image_transform batch_matmul reshape rearrange add add divs', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 10 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "    %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "    %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "    %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "    %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "    %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "    %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "    return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1024, 640), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %self_net_0_proj : [#users=1] = call_module[target=self_net_0_proj](args = (%hidden_states,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %chunk : [#users=2] = call_method[target=chunk](args = (%self_net_0_proj, 2), kwargs = {dim: -1})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %getitem_1 : [#users=1] = call_function[target=operator.getitem](args = (%chunk, 1), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %gelu : [#users=1] = call_function[target=torch._C._nn.gelu](args = (%getitem_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %mul : [#users=1] = call_function[target=operator.mul](args = (%getitem, %gelu), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %self_net_1 : [#users=1] = call_module[target=self_net_1](args = (%mul,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %self_net_2 : [#users=1] = call_module[target=self_net_2](args = (%self_net_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: return (self_net_2,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 4 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "    %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%hidden_states,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "    %self_conv : [#users=1] = call_module[target=self_conv](args = (%interpolate,), kwargs = {})\n",
            "    return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 640, 32, 32), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %hidden_states : torch.Tensor [#users=1] = placeholder[target=hidden_states]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %interpolate : [#users=1] = call_function[target=torch.nn.functional.interpolate](args = (%hidden_states,), kwargs = {scale_factor: 2.0, mode: nearest})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_conv : [#users=1] = call_module[target=self_conv](args = (%interpolate,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: return (self_conv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 5760, 640), y=float16(1, 640, 1, 1), data=float16(2, 640, 32, 32), z=float16(2, 640, 64, 64), fused_ops='resize2d pad conv2d_gemm_image_transform batch_matmul reshape rearrange add', anchor='batch_matmul')\u001b[0m...\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish building computation graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish generating the executor\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:received a subgraph with 17 nodes to optimize\n",
            "DEBUG:hidet.graph.frontend.torch.dynamo_backends:graph: graph():\n",
            "    %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "    %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "    %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "    %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "    %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "    %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "    %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "    %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "    %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "    %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "    %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "    %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "    %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "    %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "    %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "    %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "    return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet: symbolic inputs: \n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 960, 64, 64), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:hidet:   Tensor(shape=(2, 1280), dtype='float16', device='cuda:0')\n",
            "INFO:hidet.graph.frontend.torch.interpreter:start to interpret graph\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 0: %input_tensor : torch.Tensor [#users=2] = placeholder[target=input_tensor]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 1: %temb : torch.Tensor [#users=1] = placeholder[target=temb]\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 2: %self_norm1 : [#users=1] = call_module[target=self_norm1](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 3: %self_nonlinearity : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 4: %self_conv1 : [#users=1] = call_module[target=self_conv1](args = (%self_nonlinearity,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 5: %self_nonlinearity_1 : [#users=1] = call_module[target=self_nonlinearity](args = (%temb,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 6: %self_time_emb_proj : [#users=1] = call_module[target=self_time_emb_proj](args = (%self_nonlinearity_1,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 7: %getitem : [#users=1] = call_function[target=operator.getitem](args = (%self_time_emb_proj, (slice(None, None, None), slice(None, None, None), None, None)), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 8: %add : [#users=1] = call_function[target=operator.add](args = (%self_conv1, %getitem), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 9: %self_norm2 : [#users=1] = call_module[target=self_norm2](args = (%add,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 10: %self_nonlinearity_2 : [#users=1] = call_module[target=self_nonlinearity](args = (%self_norm2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 11: %self_dropout : [#users=1] = call_module[target=self_dropout](args = (%self_nonlinearity_2,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 12: %self_conv2 : [#users=1] = call_module[target=self_conv2](args = (%self_dropout,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 13: %self_conv_shortcut : [#users=1] = call_module[target=self_conv_shortcut](args = (%input_tensor,), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 14: %add_1 : [#users=1] = call_function[target=operator.add](args = (%self_conv_shortcut, %self_conv2), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 15: %truediv : [#users=1] = call_function[target=operator.truediv](args = (%add_1, 1.0), kwargs = {})\n",
            "DEBUG:hidet.graph.frontend.torch.interpreter:interpreting node 16: return (truediv,)\n",
            "INFO:hidet.graph.frontend.torch.interpreter:finish interpreting graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to optimize the flow graph\n",
            "Compiling cuda task \u001b[92madd(x=float16(1, 320, 1, 1), y=float16(1, 320, 1, 1), z=float16(1, 320, 1, 1))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.subgraph_rewrite:Applying transform: (x + a) + (y + b) => (x + y) + (a + b)\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(320, 960, 1, 1), y=float16(1, 320, 960, 1, 1), shape=[1, 320, 960, 1, 1])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 320, 960, 1, 1), y=float16(1, 960, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(320, 960, 3, 3), y=float16(1, 320, 960, 3, 3), shape=[1, 320, 960, 3, 3])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mrearrange(x=float16(1, 320, 960, 3, 3), y=float16(1, 8640, 320))\u001b[0m...\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Conv2d\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator ReduceSum\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "DEBUG:hidet.graph.transforms.resolve_variant:Resolve operator Matmul\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:finish optimizing the flow graph\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:schedule search space: 0\n",
            "INFO:hidet.graph.frontend.torch.dynamo_backends:start to build the optimized computation graph\n",
            "Compiling cuda task \u001b[92mreshape(x=float16(2, 960, 64, 64), y=float16(2, 32, 30, 64, 64), shape=[2, 32, 30, 64, 64])\u001b[0m...\n",
            "Compiling cuda task \u001b[92mnormalize_float16(x=float16(2, 32, 30, 64, 64), y=float16(2, 32, 30, 64, 64), dims=[2, 3, 4], accumulate_dtype='float32', epsilon=half(1e-05))\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 8640, 320), y=float16(1, 320, 1, 1), y=float16(320,), y=float16(960, 1, 1), x=float16(1, 2, 320), y=float16(960, 1, 1), x=float16(2, 32, 30, 64, 64), y=float16(2, 32, 10, 64, 64), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange add reshape add rearrange slice add reshape', anchor='batch_matmul')\u001b[0m...\n",
            "Compiling cuda task \u001b[92mfused(b=float16(1, 2880, 320), y=float16(320, 1, 1), y=float16(320, 1, 1), x=float16(2, 32, 10, 64, 64), y=float16(2, 320, 64, 64), fused_ops='reshape mul add silu pad conv2d_gemm_image_transform batch_matmul reshape rearrange', anchor='batch_matmul')\u001b[0m...\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "cebb6ff7f9854be18ab45631532bb245": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_f988a8b1c8214124aa1812aac60ffeab",
              "IPY_MODEL_279fb451b8e84e619c17c1d807fac9c3",
              "IPY_MODEL_39ed643430694988bc045e26ae2ca096"
            ],
            "layout": "IPY_MODEL_e0016215a25e44bb9b341d7bf9a84471"
          }
        },
        "f988a8b1c8214124aa1812aac60ffeab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_de458e35f6d44c969eb897be742301cd",
            "placeholder": "​",
            "style": "IPY_MODEL_7b8fb10274e84fb2be7c106b53db209d",
            "value": "Downloading (…)ain/model_index.json: 100%"
          }
        },
        "279fb451b8e84e619c17c1d807fac9c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f32a7d6e2b26441485b304379d17550c",
            "max": 541,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_4e8c1d48c4644503a37282f9627956b4",
            "value": 541
          }
        },
        "39ed643430694988bc045e26ae2ca096": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_28d224a69cae483fa4fcd26fc1be3c8e",
            "placeholder": "​",
            "style": "IPY_MODEL_f754a54ef2a44affa1cff5056b4b6e5c",
            "value": " 541/541 [00:00&lt;00:00, 19.9kB/s]"
          }
        },
        "e0016215a25e44bb9b341d7bf9a84471": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "de458e35f6d44c969eb897be742301cd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7b8fb10274e84fb2be7c106b53db209d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f32a7d6e2b26441485b304379d17550c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4e8c1d48c4644503a37282f9627956b4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "28d224a69cae483fa4fcd26fc1be3c8e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f754a54ef2a44affa1cff5056b4b6e5c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "aaa807afeb7e43febaede286aaa18f70": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_90c41a76bda545d1aaa91272e8fc3b86",
              "IPY_MODEL_2da67c92167940018e6cf8028d17d9c0",
              "IPY_MODEL_9c75c3abe6fb408db841a41eb4707807"
            ],
            "layout": "IPY_MODEL_4bf245d5e776446bb9d2fc03f2d3dcea"
          }
        },
        "90c41a76bda545d1aaa91272e8fc3b86": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b6bbb54abe7548148a598413f27dba60",
            "placeholder": "​",
            "style": "IPY_MODEL_98f94a81fb6840d992ee4e07a88c3b53",
            "value": "Fetching 15 files: 100%"
          }
        },
        "2da67c92167940018e6cf8028d17d9c0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_34192a11df0843ef8d5ace02428743f5",
            "max": 15,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_842ad49d22194a91804a6d29f5edb07f",
            "value": 15
          }
        },
        "9c75c3abe6fb408db841a41eb4707807": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4e2a0565ea014e87b33ef007074db3cc",
            "placeholder": "​",
            "style": "IPY_MODEL_bd6b95bb0c1e434382a22ab5283ba652",
            "value": " 15/15 [00:41&lt;00:00,  3.12s/it]"
          }
        },
        "4bf245d5e776446bb9d2fc03f2d3dcea": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6bbb54abe7548148a598413f27dba60": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "98f94a81fb6840d992ee4e07a88c3b53": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "34192a11df0843ef8d5ace02428743f5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "842ad49d22194a91804a6d29f5edb07f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "4e2a0565ea014e87b33ef007074db3cc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "bd6b95bb0c1e434382a22ab5283ba652": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "700b4ddcf5db412bbed90f83a940cf01": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_72213452182b423a9fa7c8f677d42e72",
              "IPY_MODEL_d3e138eec60a4e5c924c4edbb6d67f50",
              "IPY_MODEL_e33982fc0dd3447b9922f2b112648f3d"
            ],
            "layout": "IPY_MODEL_0370e94056de4e46877609396fff5aee"
          }
        },
        "72213452182b423a9fa7c8f677d42e72": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_601af93f10d449a39621a2b2e1c1859f",
            "placeholder": "​",
            "style": "IPY_MODEL_5ff25b47591a4d62a3c89fd8513271cd",
            "value": "Downloading (…)rocessor_config.json: 100%"
          }
        },
        "d3e138eec60a4e5c924c4edbb6d67f50": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_dff424c444be4f968fe6a472f7a93311",
            "max": 342,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_354b8c92c8884af5b9620518a22629ab",
            "value": 342
          }
        },
        "e33982fc0dd3447b9922f2b112648f3d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7719b8eff9bc4927abb537b7a3bcd813",
            "placeholder": "​",
            "style": "IPY_MODEL_c385d79ccd6546b986504cb21df23340",
            "value": " 342/342 [00:00&lt;00:00, 5.45kB/s]"
          }
        },
        "0370e94056de4e46877609396fff5aee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "601af93f10d449a39621a2b2e1c1859f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5ff25b47591a4d62a3c89fd8513271cd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dff424c444be4f968fe6a472f7a93311": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "354b8c92c8884af5b9620518a22629ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7719b8eff9bc4927abb537b7a3bcd813": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c385d79ccd6546b986504cb21df23340": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "00a6d1275a0d487fa9b57e352a200ee1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_48dfd07c9c28490d851df24cb122211d",
              "IPY_MODEL_1979d729156d458596d72bf98516d63d",
              "IPY_MODEL_56ddfa4e878e47859af912dc387ac79e"
            ],
            "layout": "IPY_MODEL_0332fa6047b541f1b7d80325c8e595b0"
          }
        },
        "48dfd07c9c28490d851df24cb122211d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66bbafa4722746b8a968b2d60e632bda",
            "placeholder": "​",
            "style": "IPY_MODEL_0cbb4d831a3443f2b182a7bd530694d2",
            "value": "Downloading (…)cheduler_config.json: 100%"
          }
        },
        "1979d729156d458596d72bf98516d63d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8771e9dde25c4ab09dc7185260809121",
            "max": 308,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b487e1721fa0431a871e226dbbbcdf45",
            "value": 308
          }
        },
        "56ddfa4e878e47859af912dc387ac79e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_518fe020b0f14e599113419f09da1db7",
            "placeholder": "​",
            "style": "IPY_MODEL_b8b372f0ccf34db095a2f144fd6faa34",
            "value": " 308/308 [00:00&lt;00:00, 4.48kB/s]"
          }
        },
        "0332fa6047b541f1b7d80325c8e595b0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66bbafa4722746b8a968b2d60e632bda": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0cbb4d831a3443f2b182a7bd530694d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8771e9dde25c4ab09dc7185260809121": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b487e1721fa0431a871e226dbbbcdf45": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "518fe020b0f14e599113419f09da1db7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b8b372f0ccf34db095a2f144fd6faa34": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1deab4a8ddc34286a8e574409cf95dde": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2d5c4771450442a881c4def31fbbdf0b",
              "IPY_MODEL_97fcde821600472ea087146889787952",
              "IPY_MODEL_7194a3c97ce048789a10be37f9e8eb22"
            ],
            "layout": "IPY_MODEL_0ef6b91f0c6c4a938469f9a5ea1b39b6"
          }
        },
        "2d5c4771450442a881c4def31fbbdf0b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b2960d912d447cf9a5f10fb4275693f",
            "placeholder": "​",
            "style": "IPY_MODEL_f44eea2837d8450da09bda5eb0e043f5",
            "value": "Downloading (…)cial_tokens_map.json: 100%"
          }
        },
        "97fcde821600472ea087146889787952": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b57820b76c7d4623a1a41cd031fa6b65",
            "max": 472,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_96bfeb7e0e8543959b2b02fb8f63798c",
            "value": 472
          }
        },
        "7194a3c97ce048789a10be37f9e8eb22": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b9bfa777aff047ccad6755cecf849a51",
            "placeholder": "​",
            "style": "IPY_MODEL_2c1ae4474645473f9dcb48a6f895a6f0",
            "value": " 472/472 [00:00&lt;00:00, 4.65kB/s]"
          }
        },
        "0ef6b91f0c6c4a938469f9a5ea1b39b6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2b2960d912d447cf9a5f10fb4275693f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f44eea2837d8450da09bda5eb0e043f5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b57820b76c7d4623a1a41cd031fa6b65": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "96bfeb7e0e8543959b2b02fb8f63798c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "b9bfa777aff047ccad6755cecf849a51": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2c1ae4474645473f9dcb48a6f895a6f0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "b2216dff695f4c92a876205e9197b122": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_094f252598b94fbc88e1d89456fc3633",
              "IPY_MODEL_4e97dc8beeed47fca16db734ae82e07b",
              "IPY_MODEL_7a6122765eb6411f9161e22614f85747"
            ],
            "layout": "IPY_MODEL_ebe1cb6dc3794ec3a1f578fc8662311a"
          }
        },
        "094f252598b94fbc88e1d89456fc3633": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_d050957797094724ab250b1fe6f8b76b",
            "placeholder": "​",
            "style": "IPY_MODEL_2182a6fc824c4f5ebea2beb31084a124",
            "value": "Downloading (…)_checker/config.json: 100%"
          }
        },
        "4e97dc8beeed47fca16db734ae82e07b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_3eecf86e29474bb6b3d821e17ec4eaad",
            "max": 4723,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_6e84568e2458472cb1ad3681df5d0a16",
            "value": 4723
          }
        },
        "7a6122765eb6411f9161e22614f85747": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2aed0c110aef42f380eb64b8f8493338",
            "placeholder": "​",
            "style": "IPY_MODEL_6a2fcb5e6f4a4e9db1c9978e57935810",
            "value": " 4.72k/4.72k [00:00&lt;00:00, 45.3kB/s]"
          }
        },
        "ebe1cb6dc3794ec3a1f578fc8662311a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d050957797094724ab250b1fe6f8b76b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2182a6fc824c4f5ebea2beb31084a124": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3eecf86e29474bb6b3d821e17ec4eaad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6e84568e2458472cb1ad3681df5d0a16": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2aed0c110aef42f380eb64b8f8493338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6a2fcb5e6f4a4e9db1c9978e57935810": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "8e22ca70e8074c5f9d8977d6ea93f3f6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb189aa715bb47aa9ccd6a08a3b9d287",
              "IPY_MODEL_09278e7da45f4e5e8752b38d8787bd73",
              "IPY_MODEL_655dfc04df6840d8bd15b08450c08a8f"
            ],
            "layout": "IPY_MODEL_0686e2ed209d4f3da85f1cd5f2fd370b"
          }
        },
        "fb189aa715bb47aa9ccd6a08a3b9d287": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8fc7fdb1c7e34de993670ed66adbbe75",
            "placeholder": "​",
            "style": "IPY_MODEL_d284547b49644053aa813d5cfcf0c706",
            "value": "Downloading (…)_encoder/config.json: 100%"
          }
        },
        "09278e7da45f4e5e8752b38d8787bd73": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_362fe019d991460299d5cd29e14addc9",
            "max": 617,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_192e152d25694245b39b137fd4ce5973",
            "value": 617
          }
        },
        "655dfc04df6840d8bd15b08450c08a8f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_c95e919a26534adfa7fe866934a82b89",
            "placeholder": "​",
            "style": "IPY_MODEL_43e101b3e126461898da489463f457c4",
            "value": " 617/617 [00:00&lt;00:00, 6.37kB/s]"
          }
        },
        "0686e2ed209d4f3da85f1cd5f2fd370b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8fc7fdb1c7e34de993670ed66adbbe75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d284547b49644053aa813d5cfcf0c706": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "362fe019d991460299d5cd29e14addc9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "192e152d25694245b39b137fd4ce5973": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "c95e919a26534adfa7fe866934a82b89": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "43e101b3e126461898da489463f457c4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7c6f07e805ef4ce9a9d74bbb2b008cab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_1011765c22e9404fb61b5b1d2ecf6068",
              "IPY_MODEL_f9203e5200e3474ea21e845a9db37299",
              "IPY_MODEL_9392ed228dcc43669ef19c05013bc606"
            ],
            "layout": "IPY_MODEL_145d440c3ab245f78f9cad216db0a21a"
          }
        },
        "1011765c22e9404fb61b5b1d2ecf6068": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6712acf9a30a480d9a71432e4b2bdcd6",
            "placeholder": "​",
            "style": "IPY_MODEL_7ffa816771ed4b868f011e8b66019e85",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "f9203e5200e3474ea21e845a9db37299": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e26a77ffc3d84ab6ac24fffa104561ac",
            "max": 1216061799,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_20b9db4587424003a225855940d728c1",
            "value": 1216061799
          }
        },
        "9392ed228dcc43669ef19c05013bc606": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_af99804eea7544b9855fa0d88243c8a4",
            "placeholder": "​",
            "style": "IPY_MODEL_c4ad3ddb7e424916abc0d4a1a44ca87d",
            "value": " 1.22G/1.22G [00:17&lt;00:00, 101MB/s]"
          }
        },
        "145d440c3ab245f78f9cad216db0a21a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6712acf9a30a480d9a71432e4b2bdcd6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7ffa816771ed4b868f011e8b66019e85": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "e26a77ffc3d84ab6ac24fffa104561ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "20b9db4587424003a225855940d728c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "af99804eea7544b9855fa0d88243c8a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c4ad3ddb7e424916abc0d4a1a44ca87d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d8a48a728bc4009bc8d29033324faf9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_59965090690747608bd2095c33c5f54f",
              "IPY_MODEL_6de84956be8b460ea4547c6a08e69032",
              "IPY_MODEL_dcb0f135674a4767981b0d51b915ed3c"
            ],
            "layout": "IPY_MODEL_7474c6b13db742219f8b9fae3ca8e5e9"
          }
        },
        "59965090690747608bd2095c33c5f54f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6c8198731bcf441ab40ba8676e38e5fb",
            "placeholder": "​",
            "style": "IPY_MODEL_3a237f4c05284d8b8a48155c2d0cc7df",
            "value": "Downloading (…)tokenizer/merges.txt: 100%"
          }
        },
        "6de84956be8b460ea4547c6a08e69032": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_48a82e73db1e40d48d133010e65bea45",
            "max": 524619,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_a040e05765244590836507066b86cde5",
            "value": 524619
          }
        },
        "dcb0f135674a4767981b0d51b915ed3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8c409d6229734866b04dd85865313f55",
            "placeholder": "​",
            "style": "IPY_MODEL_d247f50f18db4cb5a084b9dbbaf4ea60",
            "value": " 525k/525k [00:00&lt;00:00, 3.23MB/s]"
          }
        },
        "7474c6b13db742219f8b9fae3ca8e5e9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6c8198731bcf441ab40ba8676e38e5fb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3a237f4c05284d8b8a48155c2d0cc7df": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "48a82e73db1e40d48d133010e65bea45": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "a040e05765244590836507066b86cde5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8c409d6229734866b04dd85865313f55": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d247f50f18db4cb5a084b9dbbaf4ea60": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9d89531727054bf595f29bf96574593e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_d8f516ce4e4245d8962f204c764d5d4e",
              "IPY_MODEL_3a5950e92d2146628c1572ccbeef3645",
              "IPY_MODEL_25c1fa3179b74a4593398755040fc4d4"
            ],
            "layout": "IPY_MODEL_7a0bc9700e3f48df91f4d3bfcef04ef3"
          }
        },
        "d8f516ce4e4245d8962f204c764d5d4e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_619eb5eadd784bfe9aa9b894b650b454",
            "placeholder": "​",
            "style": "IPY_MODEL_8a25221d527e4892a747a86a20669eed",
            "value": "Downloading pytorch_model.bin: 100%"
          }
        },
        "3a5950e92d2146628c1572ccbeef3645": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_589c4d5195c7488eb1c051164ac24039",
            "max": 492305335,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8c9b917e9ae74cf69e729dcb584c5610",
            "value": 492305335
          }
        },
        "25c1fa3179b74a4593398755040fc4d4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_715bd7bd8fd040db87f50e6003d6ef5c",
            "placeholder": "​",
            "style": "IPY_MODEL_0d53a6118de74e7faa360bb63f3ae7ab",
            "value": " 492M/492M [00:08&lt;00:00, 75.0MB/s]"
          }
        },
        "7a0bc9700e3f48df91f4d3bfcef04ef3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "619eb5eadd784bfe9aa9b894b650b454": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8a25221d527e4892a747a86a20669eed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "589c4d5195c7488eb1c051164ac24039": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8c9b917e9ae74cf69e729dcb584c5610": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "715bd7bd8fd040db87f50e6003d6ef5c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "0d53a6118de74e7faa360bb63f3ae7ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "32d546a8801843fd930b265ce53fe7da": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_633626951197482bb60b9db58cc1821c",
              "IPY_MODEL_87c8fc867cff4091a2e8a1a667962d2f",
              "IPY_MODEL_752ed006d29742819ca0540301688cc1"
            ],
            "layout": "IPY_MODEL_94d1164cb9db4b7c91441388e810e102"
          }
        },
        "633626951197482bb60b9db58cc1821c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4b1c9ab48fa84f708f6cec2d806275f1",
            "placeholder": "​",
            "style": "IPY_MODEL_648d5eb18d254b7685a4789fc20201b0",
            "value": "Downloading (…)okenizer_config.json: 100%"
          }
        },
        "87c8fc867cff4091a2e8a1a667962d2f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ecb4ca8b11ea435f96b66401681b91a8",
            "max": 806,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_44a2178eee0f4a74959658da86682462",
            "value": 806
          }
        },
        "752ed006d29742819ca0540301688cc1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_68da365b26864d368c526faedeb394c0",
            "placeholder": "​",
            "style": "IPY_MODEL_5f0fa895708e4793b56458d3958b8995",
            "value": " 806/806 [00:00&lt;00:00, 8.25kB/s]"
          }
        },
        "94d1164cb9db4b7c91441388e810e102": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4b1c9ab48fa84f708f6cec2d806275f1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "648d5eb18d254b7685a4789fc20201b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "ecb4ca8b11ea435f96b66401681b91a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "44a2178eee0f4a74959658da86682462": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "68da365b26864d368c526faedeb394c0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5f0fa895708e4793b56458d3958b8995": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "19712264493744e69242b80aba9350c1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_3f2bfec010f546579f1dd112482ef47e",
              "IPY_MODEL_efbeeacabbc34349a03c3e65bb2a707f",
              "IPY_MODEL_8b7971b09e504e1f92e3ee8b3c0b3101"
            ],
            "layout": "IPY_MODEL_4cf4f4ca07e549e480e196293e83723d"
          }
        },
        "3f2bfec010f546579f1dd112482ef47e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_367bbe72b45a45dc947bce8a74354e87",
            "placeholder": "​",
            "style": "IPY_MODEL_4696c98ca7db46ac8e38a0f672bb4186",
            "value": "Downloading (…)e6a/unet/config.json: 100%"
          }
        },
        "efbeeacabbc34349a03c3e65bb2a707f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6cfdb583e3224618bdb7c38f79d38338",
            "max": 743,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_8dee36c8906f4dd89a39d49092eeb139",
            "value": 743
          }
        },
        "8b7971b09e504e1f92e3ee8b3c0b3101": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1e29e1c32833496b818e04903a439738",
            "placeholder": "​",
            "style": "IPY_MODEL_afa953817100431e95b43023683e706e",
            "value": " 743/743 [00:00&lt;00:00, 7.64kB/s]"
          }
        },
        "4cf4f4ca07e549e480e196293e83723d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "367bbe72b45a45dc947bce8a74354e87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4696c98ca7db46ac8e38a0f672bb4186": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "6cfdb583e3224618bdb7c38f79d38338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8dee36c8906f4dd89a39d49092eeb139": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "1e29e1c32833496b818e04903a439738": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "afa953817100431e95b43023683e706e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "c7e6459b81434c908d86194eef083f30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_2fb2a053a7fe4b489a6672a2ec90c0c8",
              "IPY_MODEL_5ecf4b2bddbf4e9e91d26c3da33ac314",
              "IPY_MODEL_f704b8ca34a04b95b53e17aa586789ff"
            ],
            "layout": "IPY_MODEL_b90c9b68748f441da87d042a4b2db808"
          }
        },
        "2fb2a053a7fe4b489a6672a2ec90c0c8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e515314b2528479c9ee84d66507760b7",
            "placeholder": "​",
            "style": "IPY_MODEL_f49af0de7764491bbf061520ca46a2c3",
            "value": "Downloading (…)8e6a/vae/config.json: 100%"
          }
        },
        "5ecf4b2bddbf4e9e91d26c3da33ac314": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cc29bdf1568f43d4bce808cacaab4cbc",
            "max": 547,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b5d8a4cc9a0f4ed4853b3f4c6436ae3b",
            "value": 547
          }
        },
        "f704b8ca34a04b95b53e17aa586789ff": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7b139b5f9c6e4d258ede4bee6a6f5d10",
            "placeholder": "​",
            "style": "IPY_MODEL_867442b9ec974644aa0176ffb244cd05",
            "value": " 547/547 [00:00&lt;00:00, 8.13kB/s]"
          }
        },
        "b90c9b68748f441da87d042a4b2db808": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e515314b2528479c9ee84d66507760b7": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f49af0de7764491bbf061520ca46a2c3": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "cc29bdf1568f43d4bce808cacaab4cbc": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b5d8a4cc9a0f4ed4853b3f4c6436ae3b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "7b139b5f9c6e4d258ede4bee6a6f5d10": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "867442b9ec974644aa0176ffb244cd05": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "28116b65548446808cfd1f247d5f3310": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_470713a6ea3643f5baf537722eadd740",
              "IPY_MODEL_149f24fa6c84463b9210078e1006366a",
              "IPY_MODEL_ce2e836ceea644bf92ef78baf5494a94"
            ],
            "layout": "IPY_MODEL_cd6f1d9541594bfeacb7e32e4e9de7b1"
          }
        },
        "470713a6ea3643f5baf537722eadd740": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_094ad3c0b3444107b642d0c065734bbd",
            "placeholder": "​",
            "style": "IPY_MODEL_2042c95befc742c79f8065aad1e77265",
            "value": "Downloading (…)tokenizer/vocab.json: 100%"
          }
        },
        "149f24fa6c84463b9210078e1006366a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_13639ede86f94c15b336504a30e29a9c",
            "max": 1059962,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac42116dc32e4fba80a8b7b361518573",
            "value": 1059962
          }
        },
        "ce2e836ceea644bf92ef78baf5494a94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_2b3e710398db47ba9eef708e68cc65a0",
            "placeholder": "​",
            "style": "IPY_MODEL_7bf68d30f36c4674a548b62b63e5fefe",
            "value": " 1.06M/1.06M [00:00&lt;00:00, 2.38MB/s]"
          }
        },
        "cd6f1d9541594bfeacb7e32e4e9de7b1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "094ad3c0b3444107b642d0c065734bbd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2042c95befc742c79f8065aad1e77265": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "13639ede86f94c15b336504a30e29a9c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac42116dc32e4fba80a8b7b361518573": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "2b3e710398db47ba9eef708e68cc65a0": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7bf68d30f36c4674a548b62b63e5fefe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "31e11e7961a8476c8f01aa34f3efb850": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bfaa3abca1234dd29fd9fdf5bd9d9d48",
              "IPY_MODEL_d153917a48984991b4f60a23aa4e6eaa",
              "IPY_MODEL_5f4bba9d674244ad94c41c20f7a1c8b0"
            ],
            "layout": "IPY_MODEL_cc01e46517e646f3a0d3ce1ae8510d04"
          }
        },
        "bfaa3abca1234dd29fd9fdf5bd9d9d48": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9728e1bd20464055ac10c7a54571fa18",
            "placeholder": "​",
            "style": "IPY_MODEL_e307ff835ba34787854697af186380bb",
            "value": "Downloading (…)on_pytorch_model.bin: 100%"
          }
        },
        "d153917a48984991b4f60a23aa4e6eaa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_002ac38a6a6d4f888608989c0b973ec5",
            "max": 3438354725,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_f77b94d8b704463d97389e42746cd6dd",
            "value": 3438354725
          }
        },
        "5f4bba9d674244ad94c41c20f7a1c8b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6e4f93c0eaa44198adbdd8a3cecaf7df",
            "placeholder": "​",
            "style": "IPY_MODEL_731e1d3785904700888a170bf771d70f",
            "value": " 3.44G/3.44G [00:40&lt;00:00, 109MB/s]"
          }
        },
        "cc01e46517e646f3a0d3ce1ae8510d04": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9728e1bd20464055ac10c7a54571fa18": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e307ff835ba34787854697af186380bb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "002ac38a6a6d4f888608989c0b973ec5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f77b94d8b704463d97389e42746cd6dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6e4f93c0eaa44198adbdd8a3cecaf7df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "731e1d3785904700888a170bf771d70f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "97ecc262cfe2405f8526beadc394c9f1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f5a0ddfdd9841f6abaa380a738c2e37",
              "IPY_MODEL_d055cf30f6f34962a3b867af89efdc4b",
              "IPY_MODEL_f612b25683654ed596af74c4febf5060"
            ],
            "layout": "IPY_MODEL_d54791ba84d14f8fb5cc154f1bb6297d"
          }
        },
        "8f5a0ddfdd9841f6abaa380a738c2e37": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_66a998ff2d0548cf97c9e046e4a1fde9",
            "placeholder": "​",
            "style": "IPY_MODEL_6ab1392947934c78b1482629c298fa1b",
            "value": "Downloading (…)on_pytorch_model.bin: 100%"
          }
        },
        "d055cf30f6f34962a3b867af89efdc4b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_1360611c64144103b4034ad769785792",
            "max": 334707217,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_80d9c0c036bb4d659a47b176c1c44eb0",
            "value": 334707217
          }
        },
        "f612b25683654ed596af74c4febf5060": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e930c17235c94733971f5a7e55808c37",
            "placeholder": "​",
            "style": "IPY_MODEL_5aa636cdf24c491cb86a62f8b41ad3b7",
            "value": " 335M/335M [00:05&lt;00:00, 63.7MB/s]"
          }
        },
        "d54791ba84d14f8fb5cc154f1bb6297d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "66a998ff2d0548cf97c9e046e4a1fde9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6ab1392947934c78b1482629c298fa1b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "1360611c64144103b4034ad769785792": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "80d9c0c036bb4d659a47b176c1c44eb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "e930c17235c94733971f5a7e55808c37": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5aa636cdf24c491cb86a62f8b41ad3b7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "5d912348ae64466a9dac3bc6f9b202d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8fb385cd1a804275a05e113ec9d38080",
              "IPY_MODEL_9999e0c4944a453b869117117abdeab2",
              "IPY_MODEL_214a407cec084678b1e1d2fd55150639"
            ],
            "layout": "IPY_MODEL_210f8e2eafea4de8bbf79149cdb10982"
          }
        },
        "8fb385cd1a804275a05e113ec9d38080": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fb323df4e2b9413aab02464a3b91667a",
            "placeholder": "​",
            "style": "IPY_MODEL_4938eaa2e6f641f39941ceb22d2e842a",
            "value": "100%"
          }
        },
        "9999e0c4944a453b869117117abdeab2": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_be3b8ab1ccd941dc9fe8713b77c1d1a8",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_b4fd476bb1754223b48d339a053713d5",
            "value": 50
          }
        },
        "214a407cec084678b1e1d2fd55150639": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a60d4681a9e24819974d45982ff02b87",
            "placeholder": "​",
            "style": "IPY_MODEL_6906523f79db47d3a8a90bbbff594127",
            "value": " 50/50 [00:11&lt;00:00,  7.31it/s]"
          }
        },
        "210f8e2eafea4de8bbf79149cdb10982": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "fb323df4e2b9413aab02464a3b91667a": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4938eaa2e6f641f39941ceb22d2e842a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "be3b8ab1ccd941dc9fe8713b77c1d1a8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b4fd476bb1754223b48d339a053713d5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a60d4681a9e24819974d45982ff02b87": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6906523f79db47d3a8a90bbbff594127": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "40d102c2aeb64df7b7f9f4f500dc9c80": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0bf1f963ddd3466291e90a12cdd0740f",
              "IPY_MODEL_9ef38652377246eb97ef986902b9d260",
              "IPY_MODEL_18429a36b02d4fcbb3dba6dbc71b1bd7"
            ],
            "layout": "IPY_MODEL_73ad556ec14d4685a17945d1545beaee"
          }
        },
        "0bf1f963ddd3466291e90a12cdd0740f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6b2345295aff4d0f86ef5967590135ac",
            "placeholder": "​",
            "style": "IPY_MODEL_b6f7a3b23e1b4b3d838c23d2ea0c4733",
            "value": "  0%"
          }
        },
        "9ef38652377246eb97ef986902b9d260": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9cc7d6a18c444318bec77016ba49ba75",
            "max": 50,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_200c75379ba04656866437a5790bcdd4",
            "value": 0
          }
        },
        "18429a36b02d4fcbb3dba6dbc71b1bd7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_8728c3632de046fb9d3a6cd7fa13ad14",
            "placeholder": "​",
            "style": "IPY_MODEL_34e52035789c4b568f42d680cf4cad2e",
            "value": " 0/50 [00:00&lt;?, ?it/s]"
          }
        },
        "73ad556ec14d4685a17945d1545beaee": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6b2345295aff4d0f86ef5967590135ac": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b6f7a3b23e1b4b3d838c23d2ea0c4733": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9cc7d6a18c444318bec77016ba49ba75": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "200c75379ba04656866437a5790bcdd4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "8728c3632de046fb9d3a6cd7fa13ad14": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "34e52035789c4b568f42d680cf4cad2e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}